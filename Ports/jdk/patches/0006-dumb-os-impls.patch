From 8bab738757abcad08e46127111e3991337f84242 Mon Sep 17 00:00:00 2001
From: AnotherTest <ali.mpfard@gmail.com>
Date: Tue, 13 Apr 2021 02:03:40 +0430
Subject: [PATCH 06/14] dumb os impls

---
 .../os/serenity/attachListener_serenity.cpp   |  582 ++
 .../os/serenity/c1_globals_serenity.hpp       |   36 +
 .../os/serenity/c2_globals_serenity.hpp       |   36 +
 .../os/serenity/cgroupSubsystem_serenity.cpp  |  516 ++
 .../os/serenity/cgroupSubsystem_serenity.hpp  |  315 +
 .../serenity/cgroupV1Subsystem_serenity.cpp   |  243 +
 .../serenity/cgroupV1Subsystem_serenity.hpp   |  118 +
 .../serenity/cgroupV2Subsystem_serenity.cpp   |  246 +
 .../serenity/cgroupV2Subsystem_serenity.hpp   |   89 +
 src/hotspot/os/serenity/decoder_serenity.cpp  |   88 +
 .../os/serenity/gc/z/zLargePages_linux.cpp    |   38 +
 .../os/serenity/gc/z/zMountPoint_linux.cpp    |  150 +
 .../os/serenity/gc/z/zMountPoint_linux.hpp    |   52 +
 src/hotspot/os/serenity/gc/z/zNUMA_linux.cpp  |   70 +
 .../gc/z/zPhysicalMemoryBacking_linux.cpp     |  723 +++
 .../gc/z/zPhysicalMemoryBacking_linux.hpp     |   77 +
 .../os/serenity/gc/z/zSyscall_linux.cpp       |   40 +
 .../os/serenity/gc/z/zSyscall_linux.hpp       |   44 +
 src/hotspot/os/serenity/globals_serenity.hpp  |   98 +
 .../os/serenity/osContainer_serenity.cpp      |  131 +
 .../os/serenity/osContainer_serenity.hpp      |   71 +
 src/hotspot/os/serenity/osThread_serenity.cpp |   50 +
 src/hotspot/os/serenity/osThread_serenity.hpp |  135 +
 src/hotspot/os/serenity/os_perf_serenity.cpp  | 1052 ++++
 src/hotspot/os/serenity/os_serenity.cpp       | 5299 +++++++++++++++++
 src/hotspot/os/serenity/os_serenity.hpp       |  165 +
 .../os/serenity/os_serenity.inline.hpp        |   44 +
 src/hotspot/os/serenity/os_share_serenity.hpp |   36 +
 .../os/serenity/threadCritical_serenity.cpp   |   61 +
 .../os/serenity/vmStructs_serenity.hpp        |   45 +
 .../os/serenity/waitBarrier_serenity.cpp      |   80 +
 .../os/serenity/waitBarrier_serenity.hpp      |   47 +
 .../serenity_x86/assembler_serenity_x86.cpp   |   32 +
 .../serenity_x86/atomic_serenity_x86.hpp      |  225 +
 .../bytes_serenity_x86.inline.hpp             |   79 +
 .../serenity_x86/copy_serenity_x86.inline.hpp |  309 +
 .../gc/z/zSyscall_serenity_x86.hpp            |   40 +
 .../serenity_x86/globals_serenity_x86.hpp     |   50 +
 .../os_cpu/serenity_x86/linux_x86_32.s        |  645 ++
 .../os_cpu/serenity_x86/linux_x86_64.s        |  380 ++
 .../serenity_x86/orderAccess_serenity_x86.hpp |   69 +
 .../os_cpu/serenity_x86/os_serenity_x86.cpp   |  711 +++
 .../os_cpu/serenity_x86/os_serenity_x86.hpp   |   51 +
 .../serenity_x86/os_serenity_x86.inline.hpp   |   46 +
 .../prefetch_serenity_x86.inline.hpp          |   47 +
 .../serenity_x86/thread_serenity_x86.cpp      |   94 +
 .../serenity_x86/thread_serenity_x86.hpp      |   48 +
 .../serenity_x86/vmStructs_serenity_x86.hpp   |   54 +
 .../serenity_x86/vm_version_serenity_x86.cpp  |   28 +
 .../share/utilities/globalDefinitions_gcc.hpp |   12 +-
 50 files changed, 13693 insertions(+), 4 deletions(-)
 create mode 100644 src/hotspot/os/serenity/attachListener_serenity.cpp
 create mode 100644 src/hotspot/os/serenity/c1_globals_serenity.hpp
 create mode 100644 src/hotspot/os/serenity/c2_globals_serenity.hpp
 create mode 100644 src/hotspot/os/serenity/cgroupSubsystem_serenity.cpp
 create mode 100644 src/hotspot/os/serenity/cgroupSubsystem_serenity.hpp
 create mode 100644 src/hotspot/os/serenity/cgroupV1Subsystem_serenity.cpp
 create mode 100644 src/hotspot/os/serenity/cgroupV1Subsystem_serenity.hpp
 create mode 100644 src/hotspot/os/serenity/cgroupV2Subsystem_serenity.cpp
 create mode 100644 src/hotspot/os/serenity/cgroupV2Subsystem_serenity.hpp
 create mode 100644 src/hotspot/os/serenity/decoder_serenity.cpp
 create mode 100644 src/hotspot/os/serenity/gc/z/zLargePages_linux.cpp
 create mode 100644 src/hotspot/os/serenity/gc/z/zMountPoint_linux.cpp
 create mode 100644 src/hotspot/os/serenity/gc/z/zMountPoint_linux.hpp
 create mode 100644 src/hotspot/os/serenity/gc/z/zNUMA_linux.cpp
 create mode 100644 src/hotspot/os/serenity/gc/z/zPhysicalMemoryBacking_linux.cpp
 create mode 100644 src/hotspot/os/serenity/gc/z/zPhysicalMemoryBacking_linux.hpp
 create mode 100644 src/hotspot/os/serenity/gc/z/zSyscall_linux.cpp
 create mode 100644 src/hotspot/os/serenity/gc/z/zSyscall_linux.hpp
 create mode 100644 src/hotspot/os/serenity/globals_serenity.hpp
 create mode 100644 src/hotspot/os/serenity/osContainer_serenity.cpp
 create mode 100644 src/hotspot/os/serenity/osContainer_serenity.hpp
 create mode 100644 src/hotspot/os/serenity/osThread_serenity.cpp
 create mode 100644 src/hotspot/os/serenity/osThread_serenity.hpp
 create mode 100644 src/hotspot/os/serenity/os_perf_serenity.cpp
 create mode 100644 src/hotspot/os/serenity/os_serenity.cpp
 create mode 100644 src/hotspot/os/serenity/os_serenity.hpp
 create mode 100644 src/hotspot/os/serenity/os_serenity.inline.hpp
 create mode 100644 src/hotspot/os/serenity/os_share_serenity.hpp
 create mode 100644 src/hotspot/os/serenity/threadCritical_serenity.cpp
 create mode 100644 src/hotspot/os/serenity/vmStructs_serenity.hpp
 create mode 100644 src/hotspot/os/serenity/waitBarrier_serenity.cpp
 create mode 100644 src/hotspot/os/serenity/waitBarrier_serenity.hpp
 create mode 100644 src/hotspot/os_cpu/serenity_x86/assembler_serenity_x86.cpp
 create mode 100644 src/hotspot/os_cpu/serenity_x86/atomic_serenity_x86.hpp
 create mode 100644 src/hotspot/os_cpu/serenity_x86/bytes_serenity_x86.inline.hpp
 create mode 100644 src/hotspot/os_cpu/serenity_x86/copy_serenity_x86.inline.hpp
 create mode 100644 src/hotspot/os_cpu/serenity_x86/gc/z/zSyscall_serenity_x86.hpp
 create mode 100644 src/hotspot/os_cpu/serenity_x86/globals_serenity_x86.hpp
 create mode 100644 src/hotspot/os_cpu/serenity_x86/linux_x86_32.s
 create mode 100644 src/hotspot/os_cpu/serenity_x86/linux_x86_64.s
 create mode 100644 src/hotspot/os_cpu/serenity_x86/orderAccess_serenity_x86.hpp
 create mode 100644 src/hotspot/os_cpu/serenity_x86/os_serenity_x86.cpp
 create mode 100644 src/hotspot/os_cpu/serenity_x86/os_serenity_x86.hpp
 create mode 100644 src/hotspot/os_cpu/serenity_x86/os_serenity_x86.inline.hpp
 create mode 100644 src/hotspot/os_cpu/serenity_x86/prefetch_serenity_x86.inline.hpp
 create mode 100644 src/hotspot/os_cpu/serenity_x86/thread_serenity_x86.cpp
 create mode 100644 src/hotspot/os_cpu/serenity_x86/thread_serenity_x86.hpp
 create mode 100644 src/hotspot/os_cpu/serenity_x86/vmStructs_serenity_x86.hpp
 create mode 100644 src/hotspot/os_cpu/serenity_x86/vm_version_serenity_x86.cpp

diff --git a/src/hotspot/os/serenity/attachListener_serenity.cpp b/src/hotspot/os/serenity/attachListener_serenity.cpp
new file mode 100644
index 00000000000..c4cf87cf92e
--- /dev/null
+++ b/src/hotspot/os/serenity/attachListener_serenity.cpp
@@ -0,0 +1,582 @@
+/*
+ * Copyright (c) 2005, 2020, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#include "precompiled.hpp"
+#include "logging/log.hpp"
+#include "memory/allocation.inline.hpp"
+#include "runtime/interfaceSupport.inline.hpp"
+#include "runtime/os.inline.hpp"
+#include "services/attachListener.hpp"
+#include "services/dtraceAttacher.hpp"
+
+#include <unistd.h>
+#include <signal.h>
+#include <sys/types.h>
+#include <sys/socket.h>
+#include <sys/un.h>
+#include <sys/stat.h>
+
+#ifndef UNIX_PATH_MAX
+#define UNIX_PATH_MAX   sizeof(((struct sockaddr_un *)0)->sun_path)
+#endif
+
+// The attach mechanism on Serenity uses a UNIX domain socket. An attach listener
+// thread is created at startup or is created on-demand via a signal from
+// the client tool. The attach listener creates a socket and binds it to a file
+// in the filesystem. The attach listener then acts as a simple (single-
+// threaded) server - it waits for a client to connect, reads the request,
+// executes it, and returns the response to the client via the socket
+// connection.
+//
+// As the socket is a UNIX domain socket it means that only clients on the
+// local machine can connect. In addition there are two other aspects to
+// the security:
+// 1. The well known file that the socket is bound to has permission 400
+// 2. When a client connect, the SO_PEERCRED socket option is used to
+//    obtain the credentials of client. We check that the effective uid
+//    of the client matches this process.
+
+// forward reference
+class SerenityAttachOperation;
+
+class SerenityAttachListener: AllStatic {
+ private:
+  // the path to which we bind the UNIX domain socket
+  static char _path[UNIX_PATH_MAX];
+  static bool _has_path;
+
+  // the file descriptor for the listening socket
+  static volatile int _listener;
+
+  static bool _atexit_registered;
+
+  // reads a request from the given connected socket
+  static SerenityAttachOperation* read_request(int s);
+
+ public:
+  enum {
+    ATTACH_PROTOCOL_VER = 1                     // protocol version
+  };
+  enum {
+    ATTACH_ERROR_BADVERSION     = 101           // error codes
+  };
+
+  static void set_path(char* path) {
+    if (path == NULL) {
+      _path[0] = '\0';
+      _has_path = false;
+    } else {
+      strncpy(_path, path, UNIX_PATH_MAX);
+      _path[UNIX_PATH_MAX-1] = '\0';
+      _has_path = true;
+    }
+  }
+
+  static void set_listener(int s)               { _listener = s; }
+
+  // initialize the listener, returns 0 if okay
+  static int init();
+
+  static char* path()                   { return _path; }
+  static bool has_path()                { return _has_path; }
+  static int listener()                 { return _listener; }
+
+  // write the given buffer to a socket
+  static int write_fully(int s, char* buf, int len);
+
+  static SerenityAttachOperation* dequeue();
+};
+
+class SerenityAttachOperation: public AttachOperation {
+ private:
+  // the connection to the client
+  int _socket;
+
+ public:
+  void complete(jint res, bufferedStream* st);
+
+  void set_socket(int s)                                { _socket = s; }
+  int socket() const                                    { return _socket; }
+
+  SerenityAttachOperation(char* name) : AttachOperation(name) {
+    set_socket(-1);
+  }
+};
+
+// statics
+char SerenityAttachListener::_path[UNIX_PATH_MAX];
+bool SerenityAttachListener::_has_path;
+volatile int SerenityAttachListener::_listener = -1;
+bool SerenityAttachListener::_atexit_registered = false;
+
+// Supporting class to help split a buffer into individual components
+class ArgumentIterator : public StackObj {
+ private:
+  char* _pos;
+  char* _end;
+ public:
+  ArgumentIterator(char* arg_buffer, size_t arg_size) {
+    _pos = arg_buffer;
+    _end = _pos + arg_size - 1;
+  }
+  char* next() {
+    if (*_pos == '\0') {
+      // advance the iterator if possible (null arguments)
+      if (_pos < _end) {
+        _pos += 1;
+      }
+      return NULL;
+    }
+    char* res = _pos;
+    char* next_pos = strchr(_pos, '\0');
+    if (next_pos < _end)  {
+      next_pos++;
+    }
+    _pos = next_pos;
+    return res;
+  }
+};
+
+
+// atexit hook to stop listener and unlink the file that it is
+// bound too.
+extern "C" {
+  static void listener_cleanup() {
+    int s = SerenityAttachListener::listener();
+    if (s != -1) {
+      SerenityAttachListener::set_listener(-1);
+      ::shutdown(s, SHUT_RDWR);
+      ::close(s);
+    }
+    if (SerenityAttachListener::has_path()) {
+      ::unlink(SerenityAttachListener::path());
+      SerenityAttachListener::set_path(NULL);
+    }
+  }
+}
+
+// Initialization - create a listener socket and bind it to a file
+
+int SerenityAttachListener::init() {
+  char path[UNIX_PATH_MAX];          // socket file
+  char initial_path[UNIX_PATH_MAX];  // socket file during setup
+  int listener;                      // listener socket (file descriptor)
+
+  // register function to cleanup
+  if (!_atexit_registered) {
+    _atexit_registered = true;
+    ::atexit(listener_cleanup);
+  }
+
+  int n = snprintf(path, UNIX_PATH_MAX, "%s/.java_pid%d",
+                   os::get_temp_directory(), os::current_process_id());
+  if (n < (int)UNIX_PATH_MAX) {
+    n = snprintf(initial_path, UNIX_PATH_MAX, "%s.tmp", path);
+  }
+  if (n >= (int)UNIX_PATH_MAX) {
+    return -1;
+  }
+
+  // create the listener socket
+  listener = ::socket(PF_UNIX, SOCK_STREAM, 0);
+  if (listener == -1) {
+    return -1;
+  }
+
+  // bind socket
+  struct sockaddr_un addr;
+  memset((void *)&addr, 0, sizeof(addr));
+  addr.sun_family = AF_UNIX;
+  strcpy(addr.sun_path, initial_path);
+  ::unlink(initial_path);
+  int res = ::bind(listener, (struct sockaddr*)&addr, sizeof(addr));
+  if (res == -1) {
+    ::close(listener);
+    return -1;
+  }
+
+  // put in listen mode, set permissions, and rename into place
+  res = ::listen(listener, 5);
+  if (res == 0) {
+    RESTARTABLE(::chmod(initial_path, S_IREAD|S_IWRITE), res);
+    if (res == 0) {
+      // make sure the file is owned by the effective user and effective group
+      // e.g. the group could be inherited from the directory in case the s bit is set
+      RESTARTABLE(::chown(initial_path, geteuid(), getegid()), res);
+      if (res == 0) {
+        res = ::rename(initial_path, path);
+      }
+    }
+  }
+  if (res == -1) {
+    ::close(listener);
+    ::unlink(initial_path);
+    return -1;
+  }
+  set_path(path);
+  set_listener(listener);
+
+  return 0;
+}
+
+// Given a socket that is connected to a peer we read the request and
+// create an AttachOperation. As the socket is blocking there is potential
+// for a denial-of-service if the peer does not response. However this happens
+// after the peer credentials have been checked and in the worst case it just
+// means that the attach listener thread is blocked.
+//
+SerenityAttachOperation* SerenityAttachListener::read_request(int s) {
+  char ver_str[8];
+  sprintf(ver_str, "%d", ATTACH_PROTOCOL_VER);
+
+  // The request is a sequence of strings so we first figure out the
+  // expected count and the maximum possible length of the request.
+  // The request is:
+  //   <ver>0<cmd>0<arg>0<arg>0<arg>0
+  // where <ver> is the protocol version (1), <cmd> is the command
+  // name ("load", "datadump", ...), and <arg> is an argument
+  int expected_str_count = 2 + AttachOperation::arg_count_max;
+  const int max_len = (sizeof(ver_str) + 1) + (AttachOperation::name_length_max + 1) +
+    AttachOperation::arg_count_max*(AttachOperation::arg_length_max + 1);
+
+  char buf[max_len];
+  int str_count = 0;
+
+  // Read until all (expected) strings have been read, the buffer is
+  // full, or EOF.
+
+  int off = 0;
+  int left = max_len;
+
+  do {
+    int n;
+    RESTARTABLE(read(s, buf+off, left), n);
+    assert(n <= left, "buffer was too small, impossible!");
+    buf[max_len - 1] = '\0';
+    if (n == -1) {
+      return NULL;      // reset by peer or other error
+    }
+    if (n == 0) {
+      break;
+    }
+    for (int i=0; i<n; i++) {
+      if (buf[off+i] == 0) {
+        // EOS found
+        str_count++;
+
+        // The first string is <ver> so check it now to
+        // check for protocol mis-match
+        if (str_count == 1) {
+          if ((strlen(buf) != strlen(ver_str)) ||
+              (atoi(buf) != ATTACH_PROTOCOL_VER)) {
+            char msg[32];
+            sprintf(msg, "%d\n", ATTACH_ERROR_BADVERSION);
+            write_fully(s, msg, strlen(msg));
+            return NULL;
+          }
+        }
+      }
+    }
+    off += n;
+    left -= n;
+  } while (left > 0 && str_count < expected_str_count);
+
+  if (str_count != expected_str_count) {
+    return NULL;        // incomplete request
+  }
+
+  // parse request
+
+  ArgumentIterator args(buf, (max_len)-left);
+
+  // version already checked
+  char* v = args.next();
+
+  char* name = args.next();
+  if (name == NULL || strlen(name) > AttachOperation::name_length_max) {
+    return NULL;
+  }
+
+  SerenityAttachOperation* op = new SerenityAttachOperation(name);
+
+  for (int i=0; i<AttachOperation::arg_count_max; i++) {
+    char* arg = args.next();
+    if (arg == NULL) {
+      op->set_arg(i, NULL);
+    } else {
+      if (strlen(arg) > AttachOperation::arg_length_max) {
+        delete op;
+        return NULL;
+      }
+      op->set_arg(i, arg);
+    }
+  }
+
+  op->set_socket(s);
+  return op;
+}
+
+
+// Dequeue an operation
+//
+// In the Serenity implementation there is only a single operation and clients
+// cannot queue commands (except at the socket level).
+//
+SerenityAttachOperation* SerenityAttachListener::dequeue() {
+  for (;;) {
+    int s;
+
+    // wait for client to connect
+    struct sockaddr addr;
+    socklen_t len = sizeof(addr);
+    RESTARTABLE(::accept(listener(), &addr, &len), s);
+    if (s == -1) {
+      return NULL;      // log a warning?
+    }
+
+    // get the credentials of the peer and check the effective uid/guid
+    struct ucred cred_info;
+    socklen_t optlen = sizeof(cred_info);
+    if (::getsockopt(s, SOL_SOCKET, SO_PEERCRED, (void*)&cred_info, &optlen) == -1) {
+      log_debug(attach)("Failed to get socket option SO_PEERCRED");
+      ::close(s);
+      continue;
+    }
+
+    if (!os::Posix::matches_effective_uid_and_gid_or_root(cred_info.uid, cred_info.gid)) {
+      log_debug(attach)("euid/egid check failed (%d/%d vs %d/%d)",
+              cred_info.uid, cred_info.gid, geteuid(), getegid());
+      ::close(s);
+      continue;
+    }
+
+    // peer credential look okay so we read the request
+    SerenityAttachOperation* op = read_request(s);
+    if (op == NULL) {
+      ::close(s);
+      continue;
+    } else {
+      return op;
+    }
+  }
+}
+
+// write the given buffer to the socket
+int SerenityAttachListener::write_fully(int s, char* buf, int len) {
+  do {
+    int n = ::write(s, buf, len);
+    if (n == -1) {
+      if (errno != EINTR) return -1;
+    } else {
+      buf += n;
+      len -= n;
+    }
+  }
+  while (len > 0);
+  return 0;
+}
+
+// Complete an operation by sending the operation result and any result
+// output to the client. At this time the socket is in blocking mode so
+// potentially we can block if there is a lot of data and the client is
+// non-responsive. For most operations this is a non-issue because the
+// default send buffer is sufficient to buffer everything. In the future
+// if there are operations that involves a very big reply then it the
+// socket could be made non-blocking and a timeout could be used.
+
+void SerenityAttachOperation::complete(jint result, bufferedStream* st) {
+  JavaThread* thread = JavaThread::current();
+  ThreadBlockInVM tbivm(thread);
+
+  thread->set_suspend_equivalent();
+  // cleared by handle_special_suspend_equivalent_condition() or
+  // java_suspend_self() via check_and_wait_while_suspended()
+
+  // write operation result
+  char msg[32];
+  sprintf(msg, "%d\n", result);
+  int rc = SerenityAttachListener::write_fully(this->socket(), msg, strlen(msg));
+
+  // write any result data
+  if (rc == 0) {
+    SerenityAttachListener::write_fully(this->socket(), (char*) st->base(), st->size());
+    ::shutdown(this->socket(), 2);
+  }
+
+  // done
+  ::close(this->socket());
+
+  // were we externally suspended while we were waiting?
+  thread->check_and_wait_while_suspended();
+
+  delete this;
+}
+
+
+// AttachListener functions
+
+AttachOperation* AttachListener::dequeue() {
+  JavaThread* thread = JavaThread::current();
+  ThreadBlockInVM tbivm(thread);
+
+  thread->set_suspend_equivalent();
+  // cleared by handle_special_suspend_equivalent_condition() or
+  // java_suspend_self() via check_and_wait_while_suspended()
+
+  AttachOperation* op = SerenityAttachListener::dequeue();
+
+  // were we externally suspended while we were waiting?
+  thread->check_and_wait_while_suspended();
+
+  return op;
+}
+
+// Performs initialization at vm startup
+// For Serenity we remove any stale .java_pid file which could cause
+// an attaching process to think we are ready to receive on the
+// domain socket before we are properly initialized
+
+void AttachListener::vm_start() {
+  char fn[UNIX_PATH_MAX];
+  struct stat st;
+  int ret;
+
+  int n = snprintf(fn, UNIX_PATH_MAX, "%s/.java_pid%d",
+           os::get_temp_directory(), os::current_process_id());
+  assert(n < (int)UNIX_PATH_MAX, "java_pid file name buffer overflow");
+
+  RESTARTABLE(::stat(fn, &st), ret);
+  if (ret == 0) {
+    ret = ::unlink(fn);
+    if (ret == -1) {
+      log_debug(attach)("Failed to remove stale attach pid file at %s", fn);
+    }
+  }
+}
+
+int AttachListener::pd_init() {
+  JavaThread* thread = JavaThread::current();
+  ThreadBlockInVM tbivm(thread);
+
+  thread->set_suspend_equivalent();
+  // cleared by handle_special_suspend_equivalent_condition() or
+  // java_suspend_self() via check_and_wait_while_suspended()
+
+  int ret_code = SerenityAttachListener::init();
+
+  // were we externally suspended while we were waiting?
+  thread->check_and_wait_while_suspended();
+
+  return ret_code;
+}
+
+bool AttachListener::check_socket_file() {
+  int ret;
+  struct stat st;
+  ret = stat(SerenityAttachListener::path(), &st);
+  if (ret == -1) { // need to restart attach listener.
+    log_debug(attach)("Socket file %s does not exist - Restart Attach Listener",
+                      SerenityAttachListener::path());
+
+    listener_cleanup();
+
+    // wait to terminate current attach listener instance...
+    {
+      // avoid deadlock if AttachListener thread is blocked at safepoint
+      ThreadBlockInVM tbivm(JavaThread::current());
+      while (AttachListener::transit_state(AL_INITIALIZING,
+                                           AL_NOT_INITIALIZED) != AL_NOT_INITIALIZED) {
+        os::naked_yield();
+      }
+    }
+    return is_init_trigger();
+  }
+  return false;
+}
+
+// Attach Listener is started lazily except in the case when
+// +ReduseSignalUsage is used
+bool AttachListener::init_at_startup() {
+  if (ReduceSignalUsage) {
+    return true;
+  } else {
+    return false;
+  }
+}
+
+// If the file .attach_pid<pid> exists in the working directory
+// or /tmp then this is the trigger to start the attach mechanism
+bool AttachListener::is_init_trigger() {
+  if (init_at_startup() || is_initialized()) {
+    return false;               // initialized at startup or already initialized
+  }
+  char fn[PATH_MAX + 1];
+  int ret;
+  struct stat st;
+  sprintf(fn, ".attach_pid%d", os::current_process_id());
+  RESTARTABLE(::stat(fn, &st), ret);
+  if (ret == -1) {
+    log_trace(attach)("Failed to find attach file: %s, trying alternate", fn);
+    snprintf(fn, sizeof(fn), "%s/.attach_pid%d",
+             os::get_temp_directory(), os::current_process_id());
+    RESTARTABLE(::stat(fn, &st), ret);
+    if (ret == -1) {
+      log_debug(attach)("Failed to find attach file: %s", fn);
+    }
+  }
+  if (ret == 0) {
+    // simple check to avoid starting the attach mechanism when
+    // a bogus non-root user creates the file
+    if (os::Posix::matches_effective_uid_or_root(st.st_uid)) {
+      init();
+      log_trace(attach)("Attach triggered by %s", fn);
+      return true;
+    } else {
+      log_debug(attach)("File %s has wrong user id %d (vs %d). Attach is not triggered", fn, st.st_uid, geteuid());
+    }
+  }
+  return false;
+}
+
+// if VM aborts then remove listener
+void AttachListener::abort() {
+  listener_cleanup();
+}
+
+void AttachListener::pd_data_dump() {
+  os::signal_notify(SIGQUIT);
+}
+
+AttachOperationFunctionInfo* AttachListener::pd_find_operation(const char* n) {
+  return NULL;
+}
+
+jint AttachListener::pd_set_flag(AttachOperation* op, outputStream* out) {
+  out->print_cr("flag '%s' cannot be changed", op->arg(0));
+  return JNI_ERR;
+}
+
+void AttachListener::pd_detachall() {
+  // do nothing for now
+}
diff --git a/src/hotspot/os/serenity/c1_globals_serenity.hpp b/src/hotspot/os/serenity/c1_globals_serenity.hpp
new file mode 100644
index 00000000000..ff831f74b0e
--- /dev/null
+++ b/src/hotspot/os/serenity/c1_globals_serenity.hpp
@@ -0,0 +1,36 @@
+/*
+ * Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#ifndef OS_SERENITY_C1_GLOBALS_SERENITY_HPP
+#define OS_SERENITY_C1_GLOBALS_SERENITY_HPP
+
+#include "utilities/globalDefinitions.hpp"
+#include "utilities/macros.hpp"
+
+//
+// Sets the default values for operating system dependent flags used by the
+// client compiler. (see c1_globals.hpp)
+//
+
+#endif // OS_SERENITY_C1_GLOBALS_SERENITY_HPP
diff --git a/src/hotspot/os/serenity/c2_globals_serenity.hpp b/src/hotspot/os/serenity/c2_globals_serenity.hpp
new file mode 100644
index 00000000000..d9e708703fb
--- /dev/null
+++ b/src/hotspot/os/serenity/c2_globals_serenity.hpp
@@ -0,0 +1,36 @@
+/*
+ * Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#ifndef OS_SERENITY_C2_GLOBALS_SERENITY_HPP
+#define OS_SERENITY_C2_GLOBALS_SERENITY_HPP
+
+#include "utilities/globalDefinitions.hpp"
+#include "utilities/macros.hpp"
+
+//
+// Sets the default values for operating system dependent flags used by the
+// server compiler. (see c2_globals.hpp)
+//
+
+#endif // OS_SERENITY_C2_GLOBALS_SERENITY_HPP
diff --git a/src/hotspot/os/serenity/cgroupSubsystem_serenity.cpp b/src/hotspot/os/serenity/cgroupSubsystem_serenity.cpp
new file mode 100644
index 00000000000..db6adbfaac5
--- /dev/null
+++ b/src/hotspot/os/serenity/cgroupSubsystem_serenity.cpp
@@ -0,0 +1,516 @@
+/*
+ * Copyright (c) 2019, 2020, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#include <string.h>
+#include <math.h>
+#include <errno.h>
+#include "cgroupSubsystem_serenity.hpp"
+#include "cgroupV1Subsystem_serenity.hpp"
+#include "cgroupV2Subsystem_serenity.hpp"
+#include "logging/log.hpp"
+#include "memory/allocation.hpp"
+#include "runtime/globals.hpp"
+#include "runtime/os.hpp"
+#include "utilities/globalDefinitions.hpp"
+
+CgroupSubsystem* CgroupSubsystemFactory::create() {
+  CgroupV1MemoryController* memory = NULL;
+  CgroupV1Controller* cpuset = NULL;
+  CgroupV1Controller* cpu = NULL;
+  CgroupV1Controller* cpuacct = NULL;
+  CgroupInfo cg_infos[CG_INFO_LENGTH];
+  u1 cg_type_flags = INVALID_CGROUPS_GENERIC;
+  const char* proc_cgroups = "/proc/cgroups";
+  const char* proc_self_cgroup = "/proc/self/cgroup";
+  const char* proc_self_mountinfo = "/proc/self/mountinfo";
+
+  bool valid_cgroup = determine_type(cg_infos, proc_cgroups, proc_self_cgroup, proc_self_mountinfo, &cg_type_flags);
+
+  if (!valid_cgroup) {
+    // Could not detect cgroup type
+    return NULL;
+  }
+  assert(is_valid_cgroup(&cg_type_flags), "Expected valid cgroup type");
+
+  if (is_cgroup_v2(&cg_type_flags)) {
+    // Cgroups v2 case, we have all the info we need.
+    // Construct the subsystem, free resources and return
+    // Note: any index in cg_infos will do as the path is the same for
+    //       all controllers.
+    CgroupController* unified = new CgroupV2Controller(cg_infos[MEMORY_IDX]._mount_path, cg_infos[MEMORY_IDX]._cgroup_path);
+    log_debug(os, container)("Detected cgroups v2 unified hierarchy");
+    cleanup(cg_infos);
+    return new CgroupV2Subsystem(unified);
+  }
+
+  /*
+   * Cgroup v1 case:
+   *
+   * Use info gathered previously from /proc/self/cgroup
+   * and map host mount point to
+   * local one via /proc/self/mountinfo content above
+   *
+   * Docker example:
+   * 5:memory:/docker/6558aed8fc662b194323ceab5b964f69cf36b3e8af877a14b80256e93aecb044
+   *
+   * Host example:
+   * 5:memory:/user.slice
+   *
+   * Construct a path to the process specific memory and cpuset
+   * cgroup directory.
+   *
+   * For a container running under Docker from memory example above
+   * the paths would be:
+   *
+   * /sys/fs/cgroup/memory
+   *
+   * For a Host from memory example above the path would be:
+   *
+   * /sys/fs/cgroup/memory/user.slice
+   *
+   */
+  assert(is_cgroup_v1(&cg_type_flags), "Cgroup v1 expected");
+  for (int i = 0; i < CG_INFO_LENGTH; i++) {
+    CgroupInfo info = cg_infos[i];
+    if (strcmp(info._name, "memory") == 0) {
+      memory = new CgroupV1MemoryController(info._root_mount_path, info._mount_path);
+      memory->set_subsystem_path(info._cgroup_path);
+    } else if (strcmp(info._name, "cpuset") == 0) {
+      cpuset = new CgroupV1Controller(info._root_mount_path, info._mount_path);
+      cpuset->set_subsystem_path(info._cgroup_path);
+    } else if (strcmp(info._name, "cpu") == 0) {
+      cpu = new CgroupV1Controller(info._root_mount_path, info._mount_path);
+      cpu->set_subsystem_path(info._cgroup_path);
+    } else if (strcmp(info._name, "cpuacct") == 0) {
+      cpuacct = new CgroupV1Controller(info._root_mount_path, info._mount_path);
+      cpuacct->set_subsystem_path(info._cgroup_path);
+    }
+  }
+  cleanup(cg_infos);
+  return new CgroupV1Subsystem(cpuset, cpu, cpuacct, memory);
+}
+
+bool CgroupSubsystemFactory::determine_type(CgroupInfo* cg_infos,
+                                            const char* proc_cgroups,
+                                            const char* proc_self_cgroup,
+                                            const char* proc_self_mountinfo,
+                                            u1* flags) {
+  FILE *mntinfo = NULL;
+  FILE *cgroups = NULL;
+  FILE *cgroup = NULL;
+  char buf[MAXPATHLEN+1];
+  char *p;
+  bool is_cgroupsV2;
+  // true iff all controllers, memory, cpu, cpuset, cpuacct are enabled
+  // at the kernel level.
+  bool all_controllers_enabled;
+
+  /*
+   * Read /proc/cgroups so as to be able to distinguish cgroups v2 vs cgroups v1.
+   *
+   * For cgroups v1 hierarchy (hybrid or legacy), cpu, cpuacct, cpuset, memory controllers
+   * must have non-zero for the hierarchy ID field and relevant controllers mounted.
+   * Conversely, for cgroups v2 (unified hierarchy), cpu, cpuacct, cpuset, memory
+   * controllers must have hierarchy ID 0 and the unified controller mounted.
+   */
+  cgroups = fopen(proc_cgroups, "r");
+  if (cgroups == NULL) {
+      log_debug(os, container)("Can't open %s, %s",
+                               proc_cgroups, os::strerror(errno));
+      *flags = INVALID_CGROUPS_GENERIC;
+      return false;
+  }
+
+  while ((p = fgets(buf, MAXPATHLEN, cgroups)) != NULL) {
+    char name[MAXPATHLEN+1];
+    int  hierarchy_id;
+    int  enabled;
+
+    // Format of /proc/cgroups documented via man 7 cgroups
+    if (sscanf(p, "%s %d %*d %d", name, &hierarchy_id, &enabled) != 3) {
+      continue;
+    }
+    if (strcmp(name, "memory") == 0) {
+      cg_infos[MEMORY_IDX]._name = os::strdup(name);
+      cg_infos[MEMORY_IDX]._hierarchy_id = hierarchy_id;
+      cg_infos[MEMORY_IDX]._enabled = (enabled == 1);
+    } else if (strcmp(name, "cpuset") == 0) {
+      cg_infos[CPUSET_IDX]._name = os::strdup(name);
+      cg_infos[CPUSET_IDX]._hierarchy_id = hierarchy_id;
+      cg_infos[CPUSET_IDX]._enabled = (enabled == 1);
+    } else if (strcmp(name, "cpu") == 0) {
+      cg_infos[CPU_IDX]._name = os::strdup(name);
+      cg_infos[CPU_IDX]._hierarchy_id = hierarchy_id;
+      cg_infos[CPU_IDX]._enabled = (enabled == 1);
+    } else if (strcmp(name, "cpuacct") == 0) {
+      cg_infos[CPUACCT_IDX]._name = os::strdup(name);
+      cg_infos[CPUACCT_IDX]._hierarchy_id = hierarchy_id;
+      cg_infos[CPUACCT_IDX]._enabled = (enabled == 1);
+    }
+  }
+  fclose(cgroups);
+
+  is_cgroupsV2 = true;
+  all_controllers_enabled = true;
+  for (int i = 0; i < CG_INFO_LENGTH; i++) {
+    is_cgroupsV2 = is_cgroupsV2 && cg_infos[i]._hierarchy_id == 0;
+    all_controllers_enabled = all_controllers_enabled && cg_infos[i]._enabled;
+  }
+
+  if (!all_controllers_enabled) {
+    // one or more controllers disabled, disable container support
+    log_debug(os, container)("One or more required controllers disabled at kernel level.");
+    cleanup(cg_infos);
+    *flags = INVALID_CGROUPS_GENERIC;
+    return false;
+  }
+
+  /*
+   * Read /proc/self/cgroup and determine:
+   *  - the cgroup path for cgroups v2 or
+   *  - on a cgroups v1 system, collect info for mapping
+   *    the host mount point to the local one via /proc/self/mountinfo below.
+   */
+  cgroup = fopen(proc_self_cgroup, "r");
+  if (cgroup == NULL) {
+    log_debug(os, container)("Can't open %s, %s",
+                             proc_self_cgroup, os::strerror(errno));
+    cleanup(cg_infos);
+    *flags = INVALID_CGROUPS_GENERIC;
+    return false;
+  }
+
+  while ((p = fgets(buf, MAXPATHLEN, cgroup)) != NULL) {
+    char *controllers;
+    char *token;
+    char *hierarchy_id_str;
+    int  hierarchy_id;
+    char *cgroup_path;
+
+    hierarchy_id_str = strsep(&p, ":");
+    hierarchy_id = atoi(hierarchy_id_str);
+    /* Get controllers and base */
+    controllers = strsep(&p, ":");
+    cgroup_path = strsep(&p, "\n");
+
+    if (controllers == NULL) {
+      continue;
+    }
+
+    while (!is_cgroupsV2 && (token = strsep(&controllers, ",")) != NULL) {
+      if (strcmp(token, "memory") == 0) {
+        assert(hierarchy_id == cg_infos[MEMORY_IDX]._hierarchy_id, "/proc/cgroups and /proc/self/cgroup hierarchy mismatch");
+        cg_infos[MEMORY_IDX]._cgroup_path = os::strdup(cgroup_path);
+      } else if (strcmp(token, "cpuset") == 0) {
+        assert(hierarchy_id == cg_infos[CPUSET_IDX]._hierarchy_id, "/proc/cgroups and /proc/self/cgroup hierarchy mismatch");
+        cg_infos[CPUSET_IDX]._cgroup_path = os::strdup(cgroup_path);
+      } else if (strcmp(token, "cpu") == 0) {
+        assert(hierarchy_id == cg_infos[CPU_IDX]._hierarchy_id, "/proc/cgroups and /proc/self/cgroup hierarchy mismatch");
+        cg_infos[CPU_IDX]._cgroup_path = os::strdup(cgroup_path);
+      } else if (strcmp(token, "cpuacct") == 0) {
+        assert(hierarchy_id == cg_infos[CPUACCT_IDX]._hierarchy_id, "/proc/cgroups and /proc/self/cgroup hierarchy mismatch");
+        cg_infos[CPUACCT_IDX]._cgroup_path = os::strdup(cgroup_path);
+      }
+    }
+    if (is_cgroupsV2) {
+      for (int i = 0; i < CG_INFO_LENGTH; i++) {
+        cg_infos[i]._cgroup_path = os::strdup(cgroup_path);
+      }
+    }
+  }
+  fclose(cgroup);
+
+  // Find various mount points by reading /proc/self/mountinfo
+  // mountinfo format is documented at https://www.kernel.org/doc/Documentation/filesystems/proc.txt
+  mntinfo = fopen(proc_self_mountinfo, "r");
+  if (mntinfo == NULL) {
+      log_debug(os, container)("Can't open %s, %s",
+                               proc_self_mountinfo, os::strerror(errno));
+      cleanup(cg_infos);
+      *flags = INVALID_CGROUPS_GENERIC;
+      return false;
+  }
+
+  bool cgroupv2_mount_point_found = false;
+  bool any_cgroup_mounts_found = false;
+  while ((p = fgets(buf, MAXPATHLEN, mntinfo)) != NULL) {
+    char tmp_mount_point[MAXPATHLEN+1];
+    char tmp_fs_type[MAXPATHLEN+1];
+    char tmproot[MAXPATHLEN+1];
+    char tmpmount[MAXPATHLEN+1];
+    char tmpcgroups[MAXPATHLEN+1];
+    char *cptr = tmpcgroups;
+    char *token;
+
+    // Cgroup v2 relevant info. We only look for the _mount_path iff is_cgroupsV2 so
+    // as to avoid memory stomping of the _mount_path pointer later on in the cgroup v1
+    // block in the hybrid case.
+    //
+    if (is_cgroupsV2 && sscanf(p, "%*d %*d %*d:%*d %*s %s %*[^-]- %s %*s %*s", tmp_mount_point, tmp_fs_type) == 2) {
+      // we likely have an early match return (e.g. cgroup fs match), be sure we have cgroup2 as fstype
+      if (!cgroupv2_mount_point_found && strcmp("cgroup2", tmp_fs_type) == 0) {
+        cgroupv2_mount_point_found = true;
+        any_cgroup_mounts_found = true;
+        for (int i = 0; i < CG_INFO_LENGTH; i++) {
+          assert(cg_infos[i]._mount_path == NULL, "_mount_path memory stomping");
+          cg_infos[i]._mount_path = os::strdup(tmp_mount_point);
+        }
+      }
+    }
+
+    /* Cgroup v1 relevant info
+     *
+     * Find the cgroup mount point for memory, cpuset, cpu, cpuacct
+     *
+     * Example for docker:
+     * 219 214 0:29 /docker/7208cebd00fa5f2e342b1094f7bed87fa25661471a4637118e65f1c995be8a34 /sys/fs/cgroup/memory ro,nosuid,nodev,noexec,relatime - cgroup cgroup rw,memory
+     *
+     * Example for host:
+     * 34 28 0:29 / /sys/fs/cgroup/memory rw,nosuid,nodev,noexec,relatime shared:16 - cgroup cgroup rw,memory
+     */
+    if (sscanf(p, "%*d %*d %*d:%*d %s %s %*[^-]- %s %*s %s", tmproot, tmpmount, tmp_fs_type, tmpcgroups) == 4) {
+      if (strcmp("cgroup", tmp_fs_type) != 0) {
+        // Skip cgroup2 fs lines on hybrid or unified hierarchy.
+        continue;
+      }
+      while ((token = strsep(&cptr, ",")) != NULL) {
+        if (strcmp(token, "memory") == 0) {
+          any_cgroup_mounts_found = true;
+          assert(cg_infos[MEMORY_IDX]._mount_path == NULL, "stomping of _mount_path");
+          cg_infos[MEMORY_IDX]._mount_path = os::strdup(tmpmount);
+          cg_infos[MEMORY_IDX]._root_mount_path = os::strdup(tmproot);
+          cg_infos[MEMORY_IDX]._data_complete = true;
+        } else if (strcmp(token, "cpuset") == 0) {
+          any_cgroup_mounts_found = true;
+          if (cg_infos[CPUSET_IDX]._mount_path != NULL) {
+            // On some systems duplicate cpuset controllers get mounted in addition to
+            // the main cgroup controllers most likely under /sys/fs/cgroup. In that
+            // case pick the one under /sys/fs/cgroup and discard others.
+            if (strstr(cg_infos[CPUSET_IDX]._mount_path, "/sys/fs/cgroup") != cg_infos[CPUSET_IDX]._mount_path) {
+              log_warning(os, container)("Duplicate cpuset controllers detected. Picking %s, skipping %s.",
+                                         tmpmount, cg_infos[CPUSET_IDX]._mount_path);
+              os::free(cg_infos[CPUSET_IDX]._mount_path);
+              cg_infos[CPUSET_IDX]._mount_path = os::strdup(tmpmount);
+            } else {
+              log_warning(os, container)("Duplicate cpuset controllers detected. Picking %s, skipping %s.",
+                                         cg_infos[CPUSET_IDX]._mount_path, tmpmount);
+            }
+          } else {
+            cg_infos[CPUSET_IDX]._mount_path = os::strdup(tmpmount);
+          }
+          cg_infos[CPUSET_IDX]._root_mount_path = os::strdup(tmproot);
+          cg_infos[CPUSET_IDX]._data_complete = true;
+        } else if (strcmp(token, "cpu") == 0) {
+          any_cgroup_mounts_found = true;
+          assert(cg_infos[CPU_IDX]._mount_path == NULL, "stomping of _mount_path");
+          cg_infos[CPU_IDX]._mount_path = os::strdup(tmpmount);
+          cg_infos[CPU_IDX]._root_mount_path = os::strdup(tmproot);
+          cg_infos[CPU_IDX]._data_complete = true;
+        } else if (strcmp(token, "cpuacct") == 0) {
+          any_cgroup_mounts_found = true;
+          assert(cg_infos[CPUACCT_IDX]._mount_path == NULL, "stomping of _mount_path");
+          cg_infos[CPUACCT_IDX]._mount_path = os::strdup(tmpmount);
+          cg_infos[CPUACCT_IDX]._root_mount_path = os::strdup(tmproot);
+          cg_infos[CPUACCT_IDX]._data_complete = true;
+        }
+      }
+    }
+  }
+  fclose(mntinfo);
+
+  // Neither cgroup2 nor cgroup filesystems mounted via /proc/self/mountinfo
+  // No point in continuing.
+  if (!any_cgroup_mounts_found) {
+    log_trace(os, container)("No relevant cgroup controllers mounted.");
+    cleanup(cg_infos);
+    *flags = INVALID_CGROUPS_NO_MOUNT;
+    return false;
+  }
+
+  if (is_cgroupsV2) {
+    if (!cgroupv2_mount_point_found) {
+      log_trace(os, container)("Mount point for cgroupv2 not found in /proc/self/mountinfo");
+      cleanup(cg_infos);
+      *flags = INVALID_CGROUPS_V2;
+      return false;
+    }
+    // Cgroups v2 case, we have all the info we need.
+    *flags = CGROUPS_V2;
+    return true;
+  }
+
+  // What follows is cgroups v1
+  log_debug(os, container)("Detected cgroups hybrid or legacy hierarchy, using cgroups v1 controllers");
+
+  if (!cg_infos[MEMORY_IDX]._data_complete) {
+    log_debug(os, container)("Required cgroup v1 memory subsystem not found");
+    cleanup(cg_infos);
+    *flags = INVALID_CGROUPS_V1;
+    return false;
+  }
+  if (!cg_infos[CPUSET_IDX]._data_complete) {
+    log_debug(os, container)("Required cgroup v1 cpuset subsystem not found");
+    cleanup(cg_infos);
+    *flags = INVALID_CGROUPS_V1;
+    return false;
+  }
+  if (!cg_infos[CPU_IDX]._data_complete) {
+    log_debug(os, container)("Required cgroup v1 cpu subsystem not found");
+    cleanup(cg_infos);
+    *flags = INVALID_CGROUPS_V1;
+    return false;
+  }
+  if (!cg_infos[CPUACCT_IDX]._data_complete) {
+    log_debug(os, container)("Required cgroup v1 cpuacct subsystem not found");
+    cleanup(cg_infos);
+    *flags = INVALID_CGROUPS_V1;
+    return false;
+  }
+  // Cgroups v1 case, we have all the info we need.
+  *flags = CGROUPS_V1;
+  return true;
+
+};
+
+void CgroupSubsystemFactory::cleanup(CgroupInfo* cg_infos) {
+  assert(cg_infos != NULL, "Invariant");
+  for (int i = 0; i < CG_INFO_LENGTH; i++) {
+    os::free(cg_infos[i]._name);
+    os::free(cg_infos[i]._cgroup_path);
+    os::free(cg_infos[i]._root_mount_path);
+    os::free(cg_infos[i]._mount_path);
+  }
+}
+
+/* active_processor_count
+ *
+ * Calculate an appropriate number of active processors for the
+ * VM to use based on these three inputs.
+ *
+ * cpu affinity
+ * cgroup cpu quota & cpu period
+ * cgroup cpu shares
+ *
+ * Algorithm:
+ *
+ * Determine the number of available CPUs from sched_getaffinity
+ *
+ * If user specified a quota (quota != -1), calculate the number of
+ * required CPUs by dividing quota by period.
+ *
+ * If shares are in effect (shares != -1), calculate the number
+ * of CPUs required for the shares by dividing the share value
+ * by PER_CPU_SHARES.
+ *
+ * All results of division are rounded up to the next whole number.
+ *
+ * If neither shares or quotas have been specified, return the
+ * number of active processors in the system.
+ *
+ * If both shares and quotas have been specified, the results are
+ * based on the flag PreferContainerQuotaForCPUCount.  If true,
+ * return the quota value.  If false return the smallest value
+ * between shares or quotas.
+ *
+ * If shares and/or quotas have been specified, the resulting number
+ * returned will never exceed the number of active processors.
+ *
+ * return:
+ *    number of CPUs
+ */
+int CgroupSubsystem::active_processor_count() {
+  int quota_count = 0, share_count = 0;
+  int cpu_count, limit_count;
+  int result;
+
+  // We use a cache with a timeout to avoid performing expensive
+  // computations in the event this function is called frequently.
+  // [See 8227006].
+  CachingCgroupController* contrl = cpu_controller();
+  CachedMetric* cpu_limit = contrl->metrics_cache();
+  if (!cpu_limit->should_check_metric()) {
+    int val = (int)cpu_limit->value();
+    log_trace(os, container)("CgroupSubsystem::active_processor_count (cached): %d", val);
+    return val;
+  }
+
+  cpu_count = limit_count = os::Serenity::active_processor_count();
+  int quota  = cpu_quota();
+  int period = cpu_period();
+  int share  = cpu_shares();
+
+  if (quota > -1 && period > 0) {
+    quota_count = ceilf((float)quota / (float)period);
+    log_trace(os, container)("CPU Quota count based on quota/period: %d", quota_count);
+  }
+  if (share > -1) {
+    share_count = ceilf((float)share / (float)PER_CPU_SHARES);
+    log_trace(os, container)("CPU Share count based on shares: %d", share_count);
+  }
+
+  // If both shares and quotas are setup results depend
+  // on flag PreferContainerQuotaForCPUCount.
+  // If true, limit CPU count to quota
+  // If false, use minimum of shares and quotas
+  if (quota_count !=0 && share_count != 0) {
+    if (PreferContainerQuotaForCPUCount) {
+      limit_count = quota_count;
+    } else {
+      limit_count = MIN2(quota_count, share_count);
+    }
+  } else if (quota_count != 0) {
+    limit_count = quota_count;
+  } else if (share_count != 0) {
+    limit_count = share_count;
+  }
+
+  result = MIN2(cpu_count, limit_count);
+  log_trace(os, container)("OSContainer::active_processor_count: %d", result);
+
+  // Update cached metric to avoid re-reading container settings too often
+  cpu_limit->set_value(result, OSCONTAINER_CACHE_TIMEOUT);
+
+  return result;
+}
+
+/* memory_limit_in_bytes
+ *
+ * Return the limit of available memory for this process.
+ *
+ * return:
+ *    memory limit in bytes or
+ *    -1 for unlimited
+ *    OSCONTAINER_ERROR for not supported
+ */
+jlong CgroupSubsystem::memory_limit_in_bytes() {
+  CachingCgroupController* contrl = memory_controller();
+  CachedMetric* memory_limit = contrl->metrics_cache();
+  if (!memory_limit->should_check_metric()) {
+    return memory_limit->value();
+  }
+  jlong mem_limit = read_memory_limit_in_bytes();
+  // Update cached metric to avoid re-reading container settings too often
+  memory_limit->set_value(mem_limit, OSCONTAINER_CACHE_TIMEOUT);
+  return mem_limit;
+}
diff --git a/src/hotspot/os/serenity/cgroupSubsystem_serenity.hpp b/src/hotspot/os/serenity/cgroupSubsystem_serenity.hpp
new file mode 100644
index 00000000000..343553d129d
--- /dev/null
+++ b/src/hotspot/os/serenity/cgroupSubsystem_serenity.hpp
@@ -0,0 +1,315 @@
+/*
+ * Copyright (c) 2019, 2020, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#ifndef CGROUP_SUBSYSTEM_SERENITY_HPP
+#define CGROUP_SUBSYSTEM_SERENITY_HPP
+
+#include "memory/allocation.hpp"
+#include "runtime/os.hpp"
+#include "logging/log.hpp"
+#include "utilities/globalDefinitions.hpp"
+#include "utilities/macros.hpp"
+#include "osContainer_serenity.hpp"
+
+// Shared cgroups code (used by cgroup version 1 and version 2)
+
+/*
+ * PER_CPU_SHARES has been set to 1024 because CPU shares' quota
+ * is commonly used in cloud frameworks like Kubernetes[1],
+ * AWS[2] and Mesos[3] in a similar way. They spawn containers with
+ * --cpu-shares option values scaled by PER_CPU_SHARES. Thus, we do
+ * the inverse for determining the number of possible available
+ * CPUs to the JVM inside a container. See JDK-8216366.
+ *
+ * [1] https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#meaning-of-cpu
+ *     In particular:
+ *        When using Docker:
+ *          The spec.containers[].resources.requests.cpu is converted to its core value, which is potentially
+ *          fractional, and multiplied by 1024. The greater of this number or 2 is used as the value of the
+ *          --cpu-shares flag in the docker run command.
+ * [2] https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_ContainerDefinition.html
+ * [3] https://github.com/apache/mesos/blob/3478e344fb77d931f6122980c6e94cd3913c441d/src/docker/docker.cpp#L648
+ *     https://github.com/apache/mesos/blob/3478e344fb77d931f6122980c6e94cd3913c441d/src/slave/containerizer/mesos/isolators/cgroups/constants.hpp#L30
+ */
+#define PER_CPU_SHARES 1024
+
+#define CGROUPS_V1               1
+#define CGROUPS_V2               2
+#define INVALID_CGROUPS_V2       3
+#define INVALID_CGROUPS_V1       4
+#define INVALID_CGROUPS_NO_MOUNT 5
+#define INVALID_CGROUPS_GENERIC  6
+
+// Four controllers: cpu, cpuset, cpuacct, memory
+#define CG_INFO_LENGTH 4
+#define CPUSET_IDX     0
+#define CPU_IDX        1
+#define CPUACCT_IDX    2
+#define MEMORY_IDX     3
+
+typedef char * cptr;
+
+class CgroupController: public CHeapObj<mtInternal> {
+  public:
+    virtual char *subsystem_path() = 0;
+};
+
+PRAGMA_DIAG_PUSH
+PRAGMA_FORMAT_NONLITERAL_IGNORED
+template <typename T> int subsystem_file_line_contents(CgroupController* c,
+                                              const char *filename,
+                                              const char *matchline,
+                                              const char *scan_fmt,
+                                              T returnval) {
+  FILE *fp = NULL;
+  char *p;
+  char file[MAXPATHLEN+1];
+  char buf[MAXPATHLEN+1];
+  char discard[MAXPATHLEN+1];
+  bool found_match = false;
+
+  if (c == NULL) {
+    log_debug(os, container)("subsystem_file_line_contents: CgroupController* is NULL");
+    return OSCONTAINER_ERROR;
+  }
+  if (c->subsystem_path() == NULL) {
+    log_debug(os, container)("subsystem_file_line_contents: subsystem path is NULL");
+    return OSCONTAINER_ERROR;
+  }
+
+  strncpy(file, c->subsystem_path(), MAXPATHLEN);
+  file[MAXPATHLEN-1] = '\0';
+  int filelen = strlen(file);
+  if ((filelen + strlen(filename)) > (MAXPATHLEN-1)) {
+    log_debug(os, container)("File path too long %s, %s", file, filename);
+    return OSCONTAINER_ERROR;
+  }
+  strncat(file, filename, MAXPATHLEN-filelen);
+  log_trace(os, container)("Path to %s is %s", filename, file);
+  fp = fopen(file, "r");
+  if (fp != NULL) {
+    int err = 0;
+    while ((p = fgets(buf, MAXPATHLEN, fp)) != NULL) {
+      found_match = false;
+      if (matchline == NULL) {
+        // single-line file case
+        int matched = sscanf(p, scan_fmt, returnval);
+        found_match = (matched == 1);
+      } else {
+        // multi-line file case
+        if (strstr(p, matchline) != NULL) {
+          // discard matchline string prefix
+          int matched = sscanf(p, scan_fmt, discard, returnval);
+          found_match = (matched == 2);
+        } else {
+          continue; // substring not found
+        }
+      }
+      if (found_match) {
+        fclose(fp);
+        return 0;
+      } else {
+        err = 1;
+        log_debug(os, container)("Type %s not found in file %s", scan_fmt, file);
+      }
+    }
+    if (err == 0) {
+      log_debug(os, container)("Empty file %s", file);
+    }
+  } else {
+    log_debug(os, container)("Open of file %s failed, %s", file, os::strerror(errno));
+  }
+  if (fp != NULL)
+    fclose(fp);
+  return OSCONTAINER_ERROR;
+}
+PRAGMA_DIAG_POP
+
+#define GET_CONTAINER_INFO(return_type, subsystem, filename,              \
+                           logstring, scan_fmt, variable)                 \
+  return_type variable;                                                   \
+{                                                                         \
+  int err;                                                                \
+  err = subsystem_file_line_contents(subsystem,                           \
+                                     filename,                            \
+                                     NULL,                                \
+                                     scan_fmt,                            \
+                                     &variable);                          \
+  if (err != 0)                                                           \
+    return (return_type) OSCONTAINER_ERROR;                               \
+                                                                          \
+  log_trace(os, container)(logstring, variable);                          \
+}
+
+#define GET_CONTAINER_INFO_CPTR(return_type, subsystem, filename,         \
+                               logstring, scan_fmt, variable, bufsize)    \
+  char variable[bufsize];                                                 \
+{                                                                         \
+  int err;                                                                \
+  err = subsystem_file_line_contents(subsystem,                           \
+                                     filename,                            \
+                                     NULL,                                \
+                                     scan_fmt,                            \
+                                     variable);                           \
+  if (err != 0)                                                           \
+    return (return_type) NULL;                                            \
+                                                                          \
+  log_trace(os, container)(logstring, variable);                          \
+}
+
+#define GET_CONTAINER_INFO_LINE(return_type, controller, filename,        \
+                           matchline, logstring, scan_fmt, variable)      \
+  return_type variable;                                                   \
+{                                                                         \
+  int err;                                                                \
+  err = subsystem_file_line_contents(controller,                          \
+                                filename,                                 \
+                                matchline,                                \
+                                scan_fmt,                                 \
+                                &variable);                               \
+  if (err != 0)                                                           \
+    return (return_type) OSCONTAINER_ERROR;                               \
+                                                                          \
+  log_trace(os, container)(logstring, variable);                          \
+}
+
+
+class CachedMetric : public CHeapObj<mtInternal>{
+  private:
+    volatile jlong _metric;
+    volatile jlong _next_check_counter;
+  public:
+    CachedMetric() {
+      _metric = -1;
+      _next_check_counter = min_jlong;
+    }
+    bool should_check_metric() {
+      return os::elapsed_counter() > _next_check_counter;
+    }
+    jlong value() { return _metric; }
+    void set_value(jlong value, jlong timeout) {
+      _metric = value;
+      // Metric is unlikely to change, but we want to remain
+      // responsive to configuration changes. A very short grace time
+      // between re-read avoids excessive overhead during startup without
+      // significantly reducing the VMs ability to promptly react to changed
+      // metric config
+      _next_check_counter = os::elapsed_counter() + timeout;
+    }
+};
+
+class CachingCgroupController : public CHeapObj<mtInternal> {
+  private:
+    CgroupController* _controller;
+    CachedMetric* _metrics_cache;
+
+  public:
+    CachingCgroupController(CgroupController* cont) {
+      _controller = cont;
+      _metrics_cache = new CachedMetric();
+    }
+
+    CachedMetric* metrics_cache() { return _metrics_cache; }
+    CgroupController* controller() { return _controller; }
+};
+
+class CgroupSubsystem: public CHeapObj<mtInternal> {
+  public:
+    jlong memory_limit_in_bytes();
+    int active_processor_count();
+
+    virtual int cpu_quota() = 0;
+    virtual int cpu_period() = 0;
+    virtual int cpu_shares() = 0;
+    virtual jlong memory_usage_in_bytes() = 0;
+    virtual jlong memory_and_swap_limit_in_bytes() = 0;
+    virtual jlong memory_soft_limit_in_bytes() = 0;
+    virtual jlong memory_max_usage_in_bytes() = 0;
+    virtual char * cpu_cpuset_cpus() = 0;
+    virtual char * cpu_cpuset_memory_nodes() = 0;
+    virtual jlong read_memory_limit_in_bytes() = 0;
+    virtual const char * container_type() = 0;
+    virtual CachingCgroupController* memory_controller() = 0;
+    virtual CachingCgroupController* cpu_controller() = 0;
+};
+
+// Utility class for storing info retrieved from /proc/cgroups,
+// /proc/self/cgroup and /proc/self/mountinfo
+// For reference see man 7 cgroups and CgroupSubsystemFactory
+class CgroupInfo : public StackObj {
+  friend class CgroupSubsystemFactory;
+  friend class WhiteBox;
+
+  private:
+    char* _name;
+    int _hierarchy_id;
+    bool _enabled;
+    bool _data_complete;    // indicating cgroup v1 data is complete for this controller
+    char* _cgroup_path;     // cgroup controller path from /proc/self/cgroup
+    char* _root_mount_path; // root mount path from /proc/self/mountinfo. Unused for cgroup v2
+    char* _mount_path;      // mount path from /proc/self/mountinfo.
+
+  public:
+    CgroupInfo() {
+      _name = NULL;
+      _hierarchy_id = -1;
+      _enabled = false;
+      _data_complete = false;
+      _cgroup_path = NULL;
+      _root_mount_path = NULL;
+      _mount_path = NULL;
+    }
+
+};
+
+class CgroupSubsystemFactory: AllStatic {
+  friend class WhiteBox;
+
+  public:
+    static CgroupSubsystem* create();
+  private:
+    static inline bool is_cgroup_v2(u1* flags) {
+       return *flags == CGROUPS_V2;
+    }
+
+#ifdef ASSERT
+    static inline bool is_valid_cgroup(u1* flags) {
+       return *flags == CGROUPS_V1 || *flags == CGROUPS_V2;
+    }
+    static inline bool is_cgroup_v1(u1* flags) {
+       return *flags == CGROUPS_V1;
+    }
+#endif
+
+    // Determine the cgroup type (version 1 or version 2), given
+    // relevant paths to files. Sets 'flags' accordingly.
+    static bool determine_type(CgroupInfo* cg_infos,
+                               const char* proc_cgroups,
+                               const char* proc_self_cgroup,
+                               const char* proc_self_mountinfo,
+                               u1* flags);
+    static void cleanup(CgroupInfo* cg_infos);
+};
+
+#endif // CGROUP_SUBSYSTEM_SERENITY_HPP
diff --git a/src/hotspot/os/serenity/cgroupV1Subsystem_serenity.cpp b/src/hotspot/os/serenity/cgroupV1Subsystem_serenity.cpp
new file mode 100644
index 00000000000..36b25e0fefe
--- /dev/null
+++ b/src/hotspot/os/serenity/cgroupV1Subsystem_serenity.cpp
@@ -0,0 +1,243 @@
+/*
+ * Copyright (c) 2019, 2020, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#include <string.h>
+#include <math.h>
+#include <errno.h>
+#include "cgroupV1Subsystem_serenity.hpp"
+#include "logging/log.hpp"
+#include "memory/allocation.hpp"
+#include "runtime/globals.hpp"
+#include "runtime/os.hpp"
+#include "utilities/globalDefinitions.hpp"
+
+/*
+ * Set directory to subsystem specific files based
+ * on the contents of the mountinfo and cgroup files.
+ */
+void CgroupV1Controller::set_subsystem_path(char *cgroup_path) {
+  char buf[MAXPATHLEN+1];
+  if (_root != NULL && cgroup_path != NULL) {
+    if (strcmp(_root, "/") == 0) {
+      int buflen;
+      strncpy(buf, _mount_point, MAXPATHLEN);
+      buf[MAXPATHLEN-1] = '\0';
+      if (strcmp(cgroup_path,"/") != 0) {
+        buflen = strlen(buf);
+        if ((buflen + strlen(cgroup_path)) > (MAXPATHLEN-1)) {
+          return;
+        }
+        strncat(buf, cgroup_path, MAXPATHLEN-buflen);
+        buf[MAXPATHLEN-1] = '\0';
+      }
+      _path = os::strdup(buf);
+    } else {
+      if (strcmp(_root, cgroup_path) == 0) {
+        strncpy(buf, _mount_point, MAXPATHLEN);
+        buf[MAXPATHLEN-1] = '\0';
+        _path = os::strdup(buf);
+      } else {
+        char *p = strstr(cgroup_path, _root);
+        if (p != NULL && p == _root) {
+          if (strlen(cgroup_path) > strlen(_root)) {
+            int buflen;
+            strncpy(buf, _mount_point, MAXPATHLEN);
+            buf[MAXPATHLEN-1] = '\0';
+            buflen = strlen(buf);
+            if ((buflen + strlen(cgroup_path) - strlen(_root)) > (MAXPATHLEN-1)) {
+              return;
+            }
+            strncat(buf, cgroup_path + strlen(_root), MAXPATHLEN-buflen);
+            buf[MAXPATHLEN-1] = '\0';
+            _path = os::strdup(buf);
+          }
+        }
+      }
+    }
+  }
+}
+
+/* uses_mem_hierarchy
+ *
+ * Return whether or not hierarchical cgroup accounting is being
+ * done.
+ *
+ * return:
+ *    A number > 0 if true, or
+ *    OSCONTAINER_ERROR for not supported
+ */
+jlong CgroupV1MemoryController::uses_mem_hierarchy() {
+  GET_CONTAINER_INFO(jlong, this, "/memory.use_hierarchy",
+                    "Use Hierarchy is: " JLONG_FORMAT, JLONG_FORMAT, use_hierarchy);
+  return use_hierarchy;
+}
+
+void CgroupV1MemoryController::set_subsystem_path(char *cgroup_path) {
+  CgroupV1Controller::set_subsystem_path(cgroup_path);
+  jlong hierarchy = uses_mem_hierarchy();
+  if (hierarchy > 0) {
+    set_hierarchical(true);
+  }
+}
+
+jlong CgroupV1Subsystem::read_memory_limit_in_bytes() {
+  GET_CONTAINER_INFO(julong, _memory->controller(), "/memory.limit_in_bytes",
+                     "Memory Limit is: " JULONG_FORMAT, JULONG_FORMAT, memlimit);
+
+  if (memlimit >= _unlimited_memory) {
+    log_trace(os, container)("Non-Hierarchical Memory Limit is: Unlimited");
+    CgroupV1MemoryController* mem_controller = reinterpret_cast<CgroupV1MemoryController*>(_memory->controller());
+    if (mem_controller->is_hierarchical()) {
+      const char* matchline = "hierarchical_memory_limit";
+      const char* format = "%s " JULONG_FORMAT;
+      GET_CONTAINER_INFO_LINE(julong, _memory->controller(), "/memory.stat", matchline,
+                             "Hierarchical Memory Limit is: " JULONG_FORMAT, format, hier_memlimit)
+      if (hier_memlimit >= _unlimited_memory) {
+        log_trace(os, container)("Hierarchical Memory Limit is: Unlimited");
+      } else {
+        return (jlong)hier_memlimit;
+      }
+    }
+    return (jlong)-1;
+  }
+  else {
+    return (jlong)memlimit;
+  }
+}
+
+jlong CgroupV1Subsystem::memory_and_swap_limit_in_bytes() {
+  GET_CONTAINER_INFO(julong, _memory->controller(), "/memory.memsw.limit_in_bytes",
+                     "Memory and Swap Limit is: " JULONG_FORMAT, JULONG_FORMAT, memswlimit);
+  if (memswlimit >= _unlimited_memory) {
+    log_trace(os, container)("Non-Hierarchical Memory and Swap Limit is: Unlimited");
+    CgroupV1MemoryController* mem_controller = reinterpret_cast<CgroupV1MemoryController*>(_memory->controller());
+    if (mem_controller->is_hierarchical()) {
+      const char* matchline = "hierarchical_memsw_limit";
+      const char* format = "%s " JULONG_FORMAT;
+      GET_CONTAINER_INFO_LINE(julong, _memory->controller(), "/memory.stat", matchline,
+                             "Hierarchical Memory and Swap Limit is : " JULONG_FORMAT, format, hier_memlimit)
+      if (hier_memlimit >= _unlimited_memory) {
+        log_trace(os, container)("Hierarchical Memory and Swap Limit is: Unlimited");
+      } else {
+        return (jlong)hier_memlimit;
+      }
+    }
+    return (jlong)-1;
+  } else {
+    return (jlong)memswlimit;
+  }
+}
+
+jlong CgroupV1Subsystem::memory_soft_limit_in_bytes() {
+  GET_CONTAINER_INFO(julong, _memory->controller(), "/memory.soft_limit_in_bytes",
+                     "Memory Soft Limit is: " JULONG_FORMAT, JULONG_FORMAT, memsoftlimit);
+  if (memsoftlimit >= _unlimited_memory) {
+    log_trace(os, container)("Memory Soft Limit is: Unlimited");
+    return (jlong)-1;
+  } else {
+    return (jlong)memsoftlimit;
+  }
+}
+
+/* memory_usage_in_bytes
+ *
+ * Return the amount of used memory for this process.
+ *
+ * return:
+ *    memory usage in bytes or
+ *    -1 for unlimited
+ *    OSCONTAINER_ERROR for not supported
+ */
+jlong CgroupV1Subsystem::memory_usage_in_bytes() {
+  GET_CONTAINER_INFO(jlong, _memory->controller(), "/memory.usage_in_bytes",
+                     "Memory Usage is: " JLONG_FORMAT, JLONG_FORMAT, memusage);
+  return memusage;
+}
+
+/* memory_max_usage_in_bytes
+ *
+ * Return the maximum amount of used memory for this process.
+ *
+ * return:
+ *    max memory usage in bytes or
+ *    OSCONTAINER_ERROR for not supported
+ */
+jlong CgroupV1Subsystem::memory_max_usage_in_bytes() {
+  GET_CONTAINER_INFO(jlong, _memory->controller(), "/memory.max_usage_in_bytes",
+                     "Maximum Memory Usage is: " JLONG_FORMAT, JLONG_FORMAT, memmaxusage);
+  return memmaxusage;
+}
+
+char * CgroupV1Subsystem::cpu_cpuset_cpus() {
+  GET_CONTAINER_INFO_CPTR(cptr, _cpuset, "/cpuset.cpus",
+                     "cpuset.cpus is: %s", "%1023s", cpus, 1024);
+  return os::strdup(cpus);
+}
+
+char * CgroupV1Subsystem::cpu_cpuset_memory_nodes() {
+  GET_CONTAINER_INFO_CPTR(cptr, _cpuset, "/cpuset.mems",
+                     "cpuset.mems is: %s", "%1023s", mems, 1024);
+  return os::strdup(mems);
+}
+
+/* cpu_quota
+ *
+ * Return the number of microseconds per period
+ * process is guaranteed to run.
+ *
+ * return:
+ *    quota time in microseconds
+ *    -1 for no quota
+ *    OSCONTAINER_ERROR for not supported
+ */
+int CgroupV1Subsystem::cpu_quota() {
+  GET_CONTAINER_INFO(int, _cpu->controller(), "/cpu.cfs_quota_us",
+                     "CPU Quota is: %d", "%d", quota);
+  return quota;
+}
+
+int CgroupV1Subsystem::cpu_period() {
+  GET_CONTAINER_INFO(int, _cpu->controller(), "/cpu.cfs_period_us",
+                     "CPU Period is: %d", "%d", period);
+  return period;
+}
+
+/* cpu_shares
+ *
+ * Return the amount of cpu shares available to the process
+ *
+ * return:
+ *    Share number (typically a number relative to 1024)
+ *                 (2048 typically expresses 2 CPUs worth of processing)
+ *    -1 for no share setup
+ *    OSCONTAINER_ERROR for not supported
+ */
+int CgroupV1Subsystem::cpu_shares() {
+  GET_CONTAINER_INFO(int, _cpu->controller(), "/cpu.shares",
+                     "CPU Shares is: %d", "%d", shares);
+  // Convert 1024 to no shares setup
+  if (shares == 1024) return -1;
+
+  return shares;
+}
diff --git a/src/hotspot/os/serenity/cgroupV1Subsystem_serenity.hpp b/src/hotspot/os/serenity/cgroupV1Subsystem_serenity.hpp
new file mode 100644
index 00000000000..603dc7a87f3
--- /dev/null
+++ b/src/hotspot/os/serenity/cgroupV1Subsystem_serenity.hpp
@@ -0,0 +1,118 @@
+/*
+ * Copyright (c) 2019, 2020, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#ifndef CGROUP_V1_SUBSYSTEM_SERENITY_HPP
+#define CGROUP_V1_SUBSYSTEM_SERENITY_HPP
+
+#include "runtime/os.hpp"
+#include "memory/allocation.hpp"
+#include "cgroupSubsystem_serenity.hpp"
+
+// Cgroups version 1 specific implementation
+
+class CgroupV1Controller: public CgroupController {
+  private:
+    /* mountinfo contents */
+    char *_root;
+    char *_mount_point;
+
+    /* Constructed subsystem directory */
+    char *_path;
+
+  public:
+    CgroupV1Controller(char *root, char *mountpoint) {
+      _root = os::strdup(root);
+      _mount_point = os::strdup(mountpoint);
+      _path = NULL;
+    }
+
+    virtual void set_subsystem_path(char *cgroup_path);
+    char *subsystem_path() { return _path; }
+};
+
+class CgroupV1MemoryController: public CgroupV1Controller {
+
+  public:
+    bool is_hierarchical() { return _uses_mem_hierarchy; }
+    void set_subsystem_path(char *cgroup_path);
+  private:
+    /* Some container runtimes set limits via cgroup
+     * hierarchy. If set to true consider also memory.stat
+     * file if everything else seems unlimited */
+    bool _uses_mem_hierarchy;
+    jlong uses_mem_hierarchy();
+    void set_hierarchical(bool value) { _uses_mem_hierarchy = value; }
+
+  public:
+    CgroupV1MemoryController(char *root, char *mountpoint) : CgroupV1Controller(root, mountpoint) {
+      _uses_mem_hierarchy = false;
+    }
+
+};
+
+class CgroupV1Subsystem: public CgroupSubsystem {
+
+  public:
+    jlong read_memory_limit_in_bytes();
+    jlong memory_and_swap_limit_in_bytes();
+    jlong memory_soft_limit_in_bytes();
+    jlong memory_usage_in_bytes();
+    jlong memory_max_usage_in_bytes();
+    char * cpu_cpuset_cpus();
+    char * cpu_cpuset_memory_nodes();
+
+    int cpu_quota();
+    int cpu_period();
+
+    int cpu_shares();
+
+    const char * container_type() {
+      return "cgroupv1";
+    }
+    CachingCgroupController * memory_controller() { return _memory; }
+    CachingCgroupController * cpu_controller() { return _cpu; }
+
+  private:
+    julong _unlimited_memory;
+
+    /* controllers */
+    CachingCgroupController* _memory = NULL;
+    CgroupV1Controller* _cpuset = NULL;
+    CachingCgroupController* _cpu = NULL;
+    CgroupV1Controller* _cpuacct = NULL;
+
+  public:
+    CgroupV1Subsystem(CgroupV1Controller* cpuset,
+                      CgroupV1Controller* cpu,
+                      CgroupV1Controller* cpuacct,
+                      CgroupV1MemoryController* memory) {
+      _cpuset = cpuset;
+      _cpu = new CachingCgroupController(cpu);
+      _cpuacct = cpuacct;
+      _memory = new CachingCgroupController(memory);
+      _unlimited_memory = (LONG_MAX / os::vm_page_size()) * os::vm_page_size();
+    }
+};
+
+#endif // CGROUP_V1_SUBSYSTEM_SERENITY_HPP
diff --git a/src/hotspot/os/serenity/cgroupV2Subsystem_serenity.cpp b/src/hotspot/os/serenity/cgroupV2Subsystem_serenity.cpp
new file mode 100644
index 00000000000..e5720726dc0
--- /dev/null
+++ b/src/hotspot/os/serenity/cgroupV2Subsystem_serenity.cpp
@@ -0,0 +1,246 @@
+/*
+ * Copyright (c) 2020, Red Hat Inc.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#include "cgroupV2Subsystem_serenity.hpp"
+
+/* cpu_shares
+ *
+ * Return the amount of cpu shares available to the process
+ *
+ * return:
+ *    Share number (typically a number relative to 1024)
+ *                 (2048 typically expresses 2 CPUs worth of processing)
+ *    -1 for no share setup
+ *    OSCONTAINER_ERROR for not supported
+ */
+int CgroupV2Subsystem::cpu_shares() {
+  GET_CONTAINER_INFO(int, _unified, "/cpu.weight",
+                     "Raw value for CPU shares is: %d", "%d", shares);
+  // Convert default value of 100 to no shares setup
+  if (shares == 100) {
+    log_debug(os, container)("CPU Shares is: %d", -1);
+    return -1;
+  }
+
+  // CPU shares (OCI) value needs to get translated into
+  // a proper Cgroups v2 value. See:
+  // https://github.com/containers/crun/blob/master/crun.1.md#cpu-controller
+  //
+  // Use the inverse of (x == OCI value, y == cgroupsv2 value):
+  // ((262142 * y - 1)/9999) + 2 = x
+  //
+  int x = 262142 * shares - 1;
+  double frac = x/9999.0;
+  x = ((int)frac) + 2;
+  log_trace(os, container)("Scaled CPU shares value is: %d", x);
+  // Since the scaled value is not precise, return the closest
+  // multiple of PER_CPU_SHARES for a more conservative mapping
+  if ( x <= PER_CPU_SHARES ) {
+     // will always map to 1 CPU
+     log_debug(os, container)("CPU Shares is: %d", x);
+     return x;
+  }
+  int f = x/PER_CPU_SHARES;
+  int lower_multiple = f * PER_CPU_SHARES;
+  int upper_multiple = (f + 1) * PER_CPU_SHARES;
+  int distance_lower = MAX2(lower_multiple, x) - MIN2(lower_multiple, x);
+  int distance_upper = MAX2(upper_multiple, x) - MIN2(upper_multiple, x);
+  x = distance_lower <= distance_upper ? lower_multiple : upper_multiple;
+  log_trace(os, container)("Closest multiple of %d of the CPU Shares value is: %d", PER_CPU_SHARES, x);
+  log_debug(os, container)("CPU Shares is: %d", x);
+  return x;
+}
+
+/* cpu_quota
+ *
+ * Return the number of microseconds per period
+ * process is guaranteed to run.
+ *
+ * return:
+ *    quota time in microseconds
+ *    -1 for no quota
+ *    OSCONTAINER_ERROR for not supported
+ */
+int CgroupV2Subsystem::cpu_quota() {
+  char * cpu_quota_str = cpu_quota_val();
+  int limit = (int)limit_from_str(cpu_quota_str);
+  log_trace(os, container)("CPU Quota is: %d", limit);
+  return limit;
+}
+
+char * CgroupV2Subsystem::cpu_cpuset_cpus() {
+  GET_CONTAINER_INFO_CPTR(cptr, _unified, "/cpuset.cpus",
+                     "cpuset.cpus is: %s", "%1023s", cpus, 1024);
+  if (cpus == NULL) {
+    return NULL;
+  }
+  return os::strdup(cpus);
+}
+
+char* CgroupV2Subsystem::cpu_quota_val() {
+  GET_CONTAINER_INFO_CPTR(cptr, _unified, "/cpu.max",
+                     "Raw value for CPU quota is: %s", "%s %*d", quota, 1024);
+  if (quota == NULL) {
+    return NULL;
+  }
+  return os::strdup(quota);
+}
+
+char * CgroupV2Subsystem::cpu_cpuset_memory_nodes() {
+  GET_CONTAINER_INFO_CPTR(cptr, _unified, "/cpuset.mems",
+                     "cpuset.mems is: %s", "%1023s", mems, 1024);
+  if (mems == NULL) {
+    return NULL;
+  }
+  return os::strdup(mems);
+}
+
+int CgroupV2Subsystem::cpu_period() {
+  GET_CONTAINER_INFO(int, _unified, "/cpu.max",
+                     "CPU Period is: %d", "%*s %d", period);
+  return period;
+}
+
+/* memory_usage_in_bytes
+ *
+ * Return the amount of used memory used by this cgroup and decendents
+ *
+ * return:
+ *    memory usage in bytes or
+ *    -1 for unlimited
+ *    OSCONTAINER_ERROR for not supported
+ */
+jlong CgroupV2Subsystem::memory_usage_in_bytes() {
+  GET_CONTAINER_INFO(jlong, _unified, "/memory.current",
+                     "Memory Usage is: " JLONG_FORMAT, JLONG_FORMAT, memusage);
+  return memusage;
+}
+
+jlong CgroupV2Subsystem::memory_soft_limit_in_bytes() {
+  char* mem_soft_limit_str = mem_soft_limit_val();
+  return limit_from_str(mem_soft_limit_str);
+}
+
+jlong CgroupV2Subsystem::memory_max_usage_in_bytes() {
+  // Log this string at trace level so as to make tests happy.
+  log_trace(os, container)("Maximum Memory Usage is not supported.");
+  return OSCONTAINER_ERROR; // not supported
+}
+
+char* CgroupV2Subsystem::mem_soft_limit_val() {
+  GET_CONTAINER_INFO_CPTR(cptr, _unified, "/memory.low",
+                         "Memory Soft Limit is: %s", "%s", mem_soft_limit_str, 1024);
+  if (mem_soft_limit_str == NULL) {
+    return NULL;
+  }
+  return os::strdup(mem_soft_limit_str);
+}
+
+// Note that for cgroups v2 the actual limits set for swap and
+// memory live in two different files, memory.swap.max and memory.max
+// respectively. In order to properly report a cgroup v1 like
+// compound value we need to sum the two values. Setting a swap limit
+// without also setting a memory limit is not allowed.
+jlong CgroupV2Subsystem::memory_and_swap_limit_in_bytes() {
+  char* mem_swp_limit_str = mem_swp_limit_val();
+  jlong swap_limit = limit_from_str(mem_swp_limit_str);
+  if (swap_limit >= 0) {
+    jlong memory_limit = read_memory_limit_in_bytes();
+    assert(memory_limit >= 0, "swap limit without memory limit?");
+    return memory_limit + swap_limit;
+  }
+  return swap_limit;
+}
+
+char* CgroupV2Subsystem::mem_swp_limit_val() {
+  GET_CONTAINER_INFO_CPTR(cptr, _unified, "/memory.swap.max",
+                         "Memory and Swap Limit is: %s", "%s", mem_swp_limit_str, 1024);
+  if (mem_swp_limit_str == NULL) {
+    return NULL;
+  }
+  return os::strdup(mem_swp_limit_str);
+}
+
+/* memory_limit_in_bytes
+ *
+ * Return the limit of available memory for this process.
+ *
+ * return:
+ *    memory limit in bytes or
+ *    -1 for unlimited, OSCONTAINER_ERROR for an error
+ */
+jlong CgroupV2Subsystem::read_memory_limit_in_bytes() {
+  char * mem_limit_str = mem_limit_val();
+  jlong limit = limit_from_str(mem_limit_str);
+  if (log_is_enabled(Trace, os, container)) {
+    if (limit == -1) {
+      log_trace(os, container)("Memory Limit is: Unlimited");
+    } else {
+      log_trace(os, container)("Memory Limit is: " JLONG_FORMAT, limit);
+    }
+  }
+  return limit;
+}
+
+jlong CgroupV2Subsystem::limit_from_str(char* limit_str) {
+  if (limit_str == NULL) {
+    return OSCONTAINER_ERROR;
+  }
+  // Unlimited memory in Cgroups V2 is the literal string 'max'
+  if (strcmp("max", limit_str) == 0) {
+    os::free(limit_str);
+    return (jlong)-1;
+  }
+  julong limit;
+  if (sscanf(limit_str, JULONG_FORMAT, &limit) != 1) {
+    os::free(limit_str);
+    return OSCONTAINER_ERROR;
+  }
+  os::free(limit_str);
+  return (jlong)limit;
+}
+
+char* CgroupV2Subsystem::mem_limit_val() {
+  GET_CONTAINER_INFO_CPTR(cptr, _unified, "/memory.max",
+                         "Raw value for memory limit is: %s", "%s", mem_limit_str, 1024);
+  if (mem_limit_str == NULL) {
+    return NULL;
+  }
+  return os::strdup(mem_limit_str);
+}
+
+char* CgroupV2Controller::construct_path(char* mount_path, char *cgroup_path) {
+  char buf[MAXPATHLEN+1];
+  int buflen;
+  strncpy(buf, mount_path, MAXPATHLEN);
+  buf[MAXPATHLEN] = '\0';
+  buflen = strlen(buf);
+  if ((buflen + strlen(cgroup_path)) > MAXPATHLEN) {
+    return NULL;
+  }
+  strncat(buf, cgroup_path, MAXPATHLEN-buflen);
+  buf[MAXPATHLEN] = '\0';
+  return os::strdup(buf);
+}
+
diff --git a/src/hotspot/os/serenity/cgroupV2Subsystem_serenity.hpp b/src/hotspot/os/serenity/cgroupV2Subsystem_serenity.hpp
new file mode 100644
index 00000000000..627ed1ab8d2
--- /dev/null
+++ b/src/hotspot/os/serenity/cgroupV2Subsystem_serenity.hpp
@@ -0,0 +1,89 @@
+/*
+ * Copyright (c) 2020, Red Hat Inc.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#ifndef CGROUP_V2_SUBSYSTEM_SERENITY_HPP
+#define CGROUP_V2_SUBSYSTEM_SERENITY_HPP
+
+#include "cgroupSubsystem_serenity.hpp"
+
+class CgroupV2Controller: public CgroupController {
+  private:
+    /* the mount path of the cgroup v2 hierarchy */
+    char *_mount_path;
+    /* The cgroup path for the controller */
+    char *_cgroup_path;
+
+    /* Constructed full path to the subsystem directory */
+    char *_path;
+    static char* construct_path(char* mount_path, char *cgroup_path);
+
+  public:
+    CgroupV2Controller(char * mount_path, char *cgroup_path) {
+      _mount_path = mount_path;
+      _cgroup_path = os::strdup(cgroup_path);
+      _path = construct_path(mount_path, cgroup_path);
+    }
+
+    char *subsystem_path() { return _path; }
+};
+
+class CgroupV2Subsystem: public CgroupSubsystem {
+  private:
+    /* One unified controller */
+    CgroupController* _unified = NULL;
+    /* Caching wrappers for cpu/memory metrics */
+    CachingCgroupController* _memory = NULL;
+    CachingCgroupController* _cpu = NULL;
+
+    char *mem_limit_val();
+    char *mem_swp_limit_val();
+    char *mem_soft_limit_val();
+    char *cpu_quota_val();
+    jlong limit_from_str(char* limit_str);
+
+  public:
+    CgroupV2Subsystem(CgroupController * unified) {
+      _unified = unified;
+      _memory = new CachingCgroupController(unified);
+      _cpu = new CachingCgroupController(unified);
+    }
+
+    jlong read_memory_limit_in_bytes();
+    int cpu_quota();
+    int cpu_period();
+    int cpu_shares();
+    jlong memory_and_swap_limit_in_bytes();
+    jlong memory_soft_limit_in_bytes();
+    jlong memory_usage_in_bytes();
+    jlong memory_max_usage_in_bytes();
+    char * cpu_cpuset_cpus();
+    char * cpu_cpuset_memory_nodes();
+    const char * container_type() {
+      return "cgroupv2";
+    }
+    CachingCgroupController * memory_controller() { return _memory; }
+    CachingCgroupController * cpu_controller() { return _cpu; }
+};
+
+#endif // CGROUP_V2_SUBSYSTEM_SERENITY_HPP
diff --git a/src/hotspot/os/serenity/decoder_serenity.cpp b/src/hotspot/os/serenity/decoder_serenity.cpp
new file mode 100644
index 00000000000..daa12ec6f6d
--- /dev/null
+++ b/src/hotspot/os/serenity/decoder_serenity.cpp
@@ -0,0 +1,88 @@
+/*
+ * Copyright (c) 1997, 2018, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#include "jvm.h"
+#include "utilities/decoder_elf.hpp"
+#include "utilities/elfFile.hpp"
+
+#include <cxxabi.h>
+
+bool ElfDecoder::demangle(const char* symbol, char *buf, int buflen) {
+  int   status;
+  char* result;
+  size_t size = (size_t)buflen;
+
+#ifdef PPC64
+  // On PPC64 ElfDecoder::decode() may return a dot (.) prefixed name
+  // (see elfFuncDescTable.hpp for details)
+  if (symbol && *symbol == '.') symbol += 1;
+#endif
+
+  // Don't pass buf to __cxa_demangle. In case of the 'buf' is too small,
+  // __cxa_demangle will call system "realloc" for additional memory, which
+  // may use different malloc/realloc mechanism that allocates 'buf'.
+  if ((result = abi::__cxa_demangle(symbol, NULL, NULL, &status)) != NULL) {
+    jio_snprintf(buf, buflen, "%s", result);
+      // call c library's free
+      ::free(result);
+      return true;
+  }
+  return false;
+}
+
+// Returns true if the elf file is marked NOT to require an executable stack,
+// or if the file could not be opened.
+// Returns false if the elf file requires an executable stack, the stack flag
+// is not set at all, or if the file can not be read.
+bool ElfFile::specifies_noexecstack(const char* filepath) {
+  if (filepath == NULL) return true;
+
+  FILE* file = fopen(filepath, "r");
+  if (file == NULL)  return true;
+
+  // AARCH64 defaults to noexecstack. All others default to execstack.
+  bool result = AARCH64_ONLY(true) NOT_AARCH64(false);
+
+  // Read file header
+  Elf_Ehdr head;
+  if (fread(&head, sizeof(Elf_Ehdr), 1, file) == 1 &&
+      is_elf_file(head) &&
+      fseek(file, head.e_phoff, SEEK_SET) == 0) {
+
+    // Read program header table
+    Elf_Phdr phdr;
+    for (int index = 0; index < head.e_phnum; index ++) {
+      if (fread((void*)&phdr, sizeof(Elf_Phdr), 1, file) != 1) {
+        result = false;
+        break;
+      }
+      if (phdr.p_type == PT_GNU_STACK) {
+        result = (phdr.p_flags == (PF_R | PF_W));
+        break;
+      }
+    }
+  }
+  fclose(file);
+  return result;
+}
diff --git a/src/hotspot/os/serenity/gc/z/zLargePages_linux.cpp b/src/hotspot/os/serenity/gc/z/zLargePages_linux.cpp
new file mode 100644
index 00000000000..caf70224599
--- /dev/null
+++ b/src/hotspot/os/serenity/gc/z/zLargePages_linux.cpp
@@ -0,0 +1,38 @@
+/*
+ * Copyright (c) 2017, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ */
+
+#include "precompiled.hpp"
+#include "gc/z/zLargePages.hpp"
+#include "runtime/globals.hpp"
+
+void ZLargePages::pd_initialize() {
+  if (UseLargePages) {
+    if (UseTransparentHugePages) {
+      _state = Transparent;
+    } else {
+      _state = Explicit;
+    }
+  } else {
+    _state = Disabled;
+  }
+}
diff --git a/src/hotspot/os/serenity/gc/z/zMountPoint_linux.cpp b/src/hotspot/os/serenity/gc/z/zMountPoint_linux.cpp
new file mode 100644
index 00000000000..d3c5ef3fd7d
--- /dev/null
+++ b/src/hotspot/os/serenity/gc/z/zMountPoint_linux.cpp
@@ -0,0 +1,150 @@
+/*
+ * Copyright (c) 2016, 2020, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ */
+
+#include "precompiled.hpp"
+#include "gc/shared/gcLogPrecious.hpp"
+#include "gc/z/zArray.inline.hpp"
+#include "gc/z/zErrno.hpp"
+#include "gc/z/zMountPoint_linux.hpp"
+#include "runtime/globals.hpp"
+
+#include <stdio.h>
+#include <unistd.h>
+
+// Mount information, see proc(5) for more details.
+#define PROC_SELF_MOUNTINFO        "/proc/self/mountinfo"
+
+ZMountPoint::ZMountPoint(const char* filesystem, const char** preferred_mountpoints) {
+  if (AllocateHeapAt != NULL) {
+    // Use specified path
+    _path = strdup(AllocateHeapAt);
+  } else {
+    // Find suitable path
+    _path = find_mountpoint(filesystem, preferred_mountpoints);
+  }
+}
+
+ZMountPoint::~ZMountPoint() {
+  free(_path);
+  _path = NULL;
+}
+
+char* ZMountPoint::get_mountpoint(const char* line, const char* filesystem) const {
+  char* line_mountpoint = NULL;
+  char* line_filesystem = NULL;
+
+  // Parse line and return a newly allocated string containing the mount point if
+  // the line contains a matching filesystem and the mount point is accessible by
+  // the current user.
+  if (sscanf(line, "%*u %*u %*u:%*u %*s %ms %*[^-]- %ms", &line_mountpoint, &line_filesystem) != 2 ||
+      strcmp(line_filesystem, filesystem) != 0 ||
+      access(line_mountpoint, R_OK|W_OK|X_OK) != 0) {
+    // Not a matching or accessible filesystem
+    free(line_mountpoint);
+    line_mountpoint = NULL;
+  }
+
+  free(line_filesystem);
+
+  return line_mountpoint;
+}
+
+void ZMountPoint::get_mountpoints(const char* filesystem, ZArray<char*>* mountpoints) const {
+  FILE* fd = fopen(PROC_SELF_MOUNTINFO, "r");
+  if (fd == NULL) {
+    ZErrno err;
+    log_error_p(gc)("Failed to open %s: %s", PROC_SELF_MOUNTINFO, err.to_string());
+    return;
+  }
+
+  char* line = NULL;
+  size_t length = 0;
+
+  while (getline(&line, &length, fd) != -1) {
+    char* const mountpoint = get_mountpoint(line, filesystem);
+    if (mountpoint != NULL) {
+      mountpoints->append(mountpoint);
+    }
+  }
+
+  free(line);
+  fclose(fd);
+}
+
+void ZMountPoint::free_mountpoints(ZArray<char*>* mountpoints) const {
+  ZArrayIterator<char*> iter(mountpoints);
+  for (char* mountpoint; iter.next(&mountpoint);) {
+    free(mountpoint);
+  }
+  mountpoints->clear();
+}
+
+char* ZMountPoint::find_preferred_mountpoint(const char* filesystem,
+                                              ZArray<char*>* mountpoints,
+                                              const char** preferred_mountpoints) const {
+  // Find preferred mount point
+  ZArrayIterator<char*> iter1(mountpoints);
+  for (char* mountpoint; iter1.next(&mountpoint);) {
+    for (const char** preferred = preferred_mountpoints; *preferred != NULL; preferred++) {
+      if (!strcmp(mountpoint, *preferred)) {
+        // Preferred mount point found
+        return strdup(mountpoint);
+      }
+    }
+  }
+
+  // Preferred mount point not found
+  log_error_p(gc)("More than one %s filesystem found:", filesystem);
+  ZArrayIterator<char*> iter2(mountpoints);
+  for (char* mountpoint; iter2.next(&mountpoint);) {
+    log_error_p(gc)("  %s", mountpoint);
+  }
+
+  return NULL;
+}
+
+char* ZMountPoint::find_mountpoint(const char* filesystem, const char** preferred_mountpoints) const {
+  char* path = NULL;
+  ZArray<char*> mountpoints;
+
+  get_mountpoints(filesystem, &mountpoints);
+
+  if (mountpoints.length() == 0) {
+    // No mount point found
+    log_error_p(gc)("Failed to find an accessible %s filesystem", filesystem);
+  } else if (mountpoints.length() == 1) {
+    // One mount point found
+    path = strdup(mountpoints.at(0));
+  } else {
+    // More than one mount point found
+    path = find_preferred_mountpoint(filesystem, &mountpoints, preferred_mountpoints);
+  }
+
+  free_mountpoints(&mountpoints);
+
+  return path;
+}
+
+const char* ZMountPoint::get() const {
+  return _path;
+}
diff --git a/src/hotspot/os/serenity/gc/z/zMountPoint_linux.hpp b/src/hotspot/os/serenity/gc/z/zMountPoint_linux.hpp
new file mode 100644
index 00000000000..6e644b0eca9
--- /dev/null
+++ b/src/hotspot/os/serenity/gc/z/zMountPoint_linux.hpp
@@ -0,0 +1,52 @@
+/*
+ * Copyright (c) 2016, 2020, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ */
+
+#ifndef OS_SERENITY_GC_Z_ZMOUNTPOINT_SERENITY_HPP
+#define OS_SERENITY_GC_Z_ZMOUNTPOINT_SERENITY_HPP
+
+#include "gc/z/zArray.hpp"
+#include "memory/allocation.hpp"
+
+class ZMountPoint : public StackObj {
+private:
+  char* _path;
+
+  char* get_mountpoint(const char* line,
+                       const char* filesystem) const;
+  void get_mountpoints(const char* filesystem,
+                       ZArray<char*>* mountpoints) const;
+  void free_mountpoints(ZArray<char*>* mountpoints) const;
+  char* find_preferred_mountpoint(const char* filesystem,
+                                  ZArray<char*>* mountpoints,
+                                  const char** preferred_mountpoints) const;
+  char* find_mountpoint(const char* filesystem,
+                        const char** preferred_mountpoints) const;
+
+public:
+  ZMountPoint(const char* filesystem, const char** preferred_mountpoints);
+  ~ZMountPoint();
+
+  const char* get() const;
+};
+
+#endif // OS_SERENITY_GC_Z_ZMOUNTPOINT_SERENITY_HPP
diff --git a/src/hotspot/os/serenity/gc/z/zNUMA_linux.cpp b/src/hotspot/os/serenity/gc/z/zNUMA_linux.cpp
new file mode 100644
index 00000000000..cfe25549ffc
--- /dev/null
+++ b/src/hotspot/os/serenity/gc/z/zNUMA_linux.cpp
@@ -0,0 +1,70 @@
+/*
+ * Copyright (c) 2016, 2020, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ */
+
+#include "gc/z/zCPU.inline.hpp"
+#include "gc/z/zErrno.hpp"
+#include "gc/z/zNUMA.hpp"
+#include "gc/z/zSyscall_linux.hpp"
+#include "runtime/globals.hpp"
+#include "runtime/os.hpp"
+#include "utilities/debug.hpp"
+
+void ZNUMA::pd_initialize() {
+  _enabled = UseNUMA;
+}
+
+uint32_t ZNUMA::count() {
+  if (!_enabled) {
+    // NUMA support not enabled
+    return 1;
+  }
+
+  return os::Linux::numa_max_node() + 1;
+}
+
+uint32_t ZNUMA::id() {
+  if (!_enabled) {
+    // NUMA support not enabled
+    return 0;
+  }
+
+  return os::Linux::get_node_by_cpu(ZCPU::id());
+}
+
+uint32_t ZNUMA::memory_id(uintptr_t addr) {
+  if (!_enabled) {
+    // NUMA support not enabled, assume everything belongs to node zero
+    return 0;
+  }
+
+  uint32_t id = (uint32_t)-1;
+
+  if (ZSyscall::get_mempolicy((int*)&id, NULL, 0, (void*)addr, MPOL_F_NODE | MPOL_F_ADDR) == -1) {
+    ZErrno err;
+    fatal("Failed to get NUMA id for memory at " PTR_FORMAT " (%s)", addr, err.to_string());
+  }
+
+  assert(id < count(), "Invalid NUMA id");
+
+  return id;
+}
diff --git a/src/hotspot/os/serenity/gc/z/zPhysicalMemoryBacking_linux.cpp b/src/hotspot/os/serenity/gc/z/zPhysicalMemoryBacking_linux.cpp
new file mode 100644
index 00000000000..7a113055423
--- /dev/null
+++ b/src/hotspot/os/serenity/gc/z/zPhysicalMemoryBacking_linux.cpp
@@ -0,0 +1,723 @@
+/*
+ * Copyright (c) 2015, 2021, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ */
+
+#include "precompiled.hpp"
+#include "gc/shared/gcLogPrecious.hpp"
+#include "gc/z/zArray.inline.hpp"
+#include "gc/z/zErrno.hpp"
+#include "gc/z/zGlobals.hpp"
+#include "gc/z/zLargePages.inline.hpp"
+#include "gc/z/zMountPoint_linux.hpp"
+#include "gc/z/zNUMA.inline.hpp"
+#include "gc/z/zPhysicalMemoryBacking_linux.hpp"
+#include "gc/z/zSyscall_linux.hpp"
+#include "logging/log.hpp"
+#include "runtime/init.hpp"
+#include "runtime/os.hpp"
+#include "runtime/safefetch.inline.hpp"
+#include "utilities/align.hpp"
+#include "utilities/debug.hpp"
+#include "utilities/growableArray.hpp"
+
+#include <fcntl.h>
+#include <stdio.h>
+#include <sys/mman.h>
+#include <sys/stat.h>
+#include <sys/statfs.h>
+#include <sys/types.h>
+#include <unistd.h>
+
+//
+// Support for building on older Linux systems
+//
+
+// memfd_create(2) flags
+#ifndef MFD_CLOEXEC
+#define MFD_CLOEXEC                      0x0001U
+#endif
+#ifndef MFD_HUGETLB
+#define MFD_HUGETLB                      0x0004U
+#endif
+
+// open(2) flags
+#ifndef O_CLOEXEC
+#define O_CLOEXEC                        02000000
+#endif
+#ifndef O_TMPFILE
+#define O_TMPFILE                        (020000000 | O_DIRECTORY)
+#endif
+
+// fallocate(2) flags
+#ifndef FALLOC_FL_KEEP_SIZE
+#define FALLOC_FL_KEEP_SIZE              0x01
+#endif
+#ifndef FALLOC_FL_PUNCH_HOLE
+#define FALLOC_FL_PUNCH_HOLE             0x02
+#endif
+
+// Filesystem types, see statfs(2)
+#ifndef TMPFS_MAGIC
+#define TMPFS_MAGIC                      0x01021994
+#endif
+#ifndef HUGETLBFS_MAGIC
+#define HUGETLBFS_MAGIC                  0x958458f6
+#endif
+
+// Filesystem names
+#define ZFILESYSTEM_TMPFS                "tmpfs"
+#define ZFILESYSTEM_HUGETLBFS            "hugetlbfs"
+
+// Proc file entry for max map mount
+#define ZFILENAME_PROC_MAX_MAP_COUNT     "/proc/sys/vm/max_map_count"
+
+// Sysfs file for transparent huge page on tmpfs
+#define ZFILENAME_SHMEM_ENABLED          "/sys/kernel/mm/transparent_hugepage/shmem_enabled"
+
+// Java heap filename
+#define ZFILENAME_HEAP                   "java_heap"
+
+// Preferred tmpfs mount points, ordered by priority
+static const char* z_preferred_tmpfs_mountpoints[] = {
+  "/dev/shm",
+  "/run/shm",
+  NULL
+};
+
+// Preferred hugetlbfs mount points, ordered by priority
+static const char* z_preferred_hugetlbfs_mountpoints[] = {
+  "/dev/hugepages",
+  "/hugepages",
+  NULL
+};
+
+static int z_fallocate_hugetlbfs_attempts = 3;
+static bool z_fallocate_supported = true;
+
+ZPhysicalMemoryBacking::ZPhysicalMemoryBacking(size_t max_capacity) :
+    _fd(-1),
+    _filesystem(0),
+    _block_size(0),
+    _available(0),
+    _initialized(false) {
+
+  // Create backing file
+  _fd = create_fd(ZFILENAME_HEAP);
+  if (_fd == -1) {
+    return;
+  }
+
+  // Truncate backing file
+  while (ftruncate(_fd, max_capacity) == -1) {
+    if (errno != EINTR) {
+      ZErrno err;
+      log_error_p(gc)("Failed to truncate backing file (%s)", err.to_string());
+      return;
+    }
+  }
+
+  // Get filesystem statistics
+  struct statfs buf;
+  if (fstatfs(_fd, &buf) == -1) {
+    ZErrno err;
+    log_error_p(gc)("Failed to determine filesystem type for backing file (%s)", err.to_string());
+    return;
+  }
+
+  _filesystem = buf.f_type;
+  _block_size = buf.f_bsize;
+  _available = buf.f_bavail * _block_size;
+
+  log_info_p(gc, init)("Heap Backing Filesystem: %s (0x" UINT64_FORMAT_X ")",
+                       is_tmpfs() ? ZFILESYSTEM_TMPFS : is_hugetlbfs() ? ZFILESYSTEM_HUGETLBFS : "other", _filesystem);
+
+  // Make sure the filesystem type matches requested large page type
+  if (ZLargePages::is_transparent() && !is_tmpfs()) {
+    log_error_p(gc)("-XX:+UseTransparentHugePages can only be enabled when using a %s filesystem",
+                    ZFILESYSTEM_TMPFS);
+    return;
+  }
+
+  if (ZLargePages::is_transparent() && !tmpfs_supports_transparent_huge_pages()) {
+    log_error_p(gc)("-XX:+UseTransparentHugePages on a %s filesystem not supported by kernel",
+                    ZFILESYSTEM_TMPFS);
+    return;
+  }
+
+  if (ZLargePages::is_explicit() && !is_hugetlbfs()) {
+    log_error_p(gc)("-XX:+UseLargePages (without -XX:+UseTransparentHugePages) can only be enabled "
+                    "when using a %s filesystem", ZFILESYSTEM_HUGETLBFS);
+    return;
+  }
+
+  if (!ZLargePages::is_explicit() && is_hugetlbfs()) {
+    log_error_p(gc)("-XX:+UseLargePages must be enabled when using a %s filesystem",
+                    ZFILESYSTEM_HUGETLBFS);
+    return;
+  }
+
+  if (ZLargePages::is_explicit() && os::large_page_size() != ZGranuleSize) {
+    log_error_p(gc)("Incompatible large page size configured " SIZE_FORMAT " (expected " SIZE_FORMAT ")",
+                    os::large_page_size(), ZGranuleSize);
+    return;
+  }
+
+  // Make sure the filesystem block size is compatible
+  if (ZGranuleSize % _block_size != 0) {
+    log_error_p(gc)("Filesystem backing the heap has incompatible block size (" SIZE_FORMAT ")",
+                    _block_size);
+    return;
+  }
+
+  if (is_hugetlbfs() && _block_size != ZGranuleSize) {
+    log_error_p(gc)("%s filesystem has unexpected block size " SIZE_FORMAT " (expected " SIZE_FORMAT ")",
+                    ZFILESYSTEM_HUGETLBFS, _block_size, ZGranuleSize);
+    return;
+  }
+
+  // Successfully initialized
+  _initialized = true;
+}
+
+int ZPhysicalMemoryBacking::create_mem_fd(const char* name) const {
+  // Create file name
+  char filename[PATH_MAX];
+  snprintf(filename, sizeof(filename), "%s%s", name, ZLargePages::is_explicit() ? ".hugetlb" : "");
+
+  // Create file
+  const int extra_flags = ZLargePages::is_explicit() ? MFD_HUGETLB : 0;
+  const int fd = ZSyscall::memfd_create(filename, MFD_CLOEXEC | extra_flags);
+  if (fd == -1) {
+    ZErrno err;
+    log_debug_p(gc, init)("Failed to create memfd file (%s)",
+                          ((ZLargePages::is_explicit() && err == EINVAL) ? "Hugepages not supported" : err.to_string()));
+    return -1;
+  }
+
+  log_info_p(gc, init)("Heap Backing File: /memfd:%s", filename);
+
+  return fd;
+}
+
+int ZPhysicalMemoryBacking::create_file_fd(const char* name) const {
+  const char* const filesystem = ZLargePages::is_explicit()
+                                 ? ZFILESYSTEM_HUGETLBFS
+                                 : ZFILESYSTEM_TMPFS;
+  const char** const preferred_mountpoints = ZLargePages::is_explicit()
+                                             ? z_preferred_hugetlbfs_mountpoints
+                                             : z_preferred_tmpfs_mountpoints;
+
+  // Find mountpoint
+  ZMountPoint mountpoint(filesystem, preferred_mountpoints);
+  if (mountpoint.get() == NULL) {
+    log_error_p(gc)("Use -XX:AllocateHeapAt to specify the path to a %s filesystem", filesystem);
+    return -1;
+  }
+
+  // Try to create an anonymous file using the O_TMPFILE flag. Note that this
+  // flag requires kernel >= 3.11. If this fails we fall back to open/unlink.
+  const int fd_anon = os::open(mountpoint.get(), O_TMPFILE|O_EXCL|O_RDWR|O_CLOEXEC, S_IRUSR|S_IWUSR);
+  if (fd_anon == -1) {
+    ZErrno err;
+    log_debug_p(gc, init)("Failed to create anonymous file in %s (%s)", mountpoint.get(),
+                          (err == EINVAL ? "Not supported" : err.to_string()));
+  } else {
+    // Get inode number for anonymous file
+    struct stat stat_buf;
+    if (fstat(fd_anon, &stat_buf) == -1) {
+      ZErrno err;
+      log_error_pd(gc)("Failed to determine inode number for anonymous file (%s)", err.to_string());
+      return -1;
+    }
+
+    log_info_p(gc, init)("Heap Backing File: %s/#" UINT64_FORMAT, mountpoint.get(), (uint64_t)stat_buf.st_ino);
+
+    return fd_anon;
+  }
+
+  log_debug_p(gc, init)("Falling back to open/unlink");
+
+  // Create file name
+  char filename[PATH_MAX];
+  snprintf(filename, sizeof(filename), "%s/%s.%d", mountpoint.get(), name, os::current_process_id());
+
+  // Create file
+  const int fd = os::open(filename, O_CREAT|O_EXCL|O_RDWR|O_CLOEXEC, S_IRUSR|S_IWUSR);
+  if (fd == -1) {
+    ZErrno err;
+    log_error_p(gc)("Failed to create file %s (%s)", filename, err.to_string());
+    return -1;
+  }
+
+  // Unlink file
+  if (unlink(filename) == -1) {
+    ZErrno err;
+    log_error_p(gc)("Failed to unlink file %s (%s)", filename, err.to_string());
+    return -1;
+  }
+
+  log_info_p(gc, init)("Heap Backing File: %s", filename);
+
+  return fd;
+}
+
+int ZPhysicalMemoryBacking::create_fd(const char* name) const {
+  if (AllocateHeapAt == NULL) {
+    // If the path is not explicitly specified, then we first try to create a memfd file
+    // instead of looking for a tmpfd/hugetlbfs mount point. Note that memfd_create() might
+    // not be supported at all (requires kernel >= 3.17), or it might not support large
+    // pages (requires kernel >= 4.14). If memfd_create() fails, then we try to create a
+    // file on an accessible tmpfs or hugetlbfs mount point.
+    const int fd = create_mem_fd(name);
+    if (fd != -1) {
+      return fd;
+    }
+
+    log_debug_p(gc)("Falling back to searching for an accessible mount point");
+  }
+
+  return create_file_fd(name);
+}
+
+bool ZPhysicalMemoryBacking::is_initialized() const {
+  return _initialized;
+}
+
+void ZPhysicalMemoryBacking::warn_available_space(size_t max_capacity) const {
+  // Note that the available space on a tmpfs or a hugetlbfs filesystem
+  // will be zero if no size limit was specified when it was mounted.
+  if (_available == 0) {
+    // No size limit set, skip check
+    log_info_p(gc, init)("Available space on backing filesystem: N/A");
+    return;
+  }
+
+  log_info_p(gc, init)("Available space on backing filesystem: " SIZE_FORMAT "M", _available / M);
+
+  // Warn if the filesystem doesn't currently have enough space available to hold
+  // the max heap size. The max heap size will be capped if we later hit this limit
+  // when trying to expand the heap.
+  if (_available < max_capacity) {
+    log_warning_p(gc)("***** WARNING! INCORRECT SYSTEM CONFIGURATION DETECTED! *****");
+    log_warning_p(gc)("Not enough space available on the backing filesystem to hold the current max Java heap");
+    log_warning_p(gc)("size (" SIZE_FORMAT "M). Please adjust the size of the backing filesystem accordingly "
+                      "(available", max_capacity / M);
+    log_warning_p(gc)("space is currently " SIZE_FORMAT "M). Continuing execution with the current filesystem "
+                      "size could", _available / M);
+    log_warning_p(gc)("lead to a premature OutOfMemoryError being thrown, due to failure to commit memory.");
+  }
+}
+
+void ZPhysicalMemoryBacking::warn_max_map_count(size_t max_capacity) const {
+  const char* const filename = ZFILENAME_PROC_MAX_MAP_COUNT;
+  FILE* const file = fopen(filename, "r");
+  if (file == NULL) {
+    // Failed to open file, skip check
+    log_debug_p(gc, init)("Failed to open %s", filename);
+    return;
+  }
+
+  size_t actual_max_map_count = 0;
+  const int result = fscanf(file, SIZE_FORMAT, &actual_max_map_count);
+  fclose(file);
+  if (result != 1) {
+    // Failed to read file, skip check
+    log_debug_p(gc, init)("Failed to read %s", filename);
+    return;
+  }
+
+  // The required max map count is impossible to calculate exactly since subsystems
+  // other than ZGC are also creating memory mappings, and we have no control over that.
+  // However, ZGC tends to create the most mappings and dominate the total count.
+  // In the worst cases, ZGC will map each granule three times, i.e. once per heap view.
+  // We speculate that we need another 20% to allow for non-ZGC subsystems to map memory.
+  const size_t required_max_map_count = (max_capacity / ZGranuleSize) * 3 * 1.2;
+  if (actual_max_map_count < required_max_map_count) {
+    log_warning_p(gc)("***** WARNING! INCORRECT SYSTEM CONFIGURATION DETECTED! *****");
+    log_warning_p(gc)("The system limit on number of memory mappings per process might be too low for the given");
+    log_warning_p(gc)("max Java heap size (" SIZE_FORMAT "M). Please adjust %s to allow for at",
+                      max_capacity / M, filename);
+    log_warning_p(gc)("least " SIZE_FORMAT " mappings (current limit is " SIZE_FORMAT "). Continuing execution "
+                      "with the current", required_max_map_count, actual_max_map_count);
+    log_warning_p(gc)("limit could lead to a premature OutOfMemoryError being thrown, due to failure to map memory.");
+  }
+}
+
+void ZPhysicalMemoryBacking::warn_commit_limits(size_t max_capacity) const {
+  // Warn if available space is too low
+  warn_available_space(max_capacity);
+
+  // Warn if max map count is too low
+  warn_max_map_count(max_capacity);
+}
+
+bool ZPhysicalMemoryBacking::is_tmpfs() const {
+  return _filesystem == TMPFS_MAGIC;
+}
+
+bool ZPhysicalMemoryBacking::is_hugetlbfs() const {
+  return _filesystem == HUGETLBFS_MAGIC;
+}
+
+bool ZPhysicalMemoryBacking::tmpfs_supports_transparent_huge_pages() const {
+  // If the shmem_enabled file exists and is readable then we
+  // know the kernel supports transparent huge pages for tmpfs.
+  return access(ZFILENAME_SHMEM_ENABLED, R_OK) == 0;
+}
+
+ZErrno ZPhysicalMemoryBacking::fallocate_compat_mmap_hugetlbfs(size_t offset, size_t length, bool touch) const {
+  // On hugetlbfs, mapping a file segment will fail immediately, without
+  // the need to touch the mapped pages first, if there aren't enough huge
+  // pages available to back the mapping.
+  void* const addr = mmap(0, length, PROT_READ|PROT_WRITE, MAP_SHARED, _fd, offset);
+  if (addr == MAP_FAILED) {
+    // Failed
+    return errno;
+  }
+
+  // Once mapped, the huge pages are only reserved. We need to touch them
+  // to associate them with the file segment. Note that we can not punch
+  // hole in file segments which only have reserved pages.
+  if (touch) {
+    char* const start = (char*)addr;
+    char* const end = start + length;
+    os::pretouch_memory(start, end, _block_size);
+  }
+
+  // Unmap again. From now on, the huge pages that were mapped are allocated
+  // to this file. There's no risk of getting a SIGBUS when mapping and
+  // touching these pages again.
+  if (munmap(addr, length) == -1) {
+    // Failed
+    return errno;
+  }
+
+  // Success
+  return 0;
+}
+
+static bool safe_touch_mapping(void* addr, size_t length, size_t page_size) {
+  char* const start = (char*)addr;
+  char* const end = start + length;
+
+  // Touching a mapping that can't be backed by memory will generate a
+  // SIGBUS. By using SafeFetch32 any SIGBUS will be safely caught and
+  // handled. On tmpfs, doing a fetch (rather than a store) is enough
+  // to cause backing pages to be allocated (there's no zero-page to
+  // worry about).
+  for (char *p = start; p < end; p += page_size) {
+    if (SafeFetch32((int*)p, -1) == -1) {
+      // Failed
+      return false;
+    }
+  }
+
+  // Success
+  return true;
+}
+
+ZErrno ZPhysicalMemoryBacking::fallocate_compat_mmap_tmpfs(size_t offset, size_t length) const {
+  // On tmpfs, we need to touch the mapped pages to figure out
+  // if there are enough pages available to back the mapping.
+  void* const addr = mmap(0, length, PROT_READ|PROT_WRITE, MAP_SHARED, _fd, offset);
+  if (addr == MAP_FAILED) {
+    // Failed
+    return errno;
+  }
+
+  // Advise mapping to use transparent huge pages
+  os::realign_memory((char*)addr, length, os::large_page_size());
+
+  // Touch the mapping (safely) to make sure it's backed by memory
+  const bool backed = safe_touch_mapping(addr, length, _block_size);
+
+  // Unmap again. If successfully touched, the backing memory will
+  // be allocated to this file. There's no risk of getting a SIGBUS
+  // when mapping and touching these pages again.
+  if (munmap(addr, length) == -1) {
+    // Failed
+    return errno;
+  }
+
+  // Success
+  return backed ? 0 : ENOMEM;
+}
+
+ZErrno ZPhysicalMemoryBacking::fallocate_compat_pwrite(size_t offset, size_t length) const {
+  uint8_t data = 0;
+
+  // Allocate backing memory by writing to each block
+  for (size_t pos = offset; pos < offset + length; pos += _block_size) {
+    if (pwrite(_fd, &data, sizeof(data), pos) == -1) {
+      // Failed
+      return errno;
+    }
+  }
+
+  // Success
+  return 0;
+}
+
+ZErrno ZPhysicalMemoryBacking::fallocate_fill_hole_compat(size_t offset, size_t length) const {
+  // fallocate(2) is only supported by tmpfs since Linux 3.5, and by hugetlbfs
+  // since Linux 4.3. When fallocate(2) is not supported we emulate it using
+  // mmap/munmap (for hugetlbfs and tmpfs with transparent huge pages) or pwrite
+  // (for tmpfs without transparent huge pages and other filesystem types).
+  if (ZLargePages::is_explicit()) {
+    return fallocate_compat_mmap_hugetlbfs(offset, length, false /* touch */);
+  } else if (ZLargePages::is_transparent()) {
+    return fallocate_compat_mmap_tmpfs(offset, length);
+  } else {
+    return fallocate_compat_pwrite(offset, length);
+  }
+}
+
+ZErrno ZPhysicalMemoryBacking::fallocate_fill_hole_syscall(size_t offset, size_t length) const {
+  const int mode = 0; // Allocate
+  const int res = ZSyscall::fallocate(_fd, mode, offset, length);
+  if (res == -1) {
+    // Failed
+    return errno;
+  }
+
+  // Success
+  return 0;
+}
+
+ZErrno ZPhysicalMemoryBacking::fallocate_fill_hole(size_t offset, size_t length) const {
+  // Using compat mode is more efficient when allocating space on hugetlbfs.
+  // Note that allocating huge pages this way will only reserve them, and not
+  // associate them with segments of the file. We must guarantee that we at
+  // some point touch these segments, otherwise we can not punch hole in them.
+  // Also note that we need to use compat mode when using transparent huge pages,
+  // since we need to use madvise(2) on the mapping before the page is allocated.
+  if (z_fallocate_supported && !ZLargePages::is_enabled()) {
+     const ZErrno err = fallocate_fill_hole_syscall(offset, length);
+     if (!err) {
+       // Success
+       return 0;
+     }
+
+     if (err != ENOSYS && err != EOPNOTSUPP) {
+       // Failed
+       return err;
+     }
+
+     // Not supported
+     log_debug_p(gc)("Falling back to fallocate() compatibility mode");
+     z_fallocate_supported = false;
+  }
+
+  return fallocate_fill_hole_compat(offset, length);
+}
+
+ZErrno ZPhysicalMemoryBacking::fallocate_punch_hole(size_t offset, size_t length) const {
+  if (ZLargePages::is_explicit()) {
+    // We can only punch hole in pages that have been touched. Non-touched
+    // pages are only reserved, and not associated with any specific file
+    // segment. We don't know which pages have been previously touched, so
+    // we always touch them here to guarantee that we can punch hole.
+    const ZErrno err = fallocate_compat_mmap_hugetlbfs(offset, length, true /* touch */);
+    if (err) {
+      // Failed
+      return err;
+    }
+  }
+
+  const int mode = FALLOC_FL_PUNCH_HOLE|FALLOC_FL_KEEP_SIZE;
+  if (ZSyscall::fallocate(_fd, mode, offset, length) == -1) {
+    // Failed
+    return errno;
+  }
+
+  // Success
+  return 0;
+}
+
+ZErrno ZPhysicalMemoryBacking::split_and_fallocate(bool punch_hole, size_t offset, size_t length) const {
+  // Try first half
+  const size_t offset0 = offset;
+  const size_t length0 = align_up(length / 2, _block_size);
+  const ZErrno err0 = fallocate(punch_hole, offset0, length0);
+  if (err0) {
+    return err0;
+  }
+
+  // Try second half
+  const size_t offset1 = offset0 + length0;
+  const size_t length1 = length - length0;
+  const ZErrno err1 = fallocate(punch_hole, offset1, length1);
+  if (err1) {
+    return err1;
+  }
+
+  // Success
+  return 0;
+}
+
+ZErrno ZPhysicalMemoryBacking::fallocate(bool punch_hole, size_t offset, size_t length) const {
+  assert(is_aligned(offset, _block_size), "Invalid offset");
+  assert(is_aligned(length, _block_size), "Invalid length");
+
+  const ZErrno err = punch_hole ? fallocate_punch_hole(offset, length) : fallocate_fill_hole(offset, length);
+  if (err == EINTR && length > _block_size) {
+    // Calling fallocate(2) with a large length can take a long time to
+    // complete. When running profilers, such as VTune, this syscall will
+    // be constantly interrupted by signals. Expanding the file in smaller
+    // steps avoids this problem.
+    return split_and_fallocate(punch_hole, offset, length);
+  }
+
+  return err;
+}
+
+bool ZPhysicalMemoryBacking::commit_inner(size_t offset, size_t length) const {
+  log_trace(gc, heap)("Committing memory: " SIZE_FORMAT "M-" SIZE_FORMAT "M (" SIZE_FORMAT "M)",
+                      offset / M, (offset + length) / M, length / M);
+
+retry:
+  const ZErrno err = fallocate(false /* punch_hole */, offset, length);
+  if (err) {
+    if (err == ENOSPC && !is_init_completed() && ZLargePages::is_explicit() && z_fallocate_hugetlbfs_attempts-- > 0) {
+      // If we fail to allocate during initialization, due to lack of space on
+      // the hugetlbfs filesystem, then we wait and retry a few times before
+      // giving up. Otherwise there is a risk that running JVMs back-to-back
+      // will fail, since there is a delay between process termination and the
+      // huge pages owned by that process being returned to the huge page pool
+      // and made available for new allocations.
+      log_debug_p(gc, init)("Failed to commit memory (%s), retrying", err.to_string());
+
+      // Wait and retry in one second, in the hope that huge pages will be
+      // available by then.
+      sleep(1);
+      goto retry;
+    }
+
+    // Failed
+    log_error_p(gc)("Failed to commit memory (%s)", err.to_string());
+    return false;
+  }
+
+  // Success
+  return true;
+}
+
+static int offset_to_node(size_t offset) {
+  const GrowableArray<int>* mapping = os::Linux::numa_nindex_to_node();
+  const size_t nindex = (offset >> ZGranuleSizeShift) % mapping->length();
+  return mapping->at((int)nindex);
+}
+
+size_t ZPhysicalMemoryBacking::commit_numa_interleaved(size_t offset, size_t length) const {
+  size_t committed = 0;
+
+  // Commit one granule at a time, so that each granule
+  // can be allocated from a different preferred node.
+  while (committed < length) {
+    const size_t granule_offset = offset + committed;
+
+    // Setup NUMA policy to allocate memory from a preferred node
+    os::Linux::numa_set_preferred(offset_to_node(granule_offset));
+
+    if (!commit_inner(granule_offset, ZGranuleSize)) {
+      // Failed
+      break;
+    }
+
+    committed += ZGranuleSize;
+  }
+
+  // Restore NUMA policy
+  os::Linux::numa_set_preferred(-1);
+
+  return committed;
+}
+
+size_t ZPhysicalMemoryBacking::commit_default(size_t offset, size_t length) const {
+  // Try to commit the whole region
+  if (commit_inner(offset, length)) {
+    // Success
+    return length;
+  }
+
+  // Failed, try to commit as much as possible
+  size_t start = offset;
+  size_t end = offset + length;
+
+  for (;;) {
+    length = align_down((end - start) / 2, ZGranuleSize);
+    if (length < ZGranuleSize) {
+      // Done, don't commit more
+      return start - offset;
+    }
+
+    if (commit_inner(start, length)) {
+      // Success, try commit more
+      start += length;
+    } else {
+      // Failed, try commit less
+      end -= length;
+    }
+  }
+}
+
+size_t ZPhysicalMemoryBacking::commit(size_t offset, size_t length) const {
+  if (ZNUMA::is_enabled() && !ZLargePages::is_explicit()) {
+    // To get granule-level NUMA interleaving when using non-large pages,
+    // we must explicitly interleave the memory at commit/fallocate time.
+    return commit_numa_interleaved(offset, length);
+  }
+
+  return commit_default(offset, length);
+}
+
+size_t ZPhysicalMemoryBacking::uncommit(size_t offset, size_t length) const {
+  log_trace(gc, heap)("Uncommitting memory: " SIZE_FORMAT "M-" SIZE_FORMAT "M (" SIZE_FORMAT "M)",
+                      offset / M, (offset + length) / M, length / M);
+
+  const ZErrno err = fallocate(true /* punch_hole */, offset, length);
+  if (err) {
+    log_error(gc)("Failed to uncommit memory (%s)", err.to_string());
+    return 0;
+  }
+
+  return length;
+}
+
+void ZPhysicalMemoryBacking::map(uintptr_t addr, size_t size, uintptr_t offset) const {
+  const void* const res = mmap((void*)addr, size, PROT_READ|PROT_WRITE, MAP_FIXED|MAP_SHARED, _fd, offset);
+  if (res == MAP_FAILED) {
+    ZErrno err;
+    fatal("Failed to map memory (%s)", err.to_string());
+  }
+}
+
+void ZPhysicalMemoryBacking::unmap(uintptr_t addr, size_t size) const {
+  // Note that we must keep the address space reservation intact and just detach
+  // the backing memory. For this reason we map a new anonymous, non-accessible
+  // and non-reserved page over the mapping instead of actually unmapping.
+  const void* const res = mmap((void*)addr, size, PROT_NONE, MAP_FIXED | MAP_ANONYMOUS | MAP_PRIVATE | MAP_NORESERVE, -1, 0);
+  if (res == MAP_FAILED) {
+    ZErrno err;
+    fatal("Failed to map memory (%s)", err.to_string());
+  }
+}
diff --git a/src/hotspot/os/serenity/gc/z/zPhysicalMemoryBacking_linux.hpp b/src/hotspot/os/serenity/gc/z/zPhysicalMemoryBacking_linux.hpp
new file mode 100644
index 00000000000..2a9afe5888e
--- /dev/null
+++ b/src/hotspot/os/serenity/gc/z/zPhysicalMemoryBacking_linux.hpp
@@ -0,0 +1,77 @@
+/*
+ * Copyright (c) 2015, 2020, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ */
+
+#ifndef OS_SERENITY_GC_Z_ZPHYSICALMEMORYBACKING_SERENITY_HPP
+#define OS_SERENITY_GC_Z_ZPHYSICALMEMORYBACKING_SERENITY_HPP
+
+class ZErrno;
+
+class ZPhysicalMemoryBacking {
+private:
+  int      _fd;
+  size_t   _size;
+  uint64_t _filesystem;
+  size_t   _block_size;
+  size_t   _available;
+  bool     _initialized;
+
+  void warn_available_space(size_t max_capacity) const;
+  void warn_max_map_count(size_t max_capacity) const;
+
+  int create_mem_fd(const char* name) const;
+  int create_file_fd(const char* name) const;
+  int create_fd(const char* name) const;
+
+  bool is_tmpfs() const;
+  bool is_hugetlbfs() const;
+  bool tmpfs_supports_transparent_huge_pages() const;
+
+  ZErrno fallocate_compat_mmap_hugetlbfs(size_t offset, size_t length, bool touch) const;
+  ZErrno fallocate_compat_mmap_tmpfs(size_t offset, size_t length) const;
+  ZErrno fallocate_compat_pwrite(size_t offset, size_t length) const;
+  ZErrno fallocate_fill_hole_compat(size_t offset, size_t length) const;
+  ZErrno fallocate_fill_hole_syscall(size_t offset, size_t length) const;
+  ZErrno fallocate_fill_hole(size_t offset, size_t length) const;
+  ZErrno fallocate_punch_hole(size_t offset, size_t length) const;
+  ZErrno split_and_fallocate(bool punch_hole, size_t offset, size_t length) const;
+  ZErrno fallocate(bool punch_hole, size_t offset, size_t length) const;
+
+  bool commit_inner(size_t offset, size_t length) const;
+  size_t commit_numa_interleaved(size_t offset, size_t length) const;
+  size_t commit_default(size_t offset, size_t length) const;
+
+public:
+  ZPhysicalMemoryBacking(size_t max_capacity);
+
+  bool is_initialized() const;
+
+  void warn_commit_limits(size_t max_capacity) const;
+
+  size_t commit(size_t offset, size_t length) const;
+  size_t uncommit(size_t offset, size_t length) const;
+
+  void map(uintptr_t addr, size_t size, uintptr_t offset) const;
+  void unmap(uintptr_t addr, size_t size) const;
+};
+
+#endif // OS_SERENITY_GC_Z_ZPHYSICALMEMORYBACKING_SERENITY_HPP
diff --git a/src/hotspot/os/serenity/gc/z/zSyscall_linux.cpp b/src/hotspot/os/serenity/gc/z/zSyscall_linux.cpp
new file mode 100644
index 00000000000..ba26cbfcb24
--- /dev/null
+++ b/src/hotspot/os/serenity/gc/z/zSyscall_linux.cpp
@@ -0,0 +1,40 @@
+/*
+ * Copyright (c) 2019, 2020, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ */
+
+#include "precompiled.hpp"
+#include "gc/z/zSyscall_linux.hpp"
+#include OS_CPU_HEADER(gc/z/zSyscall)
+
+#include <unistd.h>
+
+int ZSyscall::memfd_create(const char *name, unsigned int flags) {
+  return syscall(SYS_memfd_create, name, flags);
+}
+
+int ZSyscall::fallocate(int fd, int mode, size_t offset, size_t length) {
+  return syscall(SYS_fallocate, fd, mode, offset, length);
+}
+
+long ZSyscall::get_mempolicy(int* mode, unsigned long* nodemask, unsigned long maxnode, void* addr, unsigned long flags) {
+  return syscall(SYS_get_mempolicy, mode, nodemask, maxnode, addr, flags);
+}
diff --git a/src/hotspot/os/serenity/gc/z/zSyscall_linux.hpp b/src/hotspot/os/serenity/gc/z/zSyscall_linux.hpp
new file mode 100644
index 00000000000..a39c0ab545e
--- /dev/null
+++ b/src/hotspot/os/serenity/gc/z/zSyscall_linux.hpp
@@ -0,0 +1,44 @@
+/*
+ * Copyright (c) 2019, 2020, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ */
+
+#ifndef OS_SERENITY_GC_Z_ZSYSCALL_SERENITY_HPP
+#define OS_SERENITY_GC_Z_ZSYSCALL_SERENITY_HPP
+
+#include "memory/allocation.hpp"
+
+// Flags for get_mempolicy()
+#ifndef MPOL_F_NODE
+#define MPOL_F_NODE        (1<<0)
+#endif
+#ifndef MPOL_F_ADDR
+#define MPOL_F_ADDR        (1<<1)
+#endif
+
+class ZSyscall : public AllStatic {
+public:
+  static int memfd_create(const char* name, unsigned int flags);
+  static int fallocate(int fd, int mode, size_t offset, size_t length);
+  static long get_mempolicy(int* mode, unsigned long* nodemask, unsigned long maxnode, void* addr, unsigned long flags);
+};
+
+#endif // OS_SERENITY_GC_Z_ZSYSCALL_SERENITY_HPP
diff --git a/src/hotspot/os/serenity/globals_serenity.hpp b/src/hotspot/os/serenity/globals_serenity.hpp
new file mode 100644
index 00000000000..b575862978e
--- /dev/null
+++ b/src/hotspot/os/serenity/globals_serenity.hpp
@@ -0,0 +1,98 @@
+/*
+ * Copyright (c) 2005, 2020, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#ifndef OS_SERENITY_GLOBALS_SERENITY_HPP
+#define OS_SERENITY_GLOBALS_SERENITY_HPP
+
+//
+// Declare Linux specific flags. They are not available on other platforms.
+//
+#define RUNTIME_OS_FLAGS(develop,                                       \
+                         develop_pd,                                    \
+                         product,                                       \
+                         product_pd,                                    \
+                         notproduct,                                    \
+                         range,                                         \
+                         constraint)                                    \
+                                                                        \
+  product(bool, UseOprofile, false,                                     \
+        "enable support for Oprofile profiler")                         \
+                                                                        \
+  /*  NB: The default value of UseLinuxPosixThreadCPUClocks may be   */ \
+  /* overridden in Arguments::parse_each_vm_init_arg.                */ \
+  product(bool, UseLinuxPosixThreadCPUClocks, true,                     \
+          "enable fast Linux Posix clocks where available")             \
+                                                                        \
+  product(bool, UseHugeTLBFS, false,                                    \
+          "Use MAP_HUGETLB for large pages")                            \
+                                                                        \
+  product(bool, UseTransparentHugePages, false,                         \
+          "Use MADV_HUGEPAGE for large pages")                          \
+                                                                        \
+  product(bool, LoadExecStackDllInVMThread, true,                       \
+          "Load DLLs with executable-stack attribute in the VM Thread") \
+                                                                        \
+  product(bool, UseSHM, false,                                          \
+          "Use SYSV shared memory for large pages")                     \
+                                                                        \
+  product(bool, UseContainerSupport, true,                              \
+          "Enable detection and runtime container configuration support") \
+                                                                        \
+  product(bool, PreferContainerQuotaForCPUCount, true,                  \
+          "Calculate the container CPU availability based on the value" \
+          " of quotas (if set), when true. Otherwise, use the CPU"      \
+          " shares value, provided it is less than quota.")             \
+                                                                        \
+  product(bool, AdjustStackSizeForTLS, false,                           \
+          "Increase the thread stack size to include space for glibc "  \
+          "static thread-local storage (TLS) if true")                  \
+                                                                        \
+  product(bool, DumpPrivateMappingsInCore, true, DIAGNOSTIC,            \
+          "If true, sets bit 2 of /proc/PID/coredump_filter, thus "     \
+          "resulting in file-backed private mappings of the process to "\
+          "be dumped into the corefile.")                               \
+                                                                        \
+  product(bool, DumpSharedMappingsInCore, true, DIAGNOSTIC,             \
+          "If true, sets bit 3 of /proc/PID/coredump_filter, thus "     \
+          "resulting in file-backed shared mappings of the process to " \
+          "be dumped into the corefile.")                               \
+                                                                        \
+  product(bool, UseCpuAllocPath, false, DIAGNOSTIC,                     \
+          "Use CPU_ALLOC code path in os::active_processor_count ")     \
+                                                                        \
+  product(bool, DumpPerfMapAtExit, false, DIAGNOSTIC,                   \
+          "Write map file for Linux perf tool at exit")
+
+// end of RUNTIME_OS_FLAGS
+
+//
+// Defines Linux-specific default values. The flags are available on all
+// platforms, but they may have different default values on other platforms.
+//
+define_pd_global(size_t, PreTouchParallelChunkSize, 4 * M);
+define_pd_global(bool, UseLargePages, false);
+define_pd_global(bool, UseLargePagesIndividualAllocation, false);
+define_pd_global(bool, UseThreadPriorities, true) ;
+
+#endif // OS_SERENITY_GLOBALS_SERENITY_HPP
diff --git a/src/hotspot/os/serenity/osContainer_serenity.cpp b/src/hotspot/os/serenity/osContainer_serenity.cpp
new file mode 100644
index 00000000000..b89cfd676eb
--- /dev/null
+++ b/src/hotspot/os/serenity/osContainer_serenity.cpp
@@ -0,0 +1,131 @@
+/*
+ * Copyright (c) 2017, 2020, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#include <string.h>
+#include <math.h>
+#include <errno.h>
+#include "runtime/globals.hpp"
+#include "runtime/os.hpp"
+#include "logging/log.hpp"
+#include "osContainer_linux.hpp"
+#include "cgroupSubsystem_linux.hpp"
+
+
+bool  OSContainer::_is_initialized   = false;
+bool  OSContainer::_is_containerized = false;
+CgroupSubsystem* cgroup_subsystem;
+
+/* init
+ *
+ * Initialize the container support and determine if
+ * we are running under cgroup control.
+ */
+void OSContainer::init() {
+  jlong mem_limit;
+
+  assert(!_is_initialized, "Initializing OSContainer more than once");
+
+  _is_initialized = true;
+  _is_containerized = false;
+
+  log_trace(os, container)("OSContainer::init: Initializing Container Support");
+  if (!UseContainerSupport) {
+    log_trace(os, container)("Container Support not enabled");
+    return;
+  }
+
+  cgroup_subsystem = CgroupSubsystemFactory::create();
+  if (cgroup_subsystem == NULL) {
+    return; // Required subsystem files not found or other error
+  }
+  // We need to update the amount of physical memory now that
+  // cgroup subsystem files have been processed.
+  if ((mem_limit = cgroup_subsystem->memory_limit_in_bytes()) > 0) {
+    os::Linux::set_physical_memory(mem_limit);
+    log_info(os, container)("Memory Limit is: " JLONG_FORMAT, mem_limit);
+  }
+
+  _is_containerized = true;
+
+}
+
+const char * OSContainer::container_type() {
+  assert(cgroup_subsystem != NULL, "cgroup subsystem not available");
+  return cgroup_subsystem->container_type();
+}
+
+jlong OSContainer::memory_limit_in_bytes() {
+  assert(cgroup_subsystem != NULL, "cgroup subsystem not available");
+  return cgroup_subsystem->memory_limit_in_bytes();
+}
+
+jlong OSContainer::memory_and_swap_limit_in_bytes() {
+  assert(cgroup_subsystem != NULL, "cgroup subsystem not available");
+  return cgroup_subsystem->memory_and_swap_limit_in_bytes();
+}
+
+jlong OSContainer::memory_soft_limit_in_bytes() {
+  assert(cgroup_subsystem != NULL, "cgroup subsystem not available");
+  return cgroup_subsystem->memory_soft_limit_in_bytes();
+}
+
+jlong OSContainer::memory_usage_in_bytes() {
+  assert(cgroup_subsystem != NULL, "cgroup subsystem not available");
+  return cgroup_subsystem->memory_usage_in_bytes();
+}
+
+jlong OSContainer::memory_max_usage_in_bytes() {
+  assert(cgroup_subsystem != NULL, "cgroup subsystem not available");
+  return cgroup_subsystem->memory_max_usage_in_bytes();
+}
+
+char * OSContainer::cpu_cpuset_cpus() {
+  assert(cgroup_subsystem != NULL, "cgroup subsystem not available");
+  return cgroup_subsystem->cpu_cpuset_cpus();
+}
+
+char * OSContainer::cpu_cpuset_memory_nodes() {
+  assert(cgroup_subsystem != NULL, "cgroup subsystem not available");
+  return cgroup_subsystem->cpu_cpuset_memory_nodes();
+}
+
+int OSContainer::active_processor_count() {
+  assert(cgroup_subsystem != NULL, "cgroup subsystem not available");
+  return cgroup_subsystem->active_processor_count();
+}
+
+int OSContainer::cpu_quota() {
+  assert(cgroup_subsystem != NULL, "cgroup subsystem not available");
+  return cgroup_subsystem->cpu_quota();
+}
+
+int OSContainer::cpu_period() {
+  assert(cgroup_subsystem != NULL, "cgroup subsystem not available");
+  return cgroup_subsystem->cpu_period();
+}
+
+int OSContainer::cpu_shares() {
+  assert(cgroup_subsystem != NULL, "cgroup subsystem not available");
+  return cgroup_subsystem->cpu_shares();
+}
diff --git a/src/hotspot/os/serenity/osContainer_serenity.hpp b/src/hotspot/os/serenity/osContainer_serenity.hpp
new file mode 100644
index 00000000000..4ab19c12832
--- /dev/null
+++ b/src/hotspot/os/serenity/osContainer_serenity.hpp
@@ -0,0 +1,71 @@
+/*
+ * Copyright (c) 2017, 2020, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#ifndef OS_SERENITY_OSCONTAINER_SERENITY_HPP
+#define OS_SERENITY_OSCONTAINER_SERENITY_HPP
+
+#include "utilities/globalDefinitions.hpp"
+#include "utilities/macros.hpp"
+#include "memory/allocation.hpp"
+
+#define OSCONTAINER_ERROR (-2)
+
+// 20ms timeout between re-reads of memory limit and _active_processor_count.
+#define OSCONTAINER_CACHE_TIMEOUT (NANOSECS_PER_SEC/50)
+
+class OSContainer: AllStatic {
+
+ private:
+  static bool   _is_initialized;
+  static bool   _is_containerized;
+  static int    _active_processor_count;
+
+ public:
+  static void init();
+  static inline bool is_containerized();
+  static const char * container_type();
+
+  static jlong memory_limit_in_bytes();
+  static jlong memory_and_swap_limit_in_bytes();
+  static jlong memory_soft_limit_in_bytes();
+  static jlong memory_usage_in_bytes();
+  static jlong memory_max_usage_in_bytes();
+
+  static int active_processor_count();
+
+  static char * cpu_cpuset_cpus();
+  static char * cpu_cpuset_memory_nodes();
+
+  static int cpu_quota();
+  static int cpu_period();
+
+  static int cpu_shares();
+
+};
+
+inline bool OSContainer::is_containerized() {
+  return _is_containerized;
+}
+
+#endif // OS_SERENITY_OSCONTAINER_SERENITY_HPP
diff --git a/src/hotspot/os/serenity/osThread_serenity.cpp b/src/hotspot/os/serenity/osThread_serenity.cpp
new file mode 100644
index 00000000000..6f7e074a522
--- /dev/null
+++ b/src/hotspot/os/serenity/osThread_serenity.cpp
@@ -0,0 +1,50 @@
+/*
+ * Copyright (c) 1999, 2014, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+// no precompiled headers
+#include "memory/allocation.inline.hpp"
+#include "runtime/mutex.hpp"
+#include "runtime/osThread.hpp"
+
+#include <signal.h>
+
+void OSThread::pd_initialize() {
+  assert(this != NULL, "check");
+  _thread_id        = 0;
+  _pthread_id       = 0;
+  _siginfo = NULL;
+  _ucontext = NULL;
+  _expanding_stack = 0;
+  _alt_sig_stack = NULL;
+
+  sigemptyset(&_caller_sigmask);
+
+  _startThread_lock = new Monitor(Mutex::event, "startThread_lock", true,
+                                  Monitor::_safepoint_check_never);
+  assert(_startThread_lock !=NULL, "check");
+}
+
+void OSThread::pd_destroy() {
+  delete _startThread_lock;
+}
diff --git a/src/hotspot/os/serenity/osThread_serenity.hpp b/src/hotspot/os/serenity/osThread_serenity.hpp
new file mode 100644
index 00000000000..ab940be4c3d
--- /dev/null
+++ b/src/hotspot/os/serenity/osThread_serenity.hpp
@@ -0,0 +1,135 @@
+/*
+ * Copyright (c) 1999, 2019, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#ifndef OS_SERENITY_OSTHREAD_SERENITY_HPP
+#define OS_SERENITY_OSTHREAD_SERENITY_HPP
+ public:
+  typedef pid_t thread_id_t;
+
+ private:
+  int _thread_type;
+
+ public:
+
+  int thread_type() const {
+    return _thread_type;
+  }
+  void set_thread_type(int type) {
+    _thread_type = type;
+  }
+
+  // _pthread_id is the pthread id, which is used by library calls
+  // (e.g. pthread_kill).
+  pthread_t _pthread_id;
+
+  sigset_t _caller_sigmask; // Caller's signal mask
+
+ public:
+
+  // Methods to save/restore caller's signal mask
+  sigset_t  caller_sigmask() const       { return _caller_sigmask; }
+  void    set_caller_sigmask(sigset_t sigmask)  { _caller_sigmask = sigmask; }
+
+#ifndef PRODUCT
+  // Used for debugging, return a unique integer for each thread.
+  int thread_identifier() const   { return _thread_id; }
+#endif
+#ifdef ASSERT
+  // We expect no reposition failures so kill vm if we get one.
+  //
+  bool valid_reposition_failure() {
+    return false;
+  }
+#endif // ASSERT
+  pthread_t pthread_id() const {
+    return _pthread_id;
+  }
+  void set_pthread_id(pthread_t tid) {
+    _pthread_id = tid;
+  }
+
+  // ***************************************************************
+  // suspension support.
+  // ***************************************************************
+
+public:
+  // flags that support signal based suspend/resume on Linux are in a
+  // separate class to avoid confusion with many flags in OSThread that
+  // are used by VM level suspend/resume.
+  os::SuspendResume sr;
+
+  // _ucontext and _siginfo are used by SR_handler() to save thread context,
+  // and they will later be used to walk the stack or reposition thread PC.
+  // If the thread is not suspended in SR_handler() (e.g. self suspend),
+  // the value in _ucontext is meaningless, so we must use the last Java
+  // frame information as the frame. This will mean that for threads
+  // that are parked on a mutex the profiler (and safepoint mechanism)
+  // will see the thread as if it were still in the Java frame. This
+  // not a problem for the profiler since the Java frame is a close
+  // enough result. For the safepoint mechanism when the give it the
+  // Java frame we are not at a point where the safepoint needs the
+  // frame to that accurate (like for a compiled safepoint) since we
+  // should be in a place where we are native and will block ourselves
+  // if we transition.
+private:
+  void* _siginfo;
+  ucontext_t* _ucontext;
+  int _expanding_stack;                 /* non zero if manually expanding stack */
+  address _alt_sig_stack;               /* address of base of alternate signal stack */
+
+public:
+  void* siginfo() const                   { return _siginfo;  }
+  void set_siginfo(void* ptr)             { _siginfo = ptr;   }
+  ucontext_t* ucontext() const            { return _ucontext; }
+  void set_ucontext(ucontext_t* ptr)      { _ucontext = ptr;  }
+  void set_expanding_stack(void)          { _expanding_stack = 1;  }
+  void clear_expanding_stack(void)        { _expanding_stack = 0;  }
+  int  expanding_stack(void)              { return _expanding_stack;  }
+
+  void set_alt_sig_stack(address val)     { _alt_sig_stack = val; }
+  address alt_sig_stack(void)             { return _alt_sig_stack; }
+
+private:
+  Monitor* _startThread_lock;     // sync parent and child in thread creation
+
+public:
+
+  Monitor* startThread_lock() const {
+    return _startThread_lock;
+  }
+
+  // ***************************************************************
+  // Platform dependent initialization and cleanup
+  // ***************************************************************
+
+private:
+
+  void pd_initialize();
+  void pd_destroy();
+
+// Reconciliation History
+// osThread_solaris.hpp 1.24 99/08/27 13:11:54
+// End
+
+#endif // OS_SERENITY_OSTHREAD_SERENITY_HPP
diff --git a/src/hotspot/os/serenity/os_perf_serenity.cpp b/src/hotspot/os/serenity/os_perf_serenity.cpp
new file mode 100644
index 00000000000..370a9ae9816
--- /dev/null
+++ b/src/hotspot/os/serenity/os_perf_serenity.cpp
@@ -0,0 +1,1052 @@
+/*
+ * Copyright (c) 2012, 2020, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#include "precompiled.hpp"
+#include "jvm.h"
+#include "memory/allocation.inline.hpp"
+#include "os_linux.inline.hpp"
+#include "runtime/os.hpp"
+#include "runtime/os_perf.hpp"
+#include "utilities/globalDefinitions.hpp"
+
+#include CPU_HEADER(vm_version_ext)
+
+#include <stdio.h>
+#include <stdarg.h>
+#include <unistd.h>
+#include <errno.h>
+#include <string.h>
+#include <sys/resource.h>
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <dirent.h>
+#include <stdlib.h>
+#include <dlfcn.h>
+#include <pthread.h>
+#include <limits.h>
+#include <ifaddrs.h>
+#include <fcntl.h>
+
+/**
+   /proc/[number]/stat
+              Status information about the process.  This is used by ps(1).  It is defined in /usr/src/linux/fs/proc/array.c.
+
+              The fields, in order, with their proper scanf(3) format specifiers, are:
+
+              1. pid %d The process id.
+
+              2. comm %s
+                     The filename of the executable, in parentheses.  This is visible whether or not the executable is swapped out.
+
+              3. state %c
+                     One  character  from  the  string "RSDZTW" where R is running, S is sleeping in an interruptible wait, D is waiting in uninterruptible disk
+                     sleep, Z is zombie, T is traced or stopped (on a signal), and W is paging.
+
+              4. ppid %d
+                     The PID of the parent.
+
+              5. pgrp %d
+                     The process group ID of the process.
+
+              6. session %d
+                     The session ID of the process.
+
+              7. tty_nr %d
+                     The tty the process uses.
+
+              8. tpgid %d
+                     The process group ID of the process which currently owns the tty that the process is connected to.
+
+              9. flags %lu
+                     The flags of the process.  The math bit is decimal 4, and the traced bit is decimal 10.
+
+              10. minflt %lu
+                     The number of minor faults the process has made which have not required loading a memory page from disk.
+
+              11. cminflt %lu
+                     The number of minor faults that the process's waited-for children have made.
+
+              12. majflt %lu
+                     The number of major faults the process has made which have required loading a memory page from disk.
+
+              13. cmajflt %lu
+                     The number of major faults that the process's waited-for children have made.
+
+              14. utime %lu
+                     The number of jiffies that this process has been scheduled in user mode.
+
+              15. stime %lu
+                     The number of jiffies that this process has been scheduled in kernel mode.
+
+              16. cutime %ld
+                     The number of jiffies that this process's waited-for children have been scheduled in user mode. (See also times(2).)
+
+              17. cstime %ld
+                     The number of jiffies that this process' waited-for children have been scheduled in kernel mode.
+
+              18. priority %ld
+                     The standard nice value, plus fifteen.  The value is never negative in the kernel.
+
+              19. nice %ld
+                     The nice value ranges from 19 (nicest) to -19 (not nice to others).
+
+              20. 0 %ld  This value is hard coded to 0 as a placeholder for a removed field.
+
+              21. itrealvalue %ld
+                     The time in jiffies before the next SIGALRM is sent to the process due to an interval timer.
+
+              22. starttime %lu
+                     The time in jiffies the process started after system boot.
+
+              23. vsize %lu
+                     Virtual memory size in bytes.
+
+              24. rss %ld
+                     Resident Set Size: number of pages the process has in real memory, minus 3 for administrative purposes. This is just the pages which  count
+                     towards text, data, or stack space.  This does not include pages which have not been demand-loaded in, or which are swapped out.
+
+              25. rlim %lu
+                     Current limit in bytes on the rss of the process (usually 4294967295 on i386).
+
+              26. startcode %lu
+                     The address above which program text can run.
+
+              27. endcode %lu
+                     The address below which program text can run.
+
+              28. startstack %lu
+                     The address of the start of the stack.
+
+              29. kstkesp %lu
+                     The current value of esp (stack pointer), as found in the kernel stack page for the process.
+
+              30. kstkeip %lu
+                     The current EIP (instruction pointer).
+
+              31. signal %lu
+                     The bitmap of pending signals (usually 0).
+
+              32. blocked %lu
+                     The bitmap of blocked signals (usually 0, 2 for shells).
+
+              33. sigignore %lu
+                     The bitmap of ignored signals.
+
+              34. sigcatch %lu
+                     The bitmap of catched signals.
+
+              35. wchan %lu
+                     This  is the "channel" in which the process is waiting.  It is the address of a system call, and can be looked up in a namelist if you need
+                     a textual name.  (If you have an up-to-date /etc/psdatabase, then try ps -l to see the WCHAN field in action.)
+
+              36. nswap %lu
+                     Number of pages swapped - not maintained.
+
+              37. cnswap %lu
+                     Cumulative nswap for child processes.
+
+              38. exit_signal %d
+                     Signal to be sent to parent when we die.
+
+              39. processor %d
+                     CPU number last executed on.
+
+
+
+ ///// SSCANF FORMAT STRING. Copy and use.
+
+field:        1  2  3  4  5  6  7  8  9   10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38 39
+format:       %d %s %c %d %d %d %d %d %lu %lu %lu %lu %lu %lu %lu %ld %ld %ld %ld %ld %ld %lu %lu %ld %lu %lu %lu %lu %lu %lu %lu %lu %lu %lu %lu %lu %lu %d %d
+
+
+*/
+
+/**
+ * For platforms that have them, when declaring
+ * a printf-style function,
+ *   formatSpec is the parameter number (starting at 1)
+ *       that is the format argument ("%d pid %s")
+ *   params is the parameter number where the actual args to
+ *       the format starts. If the args are in a va_list, this
+ *       should be 0.
+ */
+#ifndef PRINTF_ARGS
+#  define PRINTF_ARGS(formatSpec,  params) ATTRIBUTE_PRINTF(formatSpec, params)
+#endif
+
+#ifndef SCANF_ARGS
+#  define SCANF_ARGS(formatSpec,   params) ATTRIBUTE_SCANF(formatSpec, params)
+#endif
+
+#ifndef _PRINTFMT_
+#  define _PRINTFMT_
+#endif
+
+#ifndef _SCANFMT_
+#  define _SCANFMT_
+#endif
+
+typedef enum {
+  CPU_LOAD_VM_ONLY,
+  CPU_LOAD_GLOBAL,
+} CpuLoadTarget;
+
+enum {
+  UNDETECTED,
+  UNDETECTABLE,
+  SERENITY26_NPTL,
+  BAREMETAL
+};
+
+struct CPUPerfCounters {
+  int   nProcs;
+  os::Linux::CPUPerfTicks jvmTicks;
+  os::Linux::CPUPerfTicks* cpus;
+};
+
+static double get_cpu_load(int which_logical_cpu, CPUPerfCounters* counters, double* pkernelLoad, CpuLoadTarget target);
+
+/** reads /proc/<pid>/stat data, with some checks and some skips.
+ *  Ensure that 'fmt' does _NOT_ contain the first two "%d %s"
+ */
+static int SCANF_ARGS(2, 0) vread_statdata(const char* procfile, _SCANFMT_ const char* fmt, va_list args) {
+  FILE*f;
+  int n;
+  char buf[2048];
+
+  if ((f = fopen(procfile, "r")) == NULL) {
+    return -1;
+  }
+
+  if ((n = fread(buf, 1, sizeof(buf), f)) != -1) {
+    char *tmp;
+
+    buf[n-1] = '\0';
+    /** skip through pid and exec name. */
+    if ((tmp = strrchr(buf, ')')) != NULL) {
+      // skip the ')' and the following space
+      // but check that buffer is long enough
+      tmp += 2;
+      if (tmp < buf + n) {
+        n = vsscanf(tmp, fmt, args);
+      }
+    }
+  }
+
+  fclose(f);
+
+  return n;
+}
+
+static int SCANF_ARGS(2, 3) read_statdata(const char* procfile, _SCANFMT_ const char* fmt, ...) {
+  int   n;
+  va_list args;
+
+  va_start(args, fmt);
+  n = vread_statdata(procfile, fmt, args);
+  va_end(args);
+  return n;
+}
+
+static FILE* open_statfile(void) {
+  FILE *f;
+
+  if ((f = fopen("/proc/stat", "r")) == NULL) {
+    static int haveWarned = 0;
+    if (!haveWarned) {
+      haveWarned = 1;
+    }
+  }
+  return f;
+}
+
+static int get_systemtype(void) {
+  static int procEntriesType = UNDETECTED;
+  DIR *taskDir;
+
+  if (procEntriesType != UNDETECTED) {
+    return procEntriesType;
+  }
+
+  // Check whether we have a task subdirectory
+  if ((taskDir = opendir("/proc/self/task")) == NULL) {
+    procEntriesType = UNDETECTABLE;
+  } else {
+    // The task subdirectory exists; we're on a Linux >= 2.6 system
+    closedir(taskDir);
+    procEntriesType = SERENITY26_NPTL;
+  }
+
+  return procEntriesType;
+}
+
+/** read user and system ticks from a named procfile, assumed to be in 'stat' format then. */
+static int read_ticks(const char* procfile, uint64_t* userTicks, uint64_t* systemTicks) {
+  return read_statdata(procfile, "%*c %*d %*d %*d %*d %*d %*u %*u %*u %*u %*u " UINT64_FORMAT " " UINT64_FORMAT,
+    userTicks, systemTicks);
+}
+
+/**
+ * Return the number of ticks spent in any of the processes belonging
+ * to the JVM on any CPU.
+ */
+static OSReturn get_jvm_ticks(os::Linux::CPUPerfTicks* pticks) {
+  uint64_t userTicks;
+  uint64_t systemTicks;
+
+  if (get_systemtype() != SERENITY26_NPTL) {
+    return OS_ERR;
+  }
+
+  if (read_ticks("/proc/self/stat", &userTicks, &systemTicks) != 2) {
+    return OS_ERR;
+  }
+
+  // get the total
+  if (! os::Linux::get_tick_information(pticks, -1)) {
+    return OS_ERR;
+  }
+
+  pticks->used       = userTicks;
+  pticks->usedKernel = systemTicks;
+
+  return OS_OK;
+}
+
+/**
+ * Return the load of the CPU as a double. 1.0 means the CPU process uses all
+ * available time for user or system processes, 0.0 means the CPU uses all time
+ * being idle.
+ *
+ * Returns a negative value if there is a problem in determining the CPU load.
+ */
+static double get_cpu_load(int which_logical_cpu, CPUPerfCounters* counters, double* pkernelLoad, CpuLoadTarget target) {
+  uint64_t udiff, kdiff, tdiff;
+  os::Linux::CPUPerfTicks* pticks;
+  os::Linux::CPUPerfTicks  tmp;
+  double user_load;
+
+  *pkernelLoad = 0.0;
+
+  if (target == CPU_LOAD_VM_ONLY) {
+    pticks = &counters->jvmTicks;
+  } else if (-1 == which_logical_cpu) {
+    pticks = &counters->cpus[counters->nProcs];
+  } else {
+    pticks = &counters->cpus[which_logical_cpu];
+  }
+
+  tmp = *pticks;
+
+  if (target == CPU_LOAD_VM_ONLY) {
+    if (get_jvm_ticks(pticks) != OS_OK) {
+      return -1.0;
+    }
+  } else if (! os::Linux::get_tick_information(pticks, which_logical_cpu)) {
+    return -1.0;
+  }
+
+  // seems like we sometimes end up with less kernel ticks when
+  // reading /proc/self/stat a second time, timing issue between cpus?
+  if (pticks->usedKernel < tmp.usedKernel) {
+    kdiff = 0;
+  } else {
+    kdiff = pticks->usedKernel - tmp.usedKernel;
+  }
+  tdiff = pticks->total - tmp.total;
+  udiff = pticks->used - tmp.used;
+
+  if (tdiff == 0) {
+    return 0.0;
+  } else if (tdiff < (udiff + kdiff)) {
+    tdiff = udiff + kdiff;
+  }
+  *pkernelLoad = (kdiff / (double)tdiff);
+  // BUG9044876, normalize return values to sane values
+  *pkernelLoad = MAX2<double>(*pkernelLoad, 0.0);
+  *pkernelLoad = MIN2<double>(*pkernelLoad, 1.0);
+
+  user_load = (udiff / (double)tdiff);
+  user_load = MAX2<double>(user_load, 0.0);
+  user_load = MIN2<double>(user_load, 1.0);
+
+  return user_load;
+}
+
+static int SCANF_ARGS(1, 2) parse_stat(_SCANFMT_ const char* fmt, ...) {
+  FILE *f;
+  va_list args;
+
+  va_start(args, fmt);
+
+  if ((f = open_statfile()) == NULL) {
+    va_end(args);
+    return OS_ERR;
+  }
+  for (;;) {
+    char line[80];
+    if (fgets(line, sizeof(line), f) != NULL) {
+      if (vsscanf(line, fmt, args) == 1) {
+        fclose(f);
+        va_end(args);
+        return OS_OK;
+      }
+    } else {
+        fclose(f);
+        va_end(args);
+        return OS_ERR;
+    }
+  }
+}
+
+static int get_noof_context_switches(uint64_t* switches) {
+  return parse_stat("ctxt " UINT64_FORMAT "\n", switches);
+}
+
+/** returns boot time in _seconds_ since epoch */
+static int get_boot_time(uint64_t* time) {
+  return parse_stat("btime " UINT64_FORMAT "\n", time);
+}
+
+static int perf_context_switch_rate(double* rate) {
+  static pthread_mutex_t contextSwitchLock = PTHREAD_MUTEX_INITIALIZER;
+  static uint64_t      bootTime;
+  static uint64_t      lastTimeNanos;
+  static uint64_t      lastSwitches;
+  static double        lastRate;
+
+  uint64_t bt = 0;
+  int res = 0;
+
+  // First time through bootTime will be zero.
+  if (bootTime == 0) {
+    uint64_t tmp;
+    if (get_boot_time(&tmp) < 0) {
+      return OS_ERR;
+    }
+    bt = tmp * 1000;
+  }
+
+  res = OS_OK;
+
+  pthread_mutex_lock(&contextSwitchLock);
+  {
+
+    uint64_t sw;
+    s8 t, d;
+
+    if (bootTime == 0) {
+      // First interval is measured from boot time which is
+      // seconds since the epoch. Thereafter we measure the
+      // elapsed time using javaTimeNanos as it is monotonic-
+      // non-decreasing.
+      lastTimeNanos = os::javaTimeNanos();
+      t = os::javaTimeMillis();
+      d = t - bt;
+      // keep bootTime zero for now to use as a first-time-through flag
+    } else {
+      t = os::javaTimeNanos();
+      d = nanos_to_millis(t - lastTimeNanos);
+    }
+
+    if (d == 0) {
+      *rate = lastRate;
+    } else if (get_noof_context_switches(&sw) == 0) {
+      *rate      = ( (double)(sw - lastSwitches) / d ) * 1000;
+      lastRate     = *rate;
+      lastSwitches = sw;
+      if (bootTime != 0) {
+        lastTimeNanos = t;
+      }
+    } else {
+      *rate = 0;
+      res   = OS_ERR;
+    }
+    if (*rate <= 0) {
+      *rate = 0;
+      lastRate = 0;
+    }
+
+    if (bootTime == 0) {
+      bootTime = bt;
+    }
+  }
+  pthread_mutex_unlock(&contextSwitchLock);
+
+  return res;
+}
+
+class CPUPerformanceInterface::CPUPerformance : public CHeapObj<mtInternal> {
+  friend class CPUPerformanceInterface;
+ private:
+  CPUPerfCounters _counters;
+
+  int cpu_load(int which_logical_cpu, double* cpu_load);
+  int context_switch_rate(double* rate);
+  int cpu_load_total_process(double* cpu_load);
+  int cpu_loads_process(double* pjvmUserLoad, double* pjvmKernelLoad, double* psystemTotalLoad);
+
+ public:
+  CPUPerformance();
+  bool initialize();
+  ~CPUPerformance();
+};
+
+CPUPerformanceInterface::CPUPerformance::CPUPerformance() {
+  _counters.nProcs = os::active_processor_count();
+  _counters.cpus = NULL;
+}
+
+bool CPUPerformanceInterface::CPUPerformance::initialize() {
+  size_t array_entry_count = _counters.nProcs + 1;
+  _counters.cpus = NEW_C_HEAP_ARRAY(os::Linux::CPUPerfTicks, array_entry_count, mtInternal);
+  memset(_counters.cpus, 0, array_entry_count * sizeof(*_counters.cpus));
+
+  // For the CPU load total
+  os::Linux::get_tick_information(&_counters.cpus[_counters.nProcs], -1);
+
+  // For each CPU
+  for (int i = 0; i < _counters.nProcs; i++) {
+    os::Linux::get_tick_information(&_counters.cpus[i], i);
+  }
+  // For JVM load
+  get_jvm_ticks(&_counters.jvmTicks);
+
+  // initialize context switch system
+  // the double is only for init
+  double init_ctx_switch_rate;
+  perf_context_switch_rate(&init_ctx_switch_rate);
+
+  return true;
+}
+
+CPUPerformanceInterface::CPUPerformance::~CPUPerformance() {
+  if (_counters.cpus != NULL) {
+    FREE_C_HEAP_ARRAY(char, _counters.cpus);
+  }
+}
+
+int CPUPerformanceInterface::CPUPerformance::cpu_load(int which_logical_cpu, double* cpu_load) {
+  double u, s;
+  u = get_cpu_load(which_logical_cpu, &_counters, &s, CPU_LOAD_GLOBAL);
+  if (u < 0) {
+    *cpu_load = 0.0;
+    return OS_ERR;
+  }
+  // Cap total systemload to 1.0
+  *cpu_load = MIN2<double>((u + s), 1.0);
+  return OS_OK;
+}
+
+int CPUPerformanceInterface::CPUPerformance::cpu_load_total_process(double* cpu_load) {
+  double u, s;
+  u = get_cpu_load(-1, &_counters, &s, CPU_LOAD_VM_ONLY);
+  if (u < 0) {
+    *cpu_load = 0.0;
+    return OS_ERR;
+  }
+  *cpu_load = u + s;
+  return OS_OK;
+}
+
+int CPUPerformanceInterface::CPUPerformance::cpu_loads_process(double* pjvmUserLoad, double* pjvmKernelLoad, double* psystemTotalLoad) {
+  double u, s, t;
+
+  assert(pjvmUserLoad != NULL, "pjvmUserLoad not inited");
+  assert(pjvmKernelLoad != NULL, "pjvmKernelLoad not inited");
+  assert(psystemTotalLoad != NULL, "psystemTotalLoad not inited");
+
+  u = get_cpu_load(-1, &_counters, &s, CPU_LOAD_VM_ONLY);
+  if (u < 0) {
+    *pjvmUserLoad = 0.0;
+    *pjvmKernelLoad = 0.0;
+    *psystemTotalLoad = 0.0;
+    return OS_ERR;
+  }
+
+  cpu_load(-1, &t);
+  // clamp at user+system and 1.0
+  if (u + s > t) {
+    t = MIN2<double>(u + s, 1.0);
+  }
+
+  *pjvmUserLoad = u;
+  *pjvmKernelLoad = s;
+  *psystemTotalLoad = t;
+
+  return OS_OK;
+}
+
+int CPUPerformanceInterface::CPUPerformance::context_switch_rate(double* rate) {
+  return perf_context_switch_rate(rate);
+}
+
+CPUPerformanceInterface::CPUPerformanceInterface() {
+  _impl = NULL;
+}
+
+bool CPUPerformanceInterface::initialize() {
+  _impl = new CPUPerformanceInterface::CPUPerformance();
+  return _impl->initialize();
+}
+
+CPUPerformanceInterface::~CPUPerformanceInterface() {
+  if (_impl != NULL) {
+    delete _impl;
+  }
+}
+
+int CPUPerformanceInterface::cpu_load(int which_logical_cpu, double* cpu_load) const {
+  return _impl->cpu_load(which_logical_cpu, cpu_load);
+}
+
+int CPUPerformanceInterface::cpu_load_total_process(double* cpu_load) const {
+  return _impl->cpu_load_total_process(cpu_load);
+}
+
+int CPUPerformanceInterface::cpu_loads_process(double* pjvmUserLoad, double* pjvmKernelLoad, double* psystemTotalLoad) const {
+  return _impl->cpu_loads_process(pjvmUserLoad, pjvmKernelLoad, psystemTotalLoad);
+}
+
+int CPUPerformanceInterface::context_switch_rate(double* rate) const {
+  return _impl->context_switch_rate(rate);
+}
+
+class SystemProcessInterface::SystemProcesses : public CHeapObj<mtInternal> {
+  friend class SystemProcessInterface;
+ private:
+  class ProcessIterator : public CHeapObj<mtInternal> {
+    friend class SystemProcessInterface::SystemProcesses;
+   private:
+    DIR*           _dir;
+    struct dirent* _entry;
+    bool           _valid;
+    char           _exeName[PATH_MAX];
+    char           _exePath[PATH_MAX];
+
+    ProcessIterator();
+    ~ProcessIterator();
+    bool initialize();
+
+    bool is_valid() const { return _valid; }
+    bool is_valid_entry(struct dirent* entry) const;
+    bool is_dir(const char* name) const;
+    int  fsize(const char* name, uint64_t& size) const;
+
+    char* allocate_string(const char* str) const;
+    void  get_exe_name();
+    char* get_exe_path();
+    char* get_cmdline();
+
+    int current(SystemProcess* process_info);
+    int next_process();
+  };
+
+  ProcessIterator* _iterator;
+  SystemProcesses();
+  bool initialize();
+  ~SystemProcesses();
+
+  //information about system processes
+  int system_processes(SystemProcess** system_processes, int* no_of_sys_processes) const;
+};
+
+bool SystemProcessInterface::SystemProcesses::ProcessIterator::is_dir(const char* name) const {
+  struct stat mystat;
+  int ret_val = 0;
+
+  ret_val = stat(name, &mystat);
+  if (ret_val < 0) {
+    return false;
+  }
+  ret_val = S_ISDIR(mystat.st_mode);
+  return ret_val > 0;
+}
+
+int SystemProcessInterface::SystemProcesses::ProcessIterator::fsize(const char* name, uint64_t& size) const {
+  assert(name != NULL, "name pointer is NULL!");
+  size = 0;
+  struct stat fbuf;
+
+  if (stat(name, &fbuf) < 0) {
+    return OS_ERR;
+  }
+  size = fbuf.st_size;
+  return OS_OK;
+}
+
+// if it has a numeric name, is a directory and has a 'stat' file in it
+bool SystemProcessInterface::SystemProcesses::ProcessIterator::is_valid_entry(struct dirent* entry) const {
+  char buffer[PATH_MAX];
+  uint64_t size = 0;
+
+  if (atoi(entry->d_name) != 0) {
+    jio_snprintf(buffer, PATH_MAX, "/proc/%s", entry->d_name);
+    buffer[PATH_MAX - 1] = '\0';
+
+    if (is_dir(buffer)) {
+      jio_snprintf(buffer, PATH_MAX, "/proc/%s/stat", entry->d_name);
+      buffer[PATH_MAX - 1] = '\0';
+      if (fsize(buffer, size) != OS_ERR) {
+        return true;
+      }
+    }
+  }
+  return false;
+}
+
+// get exe-name from /proc/<pid>/stat
+void SystemProcessInterface::SystemProcesses::ProcessIterator::get_exe_name() {
+  FILE* fp;
+  char  buffer[PATH_MAX];
+
+  jio_snprintf(buffer, PATH_MAX, "/proc/%s/stat", _entry->d_name);
+  buffer[PATH_MAX - 1] = '\0';
+  if ((fp = fopen(buffer, "r")) != NULL) {
+    if (fgets(buffer, PATH_MAX, fp) != NULL) {
+      char* start, *end;
+      // exe-name is between the first pair of ( and )
+      start = strchr(buffer, '(');
+      if (start != NULL && start[1] != '\0') {
+        start++;
+        end = strrchr(start, ')');
+        if (end != NULL) {
+          size_t len;
+          len = MIN2<size_t>(end - start, sizeof(_exeName) - 1);
+          memcpy(_exeName, start, len);
+          _exeName[len] = '\0';
+        }
+      }
+    }
+    fclose(fp);
+  }
+}
+
+// get command line from /proc/<pid>/cmdline
+char* SystemProcessInterface::SystemProcesses::ProcessIterator::get_cmdline() {
+  FILE* fp;
+  char  buffer[PATH_MAX];
+  char* cmdline = NULL;
+
+  jio_snprintf(buffer, PATH_MAX, "/proc/%s/cmdline", _entry->d_name);
+  buffer[PATH_MAX - 1] = '\0';
+  if ((fp = fopen(buffer, "r")) != NULL) {
+    size_t size = 0;
+    char   dummy;
+
+    // find out how long the file is (stat always returns 0)
+    while (fread(&dummy, 1, 1, fp) == 1) {
+      size++;
+    }
+    if (size > 0) {
+      cmdline = NEW_C_HEAP_ARRAY(char, size + 1, mtInternal);
+      cmdline[0] = '\0';
+      if (fseek(fp, 0, SEEK_SET) == 0) {
+        if (fread(cmdline, 1, size, fp) == size) {
+          // the file has the arguments separated by '\0',
+          // so we translate '\0' to ' '
+          for (size_t i = 0; i < size; i++) {
+            if (cmdline[i] == '\0') {
+              cmdline[i] = ' ';
+            }
+          }
+          cmdline[size] = '\0';
+        }
+      }
+    }
+    fclose(fp);
+  }
+  return cmdline;
+}
+
+// get full path to exe from /proc/<pid>/exe symlink
+char* SystemProcessInterface::SystemProcesses::ProcessIterator::get_exe_path() {
+  char buffer[PATH_MAX];
+
+  jio_snprintf(buffer, PATH_MAX, "/proc/%s/exe", _entry->d_name);
+  buffer[PATH_MAX - 1] = '\0';
+  return realpath(buffer, _exePath);
+}
+
+char* SystemProcessInterface::SystemProcesses::ProcessIterator::allocate_string(const char* str) const {
+  if (str != NULL) {
+    return os::strdup_check_oom(str, mtInternal);
+  }
+  return NULL;
+}
+
+int SystemProcessInterface::SystemProcesses::ProcessIterator::current(SystemProcess* process_info) {
+  if (!is_valid()) {
+    return OS_ERR;
+  }
+
+  process_info->set_pid(atoi(_entry->d_name));
+
+  get_exe_name();
+  process_info->set_name(allocate_string(_exeName));
+
+  if (get_exe_path() != NULL) {
+     process_info->set_path(allocate_string(_exePath));
+  }
+
+  char* cmdline = NULL;
+  cmdline = get_cmdline();
+  if (cmdline != NULL) {
+    process_info->set_command_line(allocate_string(cmdline));
+    FREE_C_HEAP_ARRAY(char, cmdline);
+  }
+
+  return OS_OK;
+}
+
+int SystemProcessInterface::SystemProcesses::ProcessIterator::next_process() {
+  if (!is_valid()) {
+    return OS_ERR;
+  }
+
+  do {
+    _entry = os::readdir(_dir);
+    if (_entry == NULL) {
+      // Error or reached end.  Could use errno to distinguish those cases.
+      _valid = false;
+      return OS_ERR;
+    }
+  } while(!is_valid_entry(_entry));
+
+  _valid = true;
+  return OS_OK;
+}
+
+SystemProcessInterface::SystemProcesses::ProcessIterator::ProcessIterator() {
+  _dir = NULL;
+  _entry = NULL;
+  _valid = false;
+}
+
+bool SystemProcessInterface::SystemProcesses::ProcessIterator::initialize() {
+  _dir = os::opendir("/proc");
+  _entry = NULL;
+  _valid = true;
+  next_process();
+
+  return true;
+}
+
+SystemProcessInterface::SystemProcesses::ProcessIterator::~ProcessIterator() {
+  if (_dir != NULL) {
+    os::closedir(_dir);
+  }
+}
+
+SystemProcessInterface::SystemProcesses::SystemProcesses() {
+  _iterator = NULL;
+}
+
+bool SystemProcessInterface::SystemProcesses::initialize() {
+  _iterator = new SystemProcessInterface::SystemProcesses::ProcessIterator();
+  return _iterator->initialize();
+}
+
+SystemProcessInterface::SystemProcesses::~SystemProcesses() {
+  if (_iterator != NULL) {
+    delete _iterator;
+  }
+}
+
+int SystemProcessInterface::SystemProcesses::system_processes(SystemProcess** system_processes, int* no_of_sys_processes) const {
+  assert(system_processes != NULL, "system_processes pointer is NULL!");
+  assert(no_of_sys_processes != NULL, "system_processes counter pointers is NULL!");
+  assert(_iterator != NULL, "iterator is NULL!");
+
+  // initialize pointers
+  *no_of_sys_processes = 0;
+  *system_processes = NULL;
+
+  while (_iterator->is_valid()) {
+    SystemProcess* tmp = new SystemProcess();
+    _iterator->current(tmp);
+
+    //if already existing head
+    if (*system_processes != NULL) {
+      //move "first to second"
+      tmp->set_next(*system_processes);
+    }
+    // new head
+    *system_processes = tmp;
+    // increment
+    (*no_of_sys_processes)++;
+    // step forward
+    _iterator->next_process();
+  }
+  return OS_OK;
+}
+
+int SystemProcessInterface::system_processes(SystemProcess** system_procs, int* no_of_sys_processes) const {
+  return _impl->system_processes(system_procs, no_of_sys_processes);
+}
+
+SystemProcessInterface::SystemProcessInterface() {
+  _impl = NULL;
+}
+
+bool SystemProcessInterface::initialize() {
+  _impl = new SystemProcessInterface::SystemProcesses();
+  return _impl->initialize();
+}
+
+SystemProcessInterface::~SystemProcessInterface() {
+  if (_impl != NULL) {
+    delete _impl;
+  }
+}
+
+CPUInformationInterface::CPUInformationInterface() {
+  _cpu_info = NULL;
+}
+
+bool CPUInformationInterface::initialize() {
+  _cpu_info = new CPUInformation();
+  _cpu_info->set_number_of_hardware_threads(VM_Version_Ext::number_of_threads());
+  _cpu_info->set_number_of_cores(VM_Version_Ext::number_of_cores());
+  _cpu_info->set_number_of_sockets(VM_Version_Ext::number_of_sockets());
+  _cpu_info->set_cpu_name(VM_Version_Ext::cpu_name());
+  _cpu_info->set_cpu_description(VM_Version_Ext::cpu_description());
+  return true;
+}
+
+CPUInformationInterface::~CPUInformationInterface() {
+  if (_cpu_info != NULL) {
+    if (_cpu_info->cpu_name() != NULL) {
+      const char* cpu_name = _cpu_info->cpu_name();
+      FREE_C_HEAP_ARRAY(char, cpu_name);
+      _cpu_info->set_cpu_name(NULL);
+    }
+    if (_cpu_info->cpu_description() != NULL) {
+       const char* cpu_desc = _cpu_info->cpu_description();
+       FREE_C_HEAP_ARRAY(char, cpu_desc);
+      _cpu_info->set_cpu_description(NULL);
+    }
+    delete _cpu_info;
+  }
+}
+
+int CPUInformationInterface::cpu_information(CPUInformation& cpu_info) {
+  if (_cpu_info == NULL) {
+    return OS_ERR;
+  }
+
+  cpu_info = *_cpu_info; // shallow copy assignment
+  return OS_OK;
+}
+
+class NetworkPerformanceInterface::NetworkPerformance : public CHeapObj<mtInternal> {
+  friend class NetworkPerformanceInterface;
+ private:
+  NetworkPerformance();
+  NONCOPYABLE(NetworkPerformance);
+  bool initialize();
+  ~NetworkPerformance();
+  int64_t read_counter(const char* iface, const char* counter) const;
+  int network_utilization(NetworkInterface** network_interfaces) const;
+};
+
+NetworkPerformanceInterface::NetworkPerformance::NetworkPerformance() {
+
+}
+
+bool NetworkPerformanceInterface::NetworkPerformance::initialize() {
+  return true;
+}
+
+NetworkPerformanceInterface::NetworkPerformance::~NetworkPerformance() {
+}
+
+int64_t NetworkPerformanceInterface::NetworkPerformance::read_counter(const char* iface, const char* counter) const {
+  char buf[128];
+
+  snprintf(buf, sizeof(buf), "/sys/class/net/%s/statistics/%s", iface, counter);
+
+  int fd = os::open(buf, O_RDONLY, 0);
+  if (fd == -1) {
+    return -1;
+  }
+
+  ssize_t num_bytes = read(fd, buf, sizeof(buf));
+  close(fd);
+  if ((num_bytes == -1) || (num_bytes >= static_cast<ssize_t>(sizeof(buf))) || (num_bytes < 1)) {
+    return -1;
+  }
+
+  buf[num_bytes] = '\0';
+  int64_t value = strtoll(buf, NULL, 10);
+
+  return value;
+}
+
+int NetworkPerformanceInterface::NetworkPerformance::network_utilization(NetworkInterface** network_interfaces) const
+{
+  ifaddrs* addresses;
+  ifaddrs* cur_address;
+
+  if (getifaddrs(&addresses) != 0) {
+    return OS_ERR;
+  }
+
+  NetworkInterface* ret = NULL;
+  for (cur_address = addresses; cur_address != NULL; cur_address = cur_address->ifa_next) {
+    if ((cur_address->ifa_addr == NULL) || (cur_address->ifa_addr->sa_family != AF_PACKET)) {
+      continue;
+    }
+
+    int64_t bytes_in = read_counter(cur_address->ifa_name, "rx_bytes");
+    int64_t bytes_out = read_counter(cur_address->ifa_name, "tx_bytes");
+
+    NetworkInterface* cur = new NetworkInterface(cur_address->ifa_name, bytes_in, bytes_out, ret);
+    ret = cur;
+  }
+
+  freeifaddrs(addresses);
+  *network_interfaces = ret;
+
+  return OS_OK;
+}
+
+NetworkPerformanceInterface::NetworkPerformanceInterface() {
+  _impl = NULL;
+}
+
+NetworkPerformanceInterface::~NetworkPerformanceInterface() {
+  if (_impl != NULL) {
+    delete _impl;
+  }
+}
+
+bool NetworkPerformanceInterface::initialize() {
+  _impl = new NetworkPerformanceInterface::NetworkPerformance();
+  return _impl->initialize();
+}
+
+int NetworkPerformanceInterface::network_utilization(NetworkInterface** network_interfaces) const {
+  return _impl->network_utilization(network_interfaces);
+}
diff --git a/src/hotspot/os/serenity/os_serenity.cpp b/src/hotspot/os/serenity/os_serenity.cpp
new file mode 100644
index 00000000000..807f593c6b1
--- /dev/null
+++ b/src/hotspot/os/serenity/os_serenity.cpp
@@ -0,0 +1,5299 @@
+/*
+ * Copyright (c) 1999, 2021, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+// no precompiled headers
+#include "classfile/vmSymbols.hpp"
+#include "code/icBuffer.hpp"
+#include "code/vtableStubs.hpp"
+#include "compiler/compileBroker.hpp"
+#include "compiler/disassembler.hpp"
+#include "interpreter/interpreter.hpp"
+#include "jvm.h"
+#include "jvmtifiles/jvmti.h"
+#include "logging/log.hpp"
+#include "logging/logStream.hpp"
+#include "memory/allocation.inline.hpp"
+#include "memory/filemap.hpp"
+#include "oops/oop.inline.hpp"
+#include "osContainer_linux.hpp"
+#include "os_linux.inline.hpp"
+#include "os_posix.inline.hpp"
+#include "os_share_linux.hpp"
+#include "prims/jniFastGetField.hpp"
+#include "prims/jvm_misc.hpp"
+#include "runtime/arguments.hpp"
+#include "runtime/atomic.hpp"
+#include "runtime/globals.hpp"
+#include "runtime/globals_extension.hpp"
+#include "runtime/init.hpp"
+#include "runtime/interfaceSupport.inline.hpp"
+#include "runtime/java.hpp"
+#include "runtime/javaCalls.hpp"
+#include "runtime/mutexLocker.hpp"
+#include "runtime/objectMonitor.hpp"
+#include "runtime/osThread.hpp"
+#include "runtime/perfMemory.hpp"
+#include "runtime/sharedRuntime.hpp"
+#include "runtime/statSampler.hpp"
+#include "runtime/stubRoutines.hpp"
+#include "runtime/thread.inline.hpp"
+#include "runtime/threadCritical.hpp"
+#include "runtime/threadSMR.hpp"
+#include "runtime/timer.hpp"
+#include "runtime/vm_version.hpp"
+#include "semaphore_posix.hpp"
+#include "services/memTracker.hpp"
+#include "services/runtimeService.hpp"
+#include "signals_posix.hpp"
+#include "utilities/align.hpp"
+#include "utilities/decoder.hpp"
+#include "utilities/defaultStream.hpp"
+#include "utilities/elfFile.hpp"
+#include "utilities/events.hpp"
+#include "utilities/growableArray.hpp"
+#include "utilities/macros.hpp"
+#include "utilities/powerOfTwo.hpp"
+#include "utilities/vmError.hpp"
+
+// put OS-includes here
+#include <dlfcn.h>
+#include <endian.h>
+#include <errno.h>
+#include <fcntl.h>
+#include <inttypes.h>
+#include <link.h>
+#include <linux/elf-em.h>
+#include <poll.h>
+#include <pthread.h>
+#include <pwd.h>
+#include <signal.h>
+#include <stdint.h>
+#include <stdio.h>
+#include <string.h>
+#include <sys/ioctl.h>
+#include <sys/ipc.h>
+#include <sys/mman.h>
+#include <sys/resource.h>
+#include <sys/select.h>
+#include <sys/shm.h>
+#include <sys/socket.h>
+#include <sys/stat.h>
+#include <sys/sysinfo.h>
+#include <sys/time.h>
+#include <sys/times.h>
+#include <sys/types.h>
+#include <sys/utsname.h>
+#include <syscall.h>
+#include <unistd.h>
+#ifdef __GLIBC__
+#    include <malloc.h>
+#endif
+
+#ifndef _GNU_SOURCE
+#    define _GNU_SOURCE
+#    include <sched.h>
+#    undef _GNU_SOURCE
+#else
+#    include <sched.h>
+#endif
+
+// if RUSAGE_THREAD for getrusage() has not been defined, do it here. The code calling
+// getrusage() is prepared to handle the associated failure.
+#ifndef RUSAGE_THREAD
+#    define RUSAGE_THREAD (1) /* only the calling thread */
+#endif
+
+#define MAX_PATH (2 * K)
+
+#define MAX_SECS 100000000
+
+// for timer info max values which include all bits
+#define ALL_64_BITS CONST64(0xFFFFFFFFFFFFFFFF)
+
+#ifdef MUSL_LIBC
+// dlvsym is not a part of POSIX
+// and musl libc doesn't implement it.
+static void* dlvsym(void* handle,
+    const char* symbol,
+    const char* version)
+{
+    // load the latest version of symbol
+    return dlsym(handle, symbol);
+}
+#endif
+
+enum CoredumpFilterBit {
+    FILE_BACKED_PVT_BIT = 1 << 2,
+    FILE_BACKED_SHARED_BIT = 1 << 3,
+    LARGEPAGES_BIT = 1 << 6,
+    DAX_SHARED_BIT = 1 << 8
+};
+
+////////////////////////////////////////////////////////////////////////////////
+// global variables
+julong os::Serenity::_physical_memory = 0;
+
+address os::Serenity::_initial_thread_stack_bottom = NULL;
+uintptr_t os::Serenity::_initial_thread_stack_size = 0;
+
+int (*os::Serenity::_pthread_getcpuclockid)(pthread_t, clockid_t*) = NULL;
+int (*os::Serenity::_pthread_setname_np)(pthread_t, const char*) = NULL;
+pthread_t os::Serenity::_main_thread;
+int os::Serenity::_page_size = -1;
+bool os::Serenity::_supports_fast_thread_cpu_time = false;
+const char* os::Serenity::_libc_version = NULL;
+const char* os::Serenity::_libpthread_version = NULL;
+size_t os::Serenity::_default_large_page_size = 0;
+
+#ifdef __GLIBC__
+os::Serenity::mallinfo_func_t os::Serenity::_mallinfo = NULL;
+os::Serenity::mallinfo2_func_t os::Serenity::_mallinfo2 = NULL;
+#endif // __GLIBC__
+
+static jlong initial_time_count = 0;
+
+static int clock_tics_per_sec = 100;
+
+// If the VM might have been created on the primordial thread, we need to resolve the
+// primordial thread stack bounds and check if the current thread might be the
+// primordial thread in places. If we know that the primordial thread is never used,
+// such as when the VM was created by one of the standard java launchers, we can
+// avoid this
+static bool suppress_primordial_thread_resolution = false;
+
+// utility functions
+
+julong os::available_memory()
+{
+    return Serenity::available_memory();
+}
+
+julong os::Serenity::available_memory()
+{
+    // values in struct sysinfo are "unsigned long"
+    struct sysinfo si;
+    julong avail_mem;
+
+    if (OSContainer::is_containerized()) {
+        jlong mem_limit, mem_usage;
+        if ((mem_limit = OSContainer::memory_limit_in_bytes()) < 1) {
+            log_debug(os, container)("container memory limit %s: " JLONG_FORMAT ", using host value",
+                mem_limit == OSCONTAINER_ERROR ? "failed" : "unlimited", mem_limit);
+        }
+        if (mem_limit > 0 && (mem_usage = OSContainer::memory_usage_in_bytes()) < 1) {
+            log_debug(os, container)("container memory usage failed: " JLONG_FORMAT ", using host value", mem_usage);
+        }
+        if (mem_limit > 0 && mem_usage > 0) {
+            avail_mem = mem_limit > mem_usage ? (julong)mem_limit - (julong)mem_usage : 0;
+            log_trace(os)("available container memory: " JULONG_FORMAT, avail_mem);
+            return avail_mem;
+        }
+    }
+
+    sysinfo(&si);
+    avail_mem = (julong)si.freeram * si.mem_unit;
+    log_trace(os)("available memory: " JULONG_FORMAT, avail_mem);
+    return avail_mem;
+}
+
+julong os::physical_memory()
+{
+    jlong phys_mem = 0;
+    if (OSContainer::is_containerized()) {
+        jlong mem_limit;
+        if ((mem_limit = OSContainer::memory_limit_in_bytes()) > 0) {
+            log_trace(os)("total container memory: " JLONG_FORMAT, mem_limit);
+            return mem_limit;
+        }
+        log_debug(os, container)("container memory limit %s: " JLONG_FORMAT ", using host value",
+            mem_limit == OSCONTAINER_ERROR ? "failed" : "unlimited", mem_limit);
+    }
+
+    phys_mem = Serenity::physical_memory();
+    log_trace(os)("total system memory: " JLONG_FORMAT, phys_mem);
+    return phys_mem;
+}
+
+static uint64_t initial_total_ticks = 0;
+static uint64_t initial_steal_ticks = 0;
+static bool has_initial_tick_info = false;
+
+static void next_line(FILE* f)
+{
+    int c;
+    do {
+        c = fgetc(f);
+    } while (c != '\n' && c != EOF);
+}
+
+bool os::Serenity::get_tick_information(CPUPerfTicks* pticks, int which_logical_cpu)
+{
+    FILE* fh;
+    uint64_t userTicks, niceTicks, systemTicks, idleTicks;
+    // since at least kernel 2.6 : iowait: time waiting for I/O to complete
+    // irq: time  servicing interrupts; softirq: time servicing softirqs
+    uint64_t iowTicks = 0, irqTicks = 0, sirqTicks = 0;
+    // steal (since kernel 2.6.11): time spent in other OS when running in a virtualized environment
+    uint64_t stealTicks = 0;
+    // guest (since kernel 2.6.24): time spent running a virtual CPU for guest OS under the
+    // control of the Serenity kernel
+    uint64_t guestNiceTicks = 0;
+    int logical_cpu = -1;
+    const int required_tickinfo_count = (which_logical_cpu == -1) ? 4 : 5;
+    int n;
+
+    memset(pticks, 0, sizeof(CPUPerfTicks));
+
+    if ((fh = fopen("/proc/stat", "r")) == NULL) {
+        return false;
+    }
+
+    if (which_logical_cpu == -1) {
+        n = fscanf(fh, "cpu " UINT64_FORMAT " " UINT64_FORMAT " " UINT64_FORMAT " " UINT64_FORMAT " " UINT64_FORMAT " " UINT64_FORMAT " " UINT64_FORMAT " " UINT64_FORMAT " " UINT64_FORMAT " ",
+            &userTicks, &niceTicks, &systemTicks, &idleTicks,
+            &iowTicks, &irqTicks, &sirqTicks,
+            &stealTicks, &guestNiceTicks);
+    } else {
+        // Move to next line
+        next_line(fh);
+
+        // find the line for requested cpu faster to just iterate linefeeds?
+        for (int i = 0; i < which_logical_cpu; i++) {
+            next_line(fh);
+        }
+
+        n = fscanf(fh, "cpu%u " UINT64_FORMAT " " UINT64_FORMAT " " UINT64_FORMAT " " UINT64_FORMAT " " UINT64_FORMAT " " UINT64_FORMAT " " UINT64_FORMAT " " UINT64_FORMAT " " UINT64_FORMAT " ",
+            &logical_cpu, &userTicks, &niceTicks,
+            &systemTicks, &idleTicks, &iowTicks, &irqTicks, &sirqTicks,
+            &stealTicks, &guestNiceTicks);
+    }
+
+    fclose(fh);
+    if (n < required_tickinfo_count || logical_cpu != which_logical_cpu) {
+        return false;
+    }
+    pticks->used = userTicks + niceTicks;
+    pticks->usedKernel = systemTicks + irqTicks + sirqTicks;
+    pticks->total = userTicks + niceTicks + systemTicks + idleTicks + iowTicks + irqTicks + sirqTicks + stealTicks + guestNiceTicks;
+
+    if (n > required_tickinfo_count + 3) {
+        pticks->steal = stealTicks;
+        pticks->has_steal_ticks = true;
+    } else {
+        pticks->steal = 0;
+        pticks->has_steal_ticks = false;
+    }
+
+    return true;
+}
+
+// Return true if user is running as root.
+
+bool os::have_special_privileges()
+{
+    static bool init = false;
+    static bool privileges = false;
+    if (!init) {
+        privileges = (getuid() != geteuid()) || (getgid() != getegid());
+        init = true;
+    }
+    return privileges;
+}
+
+#ifndef SYS_gettid
+// i386: 224, ia64: 1105, amd64: 186, sparc: 143
+#    ifdef __ia64__
+#        define SYS_gettid 1105
+#    else
+#        ifdef __i386__
+#            define SYS_gettid 224
+#        else
+#            ifdef __amd64__
+#                define SYS_gettid 186
+#            else
+#                ifdef __sparc__
+#                    define SYS_gettid 143
+#                else
+#                    error define gettid for the arch
+#                endif
+#            endif
+#        endif
+#    endif
+#endif
+
+// pid_t gettid()
+//
+// Returns the kernel thread id of the currently running thread. Kernel
+// thread id is used to access /proc.
+pid_t os::Serenity::gettid()
+{
+    int rslt = syscall(SYS_gettid);
+    assert(rslt != -1, "must be."); // old linuxthreads implementation?
+    return (pid_t)rslt;
+}
+
+// Most versions of linux have a bug where the number of processors are
+// determined by looking at the /proc file system.  In a chroot environment,
+// the system call returns 1.
+static bool unsafe_chroot_detected = false;
+static const char* unstable_chroot_error = "/proc file system not found.\n"
+                                           "Java may be unstable running multithreaded in a chroot "
+                                           "environment on Serenity when /proc filesystem is not mounted.";
+
+void os::Serenity::initialize_system_info()
+{
+    set_processor_count(sysconf(_SC_NPROCESSORS_CONF));
+    if (processor_count() == 1) {
+        pid_t pid = os::Serenity::gettid();
+        char fname[32];
+        jio_snprintf(fname, sizeof(fname), "/proc/%d", pid);
+        FILE* fp = fopen(fname, "r");
+        if (fp == NULL) {
+            unsafe_chroot_detected = true;
+        } else {
+            fclose(fp);
+        }
+    }
+    _physical_memory = (julong)sysconf(_SC_PHYS_PAGES) * (julong)sysconf(_SC_PAGESIZE);
+    assert(processor_count() > 0, "linux error");
+}
+
+void os::init_system_properties_values()
+{
+    // The next steps are taken in the product version:
+    //
+    // Obtain the JAVA_HOME value from the location of libjvm.so.
+    // This library should be located at:
+    // <JAVA_HOME>/lib/{client|server}/libjvm.so.
+    //
+    // If "/jre/lib/" appears at the right place in the path, then we
+    // assume libjvm.so is installed in a JDK and we use this path.
+    //
+    // Otherwise exit with message: "Could not create the Java virtual machine."
+    //
+    // The following extra steps are taken in the debugging version:
+    //
+    // If "/jre/lib/" does NOT appear at the right place in the path
+    // instead of exit check for $JAVA_HOME environment variable.
+    //
+    // If it is defined and we are able to locate $JAVA_HOME/jre/lib/<arch>,
+    // then we append a fake suffix "hotspot/libjvm.so" to this path so
+    // it looks like libjvm.so is installed there
+    // <JAVA_HOME>/jre/lib/<arch>/hotspot/libjvm.so.
+    //
+    // Otherwise exit.
+    //
+    // Important note: if the location of libjvm.so changes this
+    // code needs to be changed accordingly.
+
+    // See ld(1):
+    //      The linker uses the following search paths to locate required
+    //      shared libraries:
+    //        1: ...
+    //        ...
+    //        7: The default directories, normally /lib and /usr/lib.
+#ifndef OVERRIDE_LIBPATH
+#    if defined(AMD64) || (defined(_LP64) && defined(SPARC)) || defined(PPC64) || defined(S390)
+#        define DEFAULT_LIBPATH "/usr/lib64:/lib64:/lib:/usr/lib"
+#    else
+#        define DEFAULT_LIBPATH "/lib:/usr/lib"
+#    endif
+#else
+#    define DEFAULT_LIBPATH OVERRIDE_LIBPATH
+#endif
+
+// Base path of extensions installed on the system.
+#define SYS_EXT_DIR "/usr/java/packages"
+#define EXTENSIONS_DIR "/lib/ext"
+
+    // Buffer that fits several sprintfs.
+    // Note that the space for the colon and the trailing null are provided
+    // by the nulls included by the sizeof operator.
+    const size_t bufsize = MAX2((size_t)MAXPATHLEN,                                                  // For dll_dir & friends.
+        (size_t)MAXPATHLEN + sizeof(EXTENSIONS_DIR) + sizeof(SYS_EXT_DIR) + sizeof(EXTENSIONS_DIR)); // extensions dir
+    char* buf = NEW_C_HEAP_ARRAY(char, bufsize, mtInternal);
+
+    // sysclasspath, java_home, dll_dir
+    {
+        char* pslash;
+        os::jvm_path(buf, bufsize);
+
+        // Found the full path to libjvm.so.
+        // Now cut the path to <java_home>/jre if we can.
+        pslash = strrchr(buf, '/');
+        if (pslash != NULL) {
+            *pslash = '\0'; // Get rid of /libjvm.so.
+        }
+        pslash = strrchr(buf, '/');
+        if (pslash != NULL) {
+            *pslash = '\0'; // Get rid of /{client|server|hotspot}.
+        }
+        Arguments::set_dll_dir(buf);
+
+        if (pslash != NULL) {
+            pslash = strrchr(buf, '/');
+            if (pslash != NULL) {
+                *pslash = '\0'; // Get rid of /lib.
+            }
+        }
+        Arguments::set_java_home(buf);
+        if (!set_boot_path('/', ':')) {
+            vm_exit_during_initialization("Failed setting boot class path.", NULL);
+        }
+    }
+
+    // Where to look for native libraries.
+    //
+    // Note: Due to a legacy implementation, most of the library path
+    // is set in the launcher. This was to accomodate linking restrictions
+    // on legacy Serenity implementations (which are no longer supported).
+    // Eventually, all the library path setting will be done here.
+    //
+    // However, to prevent the proliferation of improperly built native
+    // libraries, the new path component /usr/java/packages is added here.
+    // Eventually, all the library path setting will be done here.
+    {
+        // Get the user setting of LD_LIBRARY_PATH, and prepended it. It
+        // should always exist (until the legacy problem cited above is
+        // addressed).
+        const char* v = ::getenv("LD_LIBRARY_PATH");
+        const char* v_colon = ":";
+        if (v == NULL) {
+            v = "";
+            v_colon = "";
+        }
+        // That's +1 for the colon and +1 for the trailing '\0'.
+        char* ld_library_path = NEW_C_HEAP_ARRAY(char,
+            strlen(v) + 1 + sizeof(SYS_EXT_DIR) + sizeof("/lib/") + sizeof(DEFAULT_LIBPATH) + 1,
+            mtInternal);
+        sprintf(ld_library_path, "%s%s" SYS_EXT_DIR "/lib:" DEFAULT_LIBPATH, v, v_colon);
+        Arguments::set_library_path(ld_library_path);
+        FREE_C_HEAP_ARRAY(char, ld_library_path);
+    }
+
+    // Extensions directories.
+    sprintf(buf, "%s" EXTENSIONS_DIR ":" SYS_EXT_DIR EXTENSIONS_DIR, Arguments::get_java_home());
+    Arguments::set_ext_dirs(buf);
+
+    FREE_C_HEAP_ARRAY(char, buf);
+
+#undef DEFAULT_LIBPATH
+#undef SYS_EXT_DIR
+#undef EXTENSIONS_DIR
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// breakpoint support
+
+void os::breakpoint()
+{
+    BREAKPOINT;
+}
+
+extern "C" void breakpoint()
+{
+    // use debugger to set breakpoint here
+}
+
+//////////////////////////////////////////////////////////////////////////////
+// detecting pthread library
+
+void os::Serenity::libpthread_init()
+{
+    // Save glibc and pthread version strings.
+#if !defined(_CS_GNU_LIBC_VERSION) || !defined(_CS_GNU_LIBPTHREAD_VERSION)
+#    error "glibc too old (< 2.3.2)"
+#endif
+
+#ifdef MUSL_LIBC
+    // confstr() from musl libc returns EINVAL for
+    // _CS_GNU_LIBC_VERSION and _CS_GNU_LIBPTHREAD_VERSION
+    os::Serenity::set_libc_version("musl - unknown");
+    os::Serenity::set_libpthread_version("musl - unknown");
+#else
+    size_t n = confstr(_CS_GNU_LIBC_VERSION, NULL, 0);
+    assert(n > 0, "cannot retrieve glibc version");
+    char* str = (char*)malloc(n, mtInternal);
+    confstr(_CS_GNU_LIBC_VERSION, str, n);
+    os::Serenity::set_libc_version(str);
+
+    n = confstr(_CS_GNU_LIBPTHREAD_VERSION, NULL, 0);
+    assert(n > 0, "cannot retrieve pthread version");
+    str = (char*)malloc(n, mtInternal);
+    confstr(_CS_GNU_LIBPTHREAD_VERSION, str, n);
+    os::Serenity::set_libpthread_version(str);
+#endif
+}
+
+/////////////////////////////////////////////////////////////////////////////
+// thread stack expansion
+
+// os::Serenity::manually_expand_stack() takes care of expanding the thread
+// stack. Note that this is normally not needed: pthread stacks allocate
+// thread stack using mmap() without MAP_NORESERVE, so the stack is already
+// committed. Therefore it is not necessary to expand the stack manually.
+//
+// Manually expanding the stack was historically needed on SerenityThreads
+// thread stacks, which were allocated with mmap(MAP_GROWSDOWN). Nowadays
+// it is kept to deal with very rare corner cases:
+//
+// For one, user may run the VM on an own implementation of threads
+// whose stacks are - like the old SerenityThreads - implemented using
+// mmap(MAP_GROWSDOWN).
+//
+// Also, this coding may be needed if the VM is running on the primordial
+// thread. Normally we avoid running on the primordial thread; however,
+// user may still invoke the VM on the primordial thread.
+//
+// The following historical comment describes the details about running
+// on a thread stack allocated with mmap(MAP_GROWSDOWN):
+
+// Force Serenity kernel to expand current thread stack. If "bottom" is close
+// to the stack guard, caller should block all signals.
+//
+// MAP_GROWSDOWN:
+//   A special mmap() flag that is used to implement thread stacks. It tells
+//   kernel that the memory region should extend downwards when needed. This
+//   allows early versions of SerenityThreads to only mmap the first few pages
+//   when creating a new thread. Serenity kernel will automatically expand thread
+//   stack as needed (on page faults).
+//
+//   However, because the memory region of a MAP_GROWSDOWN stack can grow on
+//   demand, if a page fault happens outside an already mapped MAP_GROWSDOWN
+//   region, it's hard to tell if the fault is due to a legitimate stack
+//   access or because of reading/writing non-exist memory (e.g. buffer
+//   overrun). As a rule, if the fault happens below current stack pointer,
+//   Serenity kernel does not expand stack, instead a SIGSEGV is sent to the
+//   application (see Serenity kernel fault.c).
+//
+//   This Serenity feature can cause SIGSEGV when VM bangs thread stack for
+//   stack overflow detection.
+//
+//   Newer version of SerenityThreads (since glibc-2.2, or, RH-7.x) and NPTL do
+//   not use MAP_GROWSDOWN.
+//
+// To get around the problem and allow stack banging on Serenity, we need to
+// manually expand thread stack after receiving the SIGSEGV.
+//
+// There are two ways to expand thread stack to address "bottom", we used
+// both of them in JVM before 1.5:
+//   1. adjust stack pointer first so that it is below "bottom", and then
+//      touch "bottom"
+//   2. mmap() the page in question
+//
+// Now alternate signal stack is gone, it's harder to use 2. For instance,
+// if current sp is already near the lower end of page 101, and we need to
+// call mmap() to map page 100, it is possible that part of the mmap() frame
+// will be placed in page 100. When page 100 is mapped, it is zero-filled.
+// That will destroy the mmap() frame and cause VM to crash.
+//
+// The following code works by adjusting sp first, then accessing the "bottom"
+// page to force a page fault. Serenity kernel will then automatically expand the
+// stack mapping.
+//
+// _expand_stack_to() assumes its frame size is less than page size, which
+// should always be true if the function is not inlined.
+
+static void NOINLINE _expand_stack_to(address bottom)
+{
+    address sp;
+    size_t size;
+    volatile char* p;
+
+    // Adjust bottom to point to the largest address within the same page, it
+    // gives us a one-page buffer if alloca() allocates slightly more memory.
+    bottom = (address)align_down((uintptr_t)bottom, os::Serenity::page_size());
+    bottom += os::Serenity::page_size() - 1;
+
+    // sp might be slightly above current stack pointer; if that's the case, we
+    // will alloca() a little more space than necessary, which is OK. Don't use
+    // os::current_stack_pointer(), as its result can be slightly below current
+    // stack pointer, causing us to not alloca enough to reach "bottom".
+    sp = (address)&sp;
+
+    if (sp > bottom) {
+        size = sp - bottom;
+        p = (volatile char*)alloca(size);
+        assert(p != NULL && p <= (volatile char*)bottom, "alloca problem?");
+        p[0] = '\0';
+    }
+}
+
+void os::Serenity::expand_stack_to(address bottom)
+{
+    _expand_stack_to(bottom);
+}
+
+bool os::Serenity::manually_expand_stack(JavaThread* t, address addr)
+{
+    assert(t != NULL, "just checking");
+    assert(t->osthread()->expanding_stack(), "expand should be set");
+
+    if (t->is_in_usable_stack(addr)) {
+        sigset_t mask_all, old_sigset;
+        sigfillset(&mask_all);
+        pthread_sigmask(SIG_SETMASK, &mask_all, &old_sigset);
+        _expand_stack_to(addr);
+        pthread_sigmask(SIG_SETMASK, &old_sigset, NULL);
+        return true;
+    }
+    return false;
+}
+
+//////////////////////////////////////////////////////////////////////////////
+// create new thread
+
+// Thread start routine for all newly created threads
+static void* thread_native_entry(Thread* thread)
+{
+
+    thread->record_stack_base_and_size();
+
+    // Try to randomize the cache line index of hot stack frames.
+    // This helps when threads of the same stack traces evict each other's
+    // cache lines. The threads can be either from the same JVM instance, or
+    // from different JVM instances. The benefit is especially true for
+    // processors with hyperthreading technology.
+    static int counter = 0;
+    int pid = os::current_process_id();
+    alloca(((pid ^ counter++) & 7) * 128);
+
+    thread->initialize_thread_current();
+
+    OSThread* osthread = thread->osthread();
+    Monitor* sync = osthread->startThread_lock();
+
+    osthread->set_thread_id(os::current_thread_id());
+
+    log_info(os, thread)("Thread is alive (tid: " UINTX_FORMAT ", pthread id: " UINTX_FORMAT ").",
+        os::current_thread_id(), (uintx)pthread_self());
+
+    // initialize signal mask for this thread
+    PosixSignals::hotspot_sigmask(thread);
+
+    // initialize floating point control register
+    os::Serenity::init_thread_fpu_state();
+
+    // handshaking with parent thread
+    {
+        MutexLocker ml(sync, Mutex::_no_safepoint_check_flag);
+
+        // notify parent thread
+        osthread->set_state(INITIALIZED);
+        sync->notify_all();
+
+        // wait until os::start_thread()
+        while (osthread->get_state() == INITIALIZED) {
+            sync->wait_without_safepoint_check();
+        }
+    }
+
+    assert(osthread->pthread_id() != 0, "pthread_id was not set as expected");
+
+    // call one more level start routine
+    thread->call_run();
+
+    // Note: at this point the thread object may already have deleted itself.
+    // Prevent dereferencing it from here on out.
+    thread = NULL;
+
+    log_info(os, thread)("Thread finished (tid: " UINTX_FORMAT ", pthread id: " UINTX_FORMAT ").",
+        os::current_thread_id(), (uintx)pthread_self());
+
+    return 0;
+}
+
+// On Serenity, glibc places static TLS blocks (for __thread variables) on
+// the thread stack. This decreases the stack size actually available
+// to threads.
+//
+// For large static TLS sizes, this may cause threads to malfunction due
+// to insufficient stack space. This is a well-known issue in glibc:
+// http://sourceware.org/bugzilla/show_bug.cgi?id=11787.
+//
+// As a workaround, we call a private but assumed-stable glibc function,
+// __pthread_get_minstack() to obtain the minstack size and derive the
+// static TLS size from it. We then increase the user requested stack
+// size by this TLS size.
+//
+// Due to compatibility concerns, this size adjustment is opt-in and
+// controlled via AdjustStackSizeForTLS.
+typedef size_t (*GetMinStack)(const pthread_attr_t* attr);
+
+GetMinStack _get_minstack_func = NULL;
+
+static void get_minstack_init()
+{
+    _get_minstack_func = (GetMinStack)dlsym(RTLD_DEFAULT, "__pthread_get_minstack");
+    log_info(os, thread)("Lookup of __pthread_get_minstack %s",
+        _get_minstack_func == NULL ? "failed" : "succeeded");
+}
+
+// Returns the size of the static TLS area glibc puts on thread stacks.
+// The value is cached on first use, which occurs when the first thread
+// is created during VM initialization.
+static size_t get_static_tls_area_size(const pthread_attr_t* attr)
+{
+    size_t tls_size = 0;
+    if (_get_minstack_func != NULL) {
+        // Obtain the pthread minstack size by calling __pthread_get_minstack.
+        size_t minstack_size = _get_minstack_func(attr);
+
+        // Remove non-TLS area size included in minstack size returned
+        // by __pthread_get_minstack() to get the static TLS size.
+        // In glibc before 2.27, minstack size includes guard_size.
+        // In glibc 2.27 and later, guard_size is automatically added
+        // to the stack size by pthread_create and is no longer included
+        // in minstack size. In both cases, the guard_size is taken into
+        // account, so there is no need to adjust the result for that.
+        //
+        // Although __pthread_get_minstack() is a private glibc function,
+        // it is expected to have a stable behavior across future glibc
+        // versions while glibc still allocates the static TLS blocks off
+        // the stack. Following is glibc 2.28 __pthread_get_minstack():
+        //
+        // size_t
+        // __pthread_get_minstack (const pthread_attr_t *attr)
+        // {
+        //   return GLRO(dl_pagesize) + __static_tls_size + PTHREAD_STACK_MIN;
+        // }
+        //
+        //
+        // The following 'minstack_size > os::vm_page_size() + PTHREAD_STACK_MIN'
+        // if check is done for precaution.
+        if (minstack_size > (size_t)os::vm_page_size() + PTHREAD_STACK_MIN) {
+            tls_size = minstack_size - os::vm_page_size() - PTHREAD_STACK_MIN;
+        }
+    }
+
+    log_info(os, thread)("Stack size adjustment for TLS is " SIZE_FORMAT,
+        tls_size);
+    return tls_size;
+}
+
+bool os::create_thread(Thread* thread, ThreadType thr_type,
+    size_t req_stack_size)
+{
+    assert(thread->osthread() == NULL, "caller responsible");
+
+    // Allocate the OSThread object
+    OSThread* osthread = new OSThread(NULL, NULL);
+    if (osthread == NULL) {
+        return false;
+    }
+
+    // set the correct thread state
+    osthread->set_thread_type(thr_type);
+
+    // Initial state is ALLOCATED but not INITIALIZED
+    osthread->set_state(ALLOCATED);
+
+    thread->set_osthread(osthread);
+
+    // init thread attributes
+    pthread_attr_t attr;
+    pthread_attr_init(&attr);
+    pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_DETACHED);
+
+    // Calculate stack size if it's not specified by caller.
+    size_t stack_size = os::Posix::get_initial_stack_size(thr_type, req_stack_size);
+    // In glibc versions prior to 2.7 the guard size mechanism
+    // is not implemented properly. The posix standard requires adding
+    // the size of the guard pages to the stack size, instead Serenity
+    // takes the space out of 'stacksize'. Thus we adapt the requested
+    // stack_size by the size of the guard pages to mimick proper
+    // behaviour. However, be careful not to end up with a size
+    // of zero due to overflow. Don't add the guard page in that case.
+    size_t guard_size = os::Serenity::default_guard_size(thr_type);
+    // Configure glibc guard page. Must happen before calling
+    // get_static_tls_area_size(), which uses the guard_size.
+    pthread_attr_setguardsize(&attr, guard_size);
+
+    size_t stack_adjust_size = 0;
+    if (AdjustStackSizeForTLS) {
+        // Adjust the stack_size for on-stack TLS - see get_static_tls_area_size().
+        stack_adjust_size += get_static_tls_area_size(&attr);
+    } else {
+        stack_adjust_size += guard_size;
+    }
+
+    stack_adjust_size = align_up(stack_adjust_size, os::vm_page_size());
+    if (stack_size <= SIZE_MAX - stack_adjust_size) {
+        stack_size += stack_adjust_size;
+    }
+    assert(is_aligned(stack_size, os::vm_page_size()), "stack_size not aligned");
+
+    int status = pthread_attr_setstacksize(&attr, stack_size);
+    if (status != 0) {
+        // pthread_attr_setstacksize() function can fail
+        // if the stack size exceeds a system-imposed limit.
+        assert_status(status == EINVAL, status, "pthread_attr_setstacksize");
+        log_warning(os, thread)("The %sthread stack size specified is invalid: " SIZE_FORMAT "k",
+            (thr_type == compiler_thread) ? "compiler " : ((thr_type == java_thread) ? "" : "VM "),
+            stack_size / K);
+        thread->set_osthread(NULL);
+        delete osthread;
+        return false;
+    }
+
+    ThreadState state;
+
+    {
+        pthread_t tid;
+        int ret = pthread_create(&tid, &attr, (void* (*)(void*))thread_native_entry, thread);
+
+        char buf[64];
+        if (ret == 0) {
+            log_info(os, thread)("Thread started (pthread id: " UINTX_FORMAT ", attributes: %s). ",
+                (uintx)tid, os::Posix::describe_pthread_attr(buf, sizeof(buf), &attr));
+        } else {
+            log_warning(os, thread)("Failed to start thread - pthread_create failed (%s) for attributes: %s.",
+                os::errno_name(ret), os::Posix::describe_pthread_attr(buf, sizeof(buf), &attr));
+            // Log some OS information which might explain why creating the thread failed.
+            log_info(os, thread)("Number of threads approx. running in the VM: %d", Threads::number_of_threads());
+            LogStream st(Log(os, thread)::info());
+            os::Posix::print_rlimit_info(&st);
+            os::print_memory_info(&st);
+            os::Serenity::print_proc_sys_info(&st);
+            os::Serenity::print_container_info(&st);
+        }
+
+        pthread_attr_destroy(&attr);
+
+        if (ret != 0) {
+            // Need to clean up stuff we've allocated so far
+            thread->set_osthread(NULL);
+            delete osthread;
+            return false;
+        }
+
+        // Store pthread info into the OSThread
+        osthread->set_pthread_id(tid);
+
+        // Wait until child thread is either initialized or aborted
+        {
+            Monitor* sync_with_child = osthread->startThread_lock();
+            MutexLocker ml(sync_with_child, Mutex::_no_safepoint_check_flag);
+            while ((state = osthread->get_state()) == ALLOCATED) {
+                sync_with_child->wait_without_safepoint_check();
+            }
+        }
+    }
+
+    // The thread is returned suspended (in state INITIALIZED),
+    // and is started higher up in the call chain
+    assert(state == INITIALIZED, "race condition");
+    return true;
+}
+
+/////////////////////////////////////////////////////////////////////////////
+// attach existing thread
+
+// bootstrap the main thread
+bool os::create_main_thread(JavaThread* thread)
+{
+    assert(os::Serenity::_main_thread == pthread_self(), "should be called inside main thread");
+    return create_attached_thread(thread);
+}
+
+bool os::create_attached_thread(JavaThread* thread)
+{
+#ifdef ASSERT
+    thread->verify_not_published();
+#endif
+
+    // Allocate the OSThread object
+    OSThread* osthread = new OSThread(NULL, NULL);
+
+    if (osthread == NULL) {
+        return false;
+    }
+
+    // Store pthread info into the OSThread
+    osthread->set_thread_id(os::Serenity::gettid());
+    osthread->set_pthread_id(::pthread_self());
+
+    // initialize floating point control register
+    os::Serenity::init_thread_fpu_state();
+
+    // Initial thread state is RUNNABLE
+    osthread->set_state(RUNNABLE);
+
+    thread->set_osthread(osthread);
+
+    if (os::is_primordial_thread()) {
+        // If current thread is primordial thread, its stack is mapped on demand,
+        // see notes about MAP_GROWSDOWN. Here we try to force kernel to map
+        // the entire stack region to avoid SEGV in stack banging.
+        // It is also useful to get around the heap-stack-gap problem on SuSE
+        // kernel (see 4821821 for details). We first expand stack to the top
+        // of yellow zone, then enable stack yellow zone (order is significant,
+        // enabling yellow zone first will crash JVM on SuSE Serenity), so there
+        // is no gap between the last two virtual memory regions.
+
+        StackOverflow* overflow_state = thread->stack_overflow_state();
+        address addr = overflow_state->stack_reserved_zone_base();
+        assert(addr != NULL, "initialization problem?");
+        assert(overflow_state->stack_available(addr) > 0, "stack guard should not be enabled");
+
+        osthread->set_expanding_stack();
+        os::Serenity::manually_expand_stack(thread, addr);
+        osthread->clear_expanding_stack();
+    }
+
+    // initialize signal mask for this thread
+    // and save the caller's signal mask
+    PosixSignals::hotspot_sigmask(thread);
+
+    log_info(os, thread)("Thread attached (tid: " UINTX_FORMAT ", pthread id: " UINTX_FORMAT ").",
+        os::current_thread_id(), (uintx)pthread_self());
+
+    return true;
+}
+
+void os::pd_start_thread(Thread* thread)
+{
+    OSThread* osthread = thread->osthread();
+    assert(osthread->get_state() != INITIALIZED, "just checking");
+    Monitor* sync_with_child = osthread->startThread_lock();
+    MutexLocker ml(sync_with_child, Mutex::_no_safepoint_check_flag);
+    sync_with_child->notify();
+}
+
+// Free Serenity resources related to the OSThread
+void os::free_thread(OSThread* osthread)
+{
+    assert(osthread != NULL, "osthread not set");
+
+    // We are told to free resources of the argument thread,
+    // but we can only really operate on the current thread.
+    assert(Thread::current()->osthread() == osthread,
+        "os::free_thread but not current thread");
+
+#ifdef ASSERT
+    sigset_t current;
+    sigemptyset(&current);
+    pthread_sigmask(SIG_SETMASK, NULL, &current);
+    assert(!sigismember(&current, PosixSignals::SR_signum), "SR signal should not be blocked!");
+#endif
+
+    // Restore caller's signal mask
+    sigset_t sigmask = osthread->caller_sigmask();
+    pthread_sigmask(SIG_SETMASK, &sigmask, NULL);
+
+    delete osthread;
+}
+
+//////////////////////////////////////////////////////////////////////////////
+// primordial thread
+
+// Check if current thread is the primordial thread, similar to Solaris thr_main.
+bool os::is_primordial_thread(void)
+{
+    if (suppress_primordial_thread_resolution) {
+        return false;
+    }
+    char dummy;
+    // If called before init complete, thread stack bottom will be null.
+    // Can be called if fatal error occurs before initialization.
+    if (os::Serenity::initial_thread_stack_bottom() == NULL)
+        return false;
+    assert(os::Serenity::initial_thread_stack_bottom() != NULL && os::Serenity::initial_thread_stack_size() != 0,
+        "os::init did not locate primordial thread's stack region");
+    if ((address)&dummy >= os::Serenity::initial_thread_stack_bottom() && (address)&dummy < os::Serenity::initial_thread_stack_bottom() + os::Serenity::initial_thread_stack_size()) {
+        return true;
+    } else {
+        return false;
+    }
+}
+
+// Find the virtual memory area that contains addr
+static bool find_vma(address addr, address* vma_low, address* vma_high)
+{
+    FILE* fp = fopen("/proc/self/maps", "r");
+    if (fp) {
+        address low, high;
+        while (!feof(fp)) {
+            if (fscanf(fp, "%p-%p", &low, &high) == 2) {
+                if (low <= addr && addr < high) {
+                    if (vma_low)
+                        *vma_low = low;
+                    if (vma_high)
+                        *vma_high = high;
+                    fclose(fp);
+                    return true;
+                }
+            }
+            for (;;) {
+                int ch = fgetc(fp);
+                if (ch == EOF || ch == (int)'\n')
+                    break;
+            }
+        }
+        fclose(fp);
+    }
+    return false;
+}
+
+// Locate primordial thread stack. This special handling of primordial thread stack
+// is needed because pthread_getattr_np() on most (all?) Serenity distros returns
+// bogus value for the primordial process thread. While the launcher has created
+// the VM in a new thread since JDK 6, we still have to allow for the use of the
+// JNI invocation API from a primordial thread.
+void os::Serenity::capture_initial_stack(size_t max_size)
+{
+
+    // max_size is either 0 (which means accept OS default for thread stacks) or
+    // a user-specified value known to be at least the minimum needed. If we
+    // are actually on the primordial thread we can make it appear that we have a
+    // smaller max_size stack by inserting the guard pages at that location. But we
+    // cannot do anything to emulate a larger stack than what has been provided by
+    // the OS or threading library. In fact if we try to use a stack greater than
+    // what is set by rlimit then we will crash the hosting process.
+
+    // Maximum stack size is the easy part, get it from RLIMIT_STACK.
+    // If this is "unlimited" then it will be a huge value.
+    struct rlimit rlim;
+    getrlimit(RLIMIT_STACK, &rlim);
+    size_t stack_size = rlim.rlim_cur;
+
+    // 6308388: a bug in ld.so will relocate its own .data section to the
+    //   lower end of primordial stack; reduce ulimit -s value a little bit
+    //   so we won't install guard page on ld.so's data section.
+    //   But ensure we don't underflow the stack size - allow 1 page spare
+    if (stack_size >= (size_t)(3 * page_size())) {
+        stack_size -= 2 * page_size();
+    }
+
+    // Try to figure out where the stack base (top) is. This is harder.
+    //
+    // When an application is started, glibc saves the initial stack pointer in
+    // a global variable "__libc_stack_end", which is then used by system
+    // libraries. __libc_stack_end should be pretty close to stack top. The
+    // variable is available since the very early days. However, because it is
+    // a private interface, it could disappear in the future.
+    //
+    // Serenity kernel saves start_stack information in /proc/<pid>/stat. Similar
+    // to __libc_stack_end, it is very close to stack top, but isn't the real
+    // stack top. Note that /proc may not exist if VM is running as a chroot
+    // program, so reading /proc/<pid>/stat could fail. Also the contents of
+    // /proc/<pid>/stat could change in the future (though unlikely).
+    //
+    // We try __libc_stack_end first. If that doesn't work, look for
+    // /proc/<pid>/stat. If neither of them works, we use current stack pointer
+    // as a hint, which should work well in most cases.
+
+    uintptr_t stack_start;
+
+    // try __libc_stack_end first
+    uintptr_t* p = (uintptr_t*)dlsym(RTLD_DEFAULT, "__libc_stack_end");
+    if (p && *p) {
+        stack_start = *p;
+    } else {
+        // see if we can get the start_stack field from /proc/self/stat
+        FILE* fp;
+        int pid;
+        char state;
+        int ppid;
+        int pgrp;
+        int session;
+        int nr;
+        int tpgrp;
+        unsigned long flags;
+        unsigned long minflt;
+        unsigned long cminflt;
+        unsigned long majflt;
+        unsigned long cmajflt;
+        unsigned long utime;
+        unsigned long stime;
+        long cutime;
+        long cstime;
+        long prio;
+        long nice;
+        long junk;
+        long it_real;
+        uintptr_t start;
+        uintptr_t vsize;
+        intptr_t rss;
+        uintptr_t rsslim;
+        uintptr_t scodes;
+        uintptr_t ecode;
+        int i;
+
+        // Figure what the primordial thread stack base is. Code is inspired
+        // by email from Hans Boehm. /proc/self/stat begins with current pid,
+        // followed by command name surrounded by parentheses, state, etc.
+        char stat[2048];
+        int statlen;
+
+        fp = fopen("/proc/self/stat", "r");
+        if (fp) {
+            statlen = fread(stat, 1, 2047, fp);
+            stat[statlen] = '\0';
+            fclose(fp);
+
+            // Skip pid and the command string. Note that we could be dealing with
+            // weird command names, e.g. user could decide to rename java launcher
+            // to "java 1.4.2 :)", then the stat file would look like
+            //                1234 (java 1.4.2 :)) R ... ...
+            // We don't really need to know the command string, just find the last
+            // occurrence of ")" and then start parsing from there. See bug 4726580.
+            char* s = strrchr(stat, ')');
+
+            i = 0;
+            if (s) {
+                // Skip blank chars
+                do {
+                    s++;
+                } while (s && isspace(*s));
+
+#define _UFM UINTX_FORMAT
+#define _DFM INTX_FORMAT
+
+                //                                     1   1   1   1   1   1   1   1   1   1   2   2    2    2    2    2    2    2    2
+                //              3  4  5  6  7  8   9   0   1   2   3   4   5   6   7   8   9   0   1    2    3    4    5    6    7    8
+                i = sscanf(s, "%c %d %d %d %d %d %lu %lu %lu %lu %lu %lu %lu %ld %ld %ld %ld %ld %ld " _UFM _UFM _DFM _UFM _UFM _UFM _UFM,
+                    &state,        // 3  %c
+                    &ppid,         // 4  %d
+                    &pgrp,         // 5  %d
+                    &session,      // 6  %d
+                    &nr,           // 7  %d
+                    &tpgrp,        // 8  %d
+                    &flags,        // 9  %lu
+                    &minflt,       // 10 %lu
+                    &cminflt,      // 11 %lu
+                    &majflt,       // 12 %lu
+                    &cmajflt,      // 13 %lu
+                    &utime,        // 14 %lu
+                    &stime,        // 15 %lu
+                    &cutime,       // 16 %ld
+                    &cstime,       // 17 %ld
+                    &prio,         // 18 %ld
+                    &nice,         // 19 %ld
+                    &junk,         // 20 %ld
+                    &it_real,      // 21 %ld
+                    &start,        // 22 UINTX_FORMAT
+                    &vsize,        // 23 UINTX_FORMAT
+                    &rss,          // 24 INTX_FORMAT
+                    &rsslim,       // 25 UINTX_FORMAT
+                    &scodes,       // 26 UINTX_FORMAT
+                    &ecode,        // 27 UINTX_FORMAT
+                    &stack_start); // 28 UINTX_FORMAT
+            }
+
+#undef _UFM
+#undef _DFM
+
+            if (i != 28 - 2) {
+                assert(false, "Bad conversion from /proc/self/stat");
+                // product mode - assume we are the primordial thread, good luck in the
+                // embedded case.
+                warning("Can't detect primordial thread stack location - bad conversion");
+                stack_start = (uintptr_t)&rlim;
+            }
+        } else {
+            // For some reason we can't open /proc/self/stat (for example, running on
+            // FreeBSD with a Serenity emulator, or inside chroot), this should work for
+            // most cases, so don't abort:
+            warning("Can't detect primordial thread stack location - no /proc/self/stat");
+            stack_start = (uintptr_t)&rlim;
+        }
+    }
+
+    // Now we have a pointer (stack_start) very close to the stack top, the
+    // next thing to do is to figure out the exact location of stack top. We
+    // can find out the virtual memory area that contains stack_start by
+    // reading /proc/self/maps, it should be the last vma in /proc/self/maps,
+    // and its upper limit is the real stack top. (again, this would fail if
+    // running inside chroot, because /proc may not exist.)
+
+    uintptr_t stack_top;
+    address low, high;
+    if (find_vma((address)stack_start, &low, &high)) {
+        // success, "high" is the true stack top. (ignore "low", because initial
+        // thread stack grows on demand, its real bottom is high - RLIMIT_STACK.)
+        stack_top = (uintptr_t)high;
+    } else {
+        // failed, likely because /proc/self/maps does not exist
+        warning("Can't detect primordial thread stack location - find_vma failed");
+        // best effort: stack_start is normally within a few pages below the real
+        // stack top, use it as stack top, and reduce stack size so we won't put
+        // guard page outside stack.
+        stack_top = stack_start;
+        stack_size -= 16 * page_size();
+    }
+
+    // stack_top could be partially down the page so align it
+    stack_top = align_up(stack_top, page_size());
+
+    // Allowed stack value is minimum of max_size and what we derived from rlimit
+    if (max_size > 0) {
+        _initial_thread_stack_size = MIN2(max_size, stack_size);
+    } else {
+        // Accept the rlimit max, but if stack is unlimited then it will be huge, so
+        // clamp it at 8MB as we do on Solaris
+        _initial_thread_stack_size = MIN2(stack_size, 8 * M);
+    }
+    _initial_thread_stack_size = align_down(_initial_thread_stack_size, page_size());
+    _initial_thread_stack_bottom = (address)stack_top - _initial_thread_stack_size;
+
+    assert(_initial_thread_stack_bottom < (address)stack_top, "overflow!");
+
+    if (log_is_enabled(Info, os, thread)) {
+        // See if we seem to be on primordial process thread
+        bool primordial = uintptr_t(&rlim) > uintptr_t(_initial_thread_stack_bottom) && uintptr_t(&rlim) < stack_top;
+
+        log_info(os, thread)("Capturing initial stack in %s thread: req. size: " SIZE_FORMAT "K, actual size: " SIZE_FORMAT "K, top=" INTPTR_FORMAT ", bottom=" INTPTR_FORMAT,
+            primordial ? "primordial" : "user", max_size / K, _initial_thread_stack_size / K,
+            stack_top, intptr_t(_initial_thread_stack_bottom));
+    }
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// time support
+
+// Time since start-up in seconds to a fine granularity.
+double os::elapsedTime()
+{
+    return ((double)os::elapsed_counter()) / os::elapsed_frequency(); // nanosecond resolution
+}
+
+jlong os::elapsed_counter()
+{
+    return javaTimeNanos() - initial_time_count;
+}
+
+jlong os::elapsed_frequency()
+{
+    return NANOSECS_PER_SEC; // nanosecond resolution
+}
+
+bool os::supports_vtime() { return true; }
+
+double os::elapsedVTime()
+{
+    struct rusage usage;
+    int retval = getrusage(RUSAGE_THREAD, &usage);
+    if (retval == 0) {
+        return (double)(usage.ru_utime.tv_sec + usage.ru_stime.tv_sec) + (double)(usage.ru_utime.tv_usec + usage.ru_stime.tv_usec) / (1000 * 1000);
+    } else {
+        // better than nothing, but not much
+        return elapsedTime();
+    }
+}
+
+void os::Serenity::fast_thread_clock_init()
+{
+    if (!UseSerenityPosixThreadCPUClocks) {
+        return;
+    }
+    clockid_t clockid;
+    struct timespec tp;
+    int (*pthread_getcpuclockid_func)(pthread_t, clockid_t*) = (int (*)(pthread_t, clockid_t*))dlsym(RTLD_DEFAULT, "pthread_getcpuclockid");
+
+    // Switch to using fast clocks for thread cpu time if
+    // the clock_getres() returns 0 error code.
+    // Note, that some kernels may support the current thread
+    // clock (CLOCK_THREAD_CPUTIME_ID) but not the clocks
+    // returned by the pthread_getcpuclockid().
+    // If the fast Posix clocks are supported then the clock_getres()
+    // must return at least tp.tv_sec == 0 which means a resolution
+    // better than 1 sec. This is extra check for reliability.
+
+    if (pthread_getcpuclockid_func && pthread_getcpuclockid_func(_main_thread, &clockid) == 0 && clock_getres(clockid, &tp) == 0 && tp.tv_sec == 0) {
+        _supports_fast_thread_cpu_time = true;
+        _pthread_getcpuclockid = pthread_getcpuclockid_func;
+    }
+}
+
+// Return the real, user, and system times in seconds from an
+// arbitrary fixed point in the past.
+bool os::getTimesSecs(double* process_real_time,
+    double* process_user_time,
+    double* process_system_time)
+{
+    struct tms ticks;
+    clock_t real_ticks = times(&ticks);
+
+    if (real_ticks == (clock_t)(-1)) {
+        return false;
+    } else {
+        double ticks_per_second = (double)clock_tics_per_sec;
+        *process_user_time = ((double)ticks.tms_utime) / ticks_per_second;
+        *process_system_time = ((double)ticks.tms_stime) / ticks_per_second;
+        *process_real_time = ((double)real_ticks) / ticks_per_second;
+
+        return true;
+    }
+}
+
+char* os::local_time_string(char* buf, size_t buflen)
+{
+    struct tm t;
+    time_t long_time;
+    time(&long_time);
+    localtime_r(&long_time, &t);
+    jio_snprintf(buf, buflen, "%d-%02d-%02d %02d:%02d:%02d",
+        t.tm_year + 1900, t.tm_mon + 1, t.tm_mday,
+        t.tm_hour, t.tm_min, t.tm_sec);
+    return buf;
+}
+
+struct tm* os::localtime_pd(const time_t* clock, struct tm* res)
+{
+    return localtime_r(clock, res);
+}
+
+// thread_id is kernel thread id (similar to Solaris LWP id)
+intx os::current_thread_id() { return os::Serenity::gettid(); }
+int os::current_process_id()
+{
+    return ::getpid();
+}
+
+// DLL functions
+
+const char* os::dll_file_extension() { return ".so"; }
+
+// This must be hard coded because it's the system's temporary
+// directory not the java application's temp directory, ala java.io.tmpdir.
+const char* os::get_temp_directory() { return "/tmp"; }
+
+static bool file_exists(const char* filename)
+{
+    struct stat statbuf;
+    if (filename == NULL || strlen(filename) == 0) {
+        return false;
+    }
+    return os::stat(filename, &statbuf) == 0;
+}
+
+// check if addr is inside libjvm.so
+bool os::address_is_in_vm(address addr)
+{
+    static address libjvm_base_addr;
+    Dl_info dlinfo;
+
+    if (libjvm_base_addr == NULL) {
+        if (dladdr(CAST_FROM_FN_PTR(void*, os::address_is_in_vm), &dlinfo) != 0) {
+            libjvm_base_addr = (address)dlinfo.dli_fbase;
+        }
+        assert(libjvm_base_addr != NULL, "Cannot obtain base address for libjvm");
+    }
+
+    if (dladdr((void*)addr, &dlinfo) != 0) {
+        if (libjvm_base_addr == (address)dlinfo.dli_fbase)
+            return true;
+    }
+
+    return false;
+}
+
+bool os::dll_address_to_function_name(address addr, char* buf,
+    int buflen, int* offset,
+    bool demangle)
+{
+    // buf is not optional, but offset is optional
+    assert(buf != NULL, "sanity check");
+
+    Dl_info dlinfo;
+
+    if (dladdr((void*)addr, &dlinfo) != 0) {
+        // see if we have a matching symbol
+        if (dlinfo.dli_saddr != NULL && dlinfo.dli_sname != NULL) {
+            if (!(demangle && Decoder::demangle(dlinfo.dli_sname, buf, buflen))) {
+                jio_snprintf(buf, buflen, "%s", dlinfo.dli_sname);
+            }
+            if (offset != NULL)
+                *offset = addr - (address)dlinfo.dli_saddr;
+            return true;
+        }
+        // no matching symbol so try for just file info
+        if (dlinfo.dli_fname != NULL && dlinfo.dli_fbase != NULL) {
+            if (Decoder::decode((address)(addr - (address)dlinfo.dli_fbase),
+                    buf, buflen, offset, dlinfo.dli_fname, demangle)) {
+                return true;
+            }
+        }
+    }
+
+    buf[0] = '\0';
+    if (offset != NULL)
+        *offset = -1;
+    return false;
+}
+
+struct _address_to_library_name {
+    address addr;  // input : memory address
+    size_t buflen; //         size of fname
+    char* fname;   // output: library name
+    address base;  //         library base addr
+};
+
+static int address_to_library_name_callback(struct dl_phdr_info* info,
+    size_t size, void* data)
+{
+    int i;
+    bool found = false;
+    address libbase = NULL;
+    struct _address_to_library_name* d = (struct _address_to_library_name*)data;
+
+    // iterate through all loadable segments
+    for (i = 0; i < info->dlpi_phnum; i++) {
+        address segbase = (address)(info->dlpi_addr + info->dlpi_phdr[i].p_vaddr);
+        if (info->dlpi_phdr[i].p_type == PT_LOAD) {
+            // base address of a library is the lowest address of its loaded
+            // segments.
+            if (libbase == NULL || libbase > segbase) {
+                libbase = segbase;
+            }
+            // see if 'addr' is within current segment
+            if (segbase <= d->addr && d->addr < segbase + info->dlpi_phdr[i].p_memsz) {
+                found = true;
+            }
+        }
+    }
+
+    // dlpi_name is NULL or empty if the ELF file is executable, return 0
+    // so dll_address_to_library_name() can fall through to use dladdr() which
+    // can figure out executable name from argv[0].
+    if (found && info->dlpi_name && info->dlpi_name[0]) {
+        d->base = libbase;
+        if (d->fname) {
+            jio_snprintf(d->fname, d->buflen, "%s", info->dlpi_name);
+        }
+        return 1;
+    }
+    return 0;
+}
+
+bool os::dll_address_to_library_name(address addr, char* buf,
+    int buflen, int* offset)
+{
+    // buf is not optional, but offset is optional
+    assert(buf != NULL, "sanity check");
+
+    Dl_info dlinfo;
+    struct _address_to_library_name data;
+
+    // There is a bug in old glibc dladdr() implementation that it could resolve
+    // to wrong library name if the .so file has a base address != NULL. Here
+    // we iterate through the program headers of all loaded libraries to find
+    // out which library 'addr' really belongs to. This workaround can be
+    // removed once the minimum requirement for glibc is moved to 2.3.x.
+    data.addr = addr;
+    data.fname = buf;
+    data.buflen = buflen;
+    data.base = NULL;
+    int rslt = dl_iterate_phdr(address_to_library_name_callback, (void*)&data);
+
+    if (rslt) {
+        // buf already contains library name
+        if (offset)
+            *offset = addr - data.base;
+        return true;
+    }
+    if (dladdr((void*)addr, &dlinfo) != 0) {
+        if (dlinfo.dli_fname != NULL) {
+            jio_snprintf(buf, buflen, "%s", dlinfo.dli_fname);
+        }
+        if (dlinfo.dli_fbase != NULL && offset != NULL) {
+            *offset = addr - (address)dlinfo.dli_fbase;
+        }
+        return true;
+    }
+
+    buf[0] = '\0';
+    if (offset)
+        *offset = -1;
+    return false;
+}
+
+// Loads .dll/.so and
+// in case of error it checks if .dll/.so was built for the
+// same architecture as Hotspot is running on
+
+// Remember the stack's state. The Serenity dynamic linker will change
+// the stack to 'executable' at most once, so we must safepoint only once.
+bool os::Serenity::_stack_is_executable = false;
+
+// VM operation that loads a library.  This is necessary if stack protection
+// of the Java stacks can be lost during loading the library.  If we
+// do not stop the Java threads, they can stack overflow before the stacks
+// are protected again.
+class VM_SerenityDllLoad : public VM_Operation {
+private:
+    const char* _filename;
+    char* _ebuf;
+    int _ebuflen;
+    void* _lib;
+
+public:
+    VM_SerenityDllLoad(const char* fn, char* ebuf, int ebuflen)
+        : _filename(fn)
+        , _ebuf(ebuf)
+        , _ebuflen(ebuflen)
+        , _lib(NULL)
+    {
+    }
+    VMOp_Type type() const { return VMOp_SerenityDllLoad; }
+    void doit()
+    {
+        _lib = os::Serenity::dll_load_in_vmthread(_filename, _ebuf, _ebuflen);
+        os::Serenity::_stack_is_executable = true;
+    }
+    void* loaded_library() { return _lib; }
+};
+
+void* os::dll_load(const char* filename, char* ebuf, int ebuflen)
+{
+    void* result = NULL;
+    bool load_attempted = false;
+
+    log_info(os)("attempting shared library load of %s", filename);
+
+    // Check whether the library to load might change execution rights
+    // of the stack. If they are changed, the protection of the stack
+    // guard pages will be lost. We need a safepoint to fix this.
+    //
+    // See Serenity man page execstack(8) for more info.
+    if (os::uses_stack_guard_pages() && !os::Serenity::_stack_is_executable) {
+        if (!ElfFile::specifies_noexecstack(filename)) {
+            if (!is_init_completed()) {
+                os::Serenity::_stack_is_executable = true;
+                // This is OK - No Java threads have been created yet, and hence no
+                // stack guard pages to fix.
+                //
+                // Dynamic loader will make all stacks executable after
+                // this function returns, and will not do that again.
+                assert(Threads::number_of_threads() == 0, "no Java threads should exist yet.");
+            } else {
+                warning("You have loaded library %s which might have disabled stack guard. "
+                        "The VM will try to fix the stack guard now.\n"
+                        "It's highly recommended that you fix the library with "
+                        "'execstack -c <libfile>', or link it with '-z noexecstack'.",
+                    filename);
+
+                JavaThread* jt = JavaThread::current();
+                if (jt->thread_state() != _thread_in_native) {
+                    // This happens when a compiler thread tries to load a hsdis-<arch>.so file
+                    // that requires ExecStack. Cannot enter safe point. Let's give up.
+                    warning("Unable to fix stack guard. Giving up.");
+                } else {
+                    if (!LoadExecStackDllInVMThread) {
+                        // This is for the case where the DLL has an static
+                        // constructor function that executes JNI code. We cannot
+                        // load such DLLs in the VMThread.
+                        result = os::Serenity::dlopen_helper(filename, ebuf, ebuflen);
+                    }
+
+                    ThreadInVMfromNative tiv(jt);
+                    debug_only(VMNativeEntryWrapper vew;)
+
+                        VM_SerenityDllLoad op(filename, ebuf, ebuflen);
+                    VMThread::execute(&op);
+                    if (LoadExecStackDllInVMThread) {
+                        result = op.loaded_library();
+                    }
+                    load_attempted = true;
+                }
+            }
+        }
+    }
+
+    if (!load_attempted) {
+        result = os::Serenity::dlopen_helper(filename, ebuf, ebuflen);
+    }
+
+    if (result != NULL) {
+        // Successful loading
+        return result;
+    }
+
+    Elf32_Ehdr elf_head;
+    int diag_msg_max_length = ebuflen - strlen(ebuf);
+    char* diag_msg_buf = ebuf + strlen(ebuf);
+
+    if (diag_msg_max_length == 0) {
+        // No more space in ebuf for additional diagnostics message
+        return NULL;
+    }
+
+    int file_descriptor = ::open(filename, O_RDONLY | O_NONBLOCK);
+
+    if (file_descriptor < 0) {
+        // Can't open library, report dlerror() message
+        return NULL;
+    }
+
+    bool failed_to_read_elf_head = (sizeof(elf_head) != (::read(file_descriptor, &elf_head, sizeof(elf_head))));
+
+    ::close(file_descriptor);
+    if (failed_to_read_elf_head) {
+        // file i/o error - report dlerror() msg
+        return NULL;
+    }
+
+    if (elf_head.e_ident[EI_DATA] != LITTLE_ENDIAN_ONLY(ELFDATA2LSB) BIG_ENDIAN_ONLY(ELFDATA2MSB)) {
+        // handle invalid/out of range endianness values
+        if (elf_head.e_ident[EI_DATA] == 0 || elf_head.e_ident[EI_DATA] > 2) {
+            return NULL;
+        }
+
+#if defined(VM_LITTLE_ENDIAN)
+        // VM is LE, shared object BE
+        elf_head.e_machine = be16toh(elf_head.e_machine);
+#else
+        // VM is BE, shared object LE
+        elf_head.e_machine = le16toh(elf_head.e_machine);
+#endif
+    }
+
+    typedef struct {
+        Elf32_Half code;          // Actual value as defined in elf.h
+        Elf32_Half compat_class;  // Compatibility of archs at VM's sense
+        unsigned char elf_class;  // 32 or 64 bit
+        unsigned char endianness; // MSB or LSB
+        char* name;               // String representation
+    } arch_t;
+
+#ifndef EM_AARCH64
+#    define EM_AARCH64 183 /* ARM AARCH64 */
+#endif
+#ifndef EM_RISCV
+#    define EM_RISCV 243 /* RISC-V */
+#endif
+
+    static const arch_t arch_array[] = {
+        { EM_386, EM_386, ELFCLASS32, ELFDATA2LSB, (char*)"IA 32" },
+        { EM_486, EM_386, ELFCLASS32, ELFDATA2LSB, (char*)"IA 32" },
+        { EM_IA_64, EM_IA_64, ELFCLASS64, ELFDATA2LSB, (char*)"IA 64" },
+        { EM_X86_64, EM_X86_64, ELFCLASS64, ELFDATA2LSB, (char*)"AMD 64" },
+        { EM_SPARC, EM_SPARC, ELFCLASS32, ELFDATA2MSB, (char*)"Sparc 32" },
+        { EM_SPARC32PLUS, EM_SPARC, ELFCLASS32, ELFDATA2MSB, (char*)"Sparc 32" },
+        { EM_SPARCV9, EM_SPARCV9, ELFCLASS64, ELFDATA2MSB, (char*)"Sparc v9 64" },
+        { EM_PPC, EM_PPC, ELFCLASS32, ELFDATA2MSB, (char*)"Power PC 32" },
+#if defined(VM_LITTLE_ENDIAN)
+        { EM_PPC64, EM_PPC64, ELFCLASS64, ELFDATA2LSB, (char*)"Power PC 64 LE" },
+        { EM_SH, EM_SH, ELFCLASS32, ELFDATA2LSB, (char*)"SuperH" },
+#else
+        { EM_PPC64, EM_PPC64, ELFCLASS64, ELFDATA2MSB, (char*)"Power PC 64" },
+        { EM_SH, EM_SH, ELFCLASS32, ELFDATA2MSB, (char*)"SuperH BE" },
+#endif
+        { EM_ARM, EM_ARM, ELFCLASS32, ELFDATA2LSB, (char*)"ARM" },
+        // we only support 64 bit z architecture
+        { EM_S390, EM_S390, ELFCLASS64, ELFDATA2MSB, (char*)"IBM System/390" },
+        { EM_ALPHA, EM_ALPHA, ELFCLASS64, ELFDATA2LSB, (char*)"Alpha" },
+        { EM_MIPS_RS3_LE, EM_MIPS_RS3_LE, ELFCLASS32, ELFDATA2LSB, (char*)"MIPSel" },
+        { EM_MIPS, EM_MIPS, ELFCLASS32, ELFDATA2MSB, (char*)"MIPS" },
+        { EM_PARISC, EM_PARISC, ELFCLASS32, ELFDATA2MSB, (char*)"PARISC" },
+        { EM_68K, EM_68K, ELFCLASS32, ELFDATA2MSB, (char*)"M68k" },
+        { EM_AARCH64, EM_AARCH64, ELFCLASS64, ELFDATA2LSB, (char*)"AARCH64" },
+        { EM_RISCV, EM_RISCV, ELFCLASS64, ELFDATA2LSB, (char*)"RISC-V" },
+    };
+
+#if (defined IA32)
+    static Elf32_Half running_arch_code = EM_386;
+#elif (defined AMD64) || (defined X32)
+    static Elf32_Half running_arch_code = EM_X86_64;
+#elif (defined IA64)
+    static Elf32_Half running_arch_code = EM_IA_64;
+#elif (defined __sparc) && (defined _LP64)
+    static Elf32_Half running_arch_code = EM_SPARCV9;
+#elif (defined __sparc) && (!defined _LP64)
+    static Elf32_Half running_arch_code = EM_SPARC;
+#elif (defined __powerpc64__)
+    static Elf32_Half running_arch_code = EM_PPC64;
+#elif (defined __powerpc__)
+    static Elf32_Half running_arch_code = EM_PPC;
+#elif (defined AARCH64)
+    static Elf32_Half running_arch_code = EM_AARCH64;
+#elif (defined ARM)
+    static Elf32_Half running_arch_code = EM_ARM;
+#elif (defined S390)
+    static Elf32_Half running_arch_code = EM_S390;
+#elif (defined ALPHA)
+    static Elf32_Half running_arch_code = EM_ALPHA;
+#elif (defined MIPSEL)
+    static Elf32_Half running_arch_code = EM_MIPS_RS3_LE;
+#elif (defined PARISC)
+    static Elf32_Half running_arch_code = EM_PARISC;
+#elif (defined MIPS)
+    static Elf32_Half running_arch_code = EM_MIPS;
+#elif (defined M68K)
+    static Elf32_Half running_arch_code = EM_68K;
+#elif (defined SH)
+    static Elf32_Half running_arch_code = EM_SH;
+#elif (defined RISCV)
+    static Elf32_Half running_arch_code = EM_RISCV;
+#else
+#    error Method os::dll_load requires that one of following is defined:\
+        AARCH64, ALPHA, ARM, AMD64, IA32, IA64, M68K, MIPS, MIPSEL, PARISC, __powerpc__, __powerpc64__, RISCV, S390, SH, __sparc
+#endif
+
+    // Identify compatibility class for VM's architecture and library's architecture
+    // Obtain string descriptions for architectures
+
+    arch_t lib_arch = { elf_head.e_machine, 0, elf_head.e_ident[EI_CLASS], elf_head.e_ident[EI_DATA], NULL };
+    int running_arch_index = -1;
+
+    for (unsigned int i = 0; i < ARRAY_SIZE(arch_array); i++) {
+        if (running_arch_code == arch_array[i].code) {
+            running_arch_index = i;
+        }
+        if (lib_arch.code == arch_array[i].code) {
+            lib_arch.compat_class = arch_array[i].compat_class;
+            lib_arch.name = arch_array[i].name;
+        }
+    }
+
+    assert(running_arch_index != -1,
+        "Didn't find running architecture code (running_arch_code) in arch_array");
+    if (running_arch_index == -1) {
+        // Even though running architecture detection failed
+        // we may still continue with reporting dlerror() message
+        return NULL;
+    }
+
+    if (lib_arch.compat_class != arch_array[running_arch_index].compat_class) {
+        if (lib_arch.name != NULL) {
+            ::snprintf(diag_msg_buf, diag_msg_max_length - 1,
+                " (Possible cause: can't load %s .so on a %s platform)",
+                lib_arch.name, arch_array[running_arch_index].name);
+        } else {
+            ::snprintf(diag_msg_buf, diag_msg_max_length - 1,
+                " (Possible cause: can't load this .so (machine code=0x%x) on a %s platform)",
+                lib_arch.code, arch_array[running_arch_index].name);
+        }
+        return NULL;
+    }
+
+    if (lib_arch.endianness != arch_array[running_arch_index].endianness) {
+        ::snprintf(diag_msg_buf, diag_msg_max_length - 1, " (Possible cause: endianness mismatch)");
+        return NULL;
+    }
+
+    // ELF file class/capacity : 0 - invalid, 1 - 32bit, 2 - 64bit
+    if (lib_arch.elf_class > 2 || lib_arch.elf_class < 1) {
+        ::snprintf(diag_msg_buf, diag_msg_max_length - 1, " (Possible cause: invalid ELF file class)");
+        return NULL;
+    }
+
+    if (lib_arch.elf_class != arch_array[running_arch_index].elf_class) {
+        ::snprintf(diag_msg_buf, diag_msg_max_length - 1,
+            " (Possible cause: architecture word width mismatch, can't load %d-bit .so on a %d-bit platform)",
+            (int)lib_arch.elf_class * 32, arch_array[running_arch_index].elf_class * 32);
+        return NULL;
+    }
+
+    return NULL;
+}
+
+void* os::Serenity::dlopen_helper(const char* filename, char* ebuf,
+    int ebuflen)
+{
+    void* result = ::dlopen(filename, RTLD_LAZY);
+    if (result == NULL) {
+        const char* error_report = ::dlerror();
+        if (error_report == NULL) {
+            error_report = "dlerror returned no error description";
+        }
+        if (ebuf != NULL && ebuflen > 0) {
+            ::strncpy(ebuf, error_report, ebuflen - 1);
+            ebuf[ebuflen - 1] = '\0';
+        }
+        Events::log(NULL, "Loading shared library %s failed, %s", filename, error_report);
+        log_info(os)("shared library load of %s failed, %s", filename, error_report);
+    } else {
+        Events::log(NULL, "Loaded shared library %s", filename);
+        log_info(os)("shared library load of %s was successful", filename);
+    }
+    return result;
+}
+
+void* os::Serenity::dll_load_in_vmthread(const char* filename, char* ebuf,
+    int ebuflen)
+{
+    void* result = NULL;
+    if (LoadExecStackDllInVMThread) {
+        result = dlopen_helper(filename, ebuf, ebuflen);
+    }
+
+    // Since 7019808, libjvm.so is linked with -noexecstack. If the VM loads a
+    // library that requires an executable stack, or which does not have this
+    // stack attribute set, dlopen changes the stack attribute to executable. The
+    // read protection of the guard pages gets lost.
+    //
+    // Need to check _stack_is_executable again as multiple VM_SerenityDllLoad
+    // may have been queued at the same time.
+
+    if (!_stack_is_executable) {
+        for (JavaThreadIteratorWithHandle jtiwh; JavaThread* jt = jtiwh.next();) {
+            StackOverflow* overflow_state = jt->stack_overflow_state();
+            if (!overflow_state->stack_guard_zone_unused() && // Stack not yet fully initialized
+                overflow_state->stack_guards_enabled()) {     // No pending stack overflow exceptions
+                if (!os::guard_memory((char*)jt->stack_end(), StackOverflow::stack_guard_zone_size())) {
+                    warning("Attempt to reguard stack yellow zone failed.");
+                }
+            }
+        }
+    }
+
+    return result;
+}
+
+void* os::dll_lookup(void* handle, const char* name)
+{
+    void* res = dlsym(handle, name);
+    return res;
+}
+
+void* os::get_default_process_handle()
+{
+    return (void*)::dlopen(NULL, RTLD_LAZY);
+}
+
+static bool _print_ascii_file(const char* filename, outputStream* st, const char* hdr = NULL)
+{
+    int fd = ::open(filename, O_RDONLY);
+    if (fd == -1) {
+        return false;
+    }
+
+    if (hdr != NULL) {
+        st->print_cr("%s", hdr);
+    }
+
+    char buf[33];
+    int bytes;
+    buf[32] = '\0';
+    while ((bytes = ::read(fd, buf, sizeof(buf) - 1)) > 0) {
+        st->print_raw(buf, bytes);
+    }
+
+    ::close(fd);
+
+    return true;
+}
+
+static void _print_ascii_file_h(const char* header, const char* filename, outputStream* st, bool same_line = true)
+{
+    st->print("%s:%c", header, same_line ? ' ' : '\n');
+    if (!_print_ascii_file(filename, st)) {
+        st->print_cr("<Not Available>");
+    }
+}
+
+void os::print_dll_info(outputStream* st)
+{
+    st->print_cr("Dynamic libraries:");
+
+    char fname[32];
+    pid_t pid = os::Serenity::gettid();
+
+    jio_snprintf(fname, sizeof(fname), "/proc/%d/maps", pid);
+
+    if (!_print_ascii_file(fname, st)) {
+        st->print_cr("Can not get library information for pid = %d", pid);
+    }
+}
+
+struct loaded_modules_info_param {
+    os::LoadedModulesCallbackFunc callback;
+    void* param;
+};
+
+static int dl_iterate_callback(struct dl_phdr_info* info, size_t size, void* data)
+{
+    if ((info->dlpi_name == NULL) || (*info->dlpi_name == '\0')) {
+        return 0;
+    }
+
+    struct loaded_modules_info_param* callback_param = reinterpret_cast<struct loaded_modules_info_param*>(data);
+    address base = NULL;
+    address top = NULL;
+    for (int idx = 0; idx < info->dlpi_phnum; idx++) {
+        const ElfW(Phdr)* phdr = info->dlpi_phdr + idx;
+        if (phdr->p_type == PT_LOAD) {
+            address raw_phdr_base = reinterpret_cast<address>(info->dlpi_addr + phdr->p_vaddr);
+
+            address phdr_base = align_down(raw_phdr_base, phdr->p_align);
+            if ((base == NULL) || (base > phdr_base)) {
+                base = phdr_base;
+            }
+
+            address phdr_top = align_up(raw_phdr_base + phdr->p_memsz, phdr->p_align);
+            if ((top == NULL) || (top < phdr_top)) {
+                top = phdr_top;
+            }
+        }
+    }
+
+    return callback_param->callback(info->dlpi_name, base, top, callback_param->param);
+}
+
+int os::get_loaded_modules_info(os::LoadedModulesCallbackFunc callback, void* param)
+{
+    struct loaded_modules_info_param callback_param = { callback, param };
+    return dl_iterate_phdr(&dl_iterate_callback, &callback_param);
+}
+
+void os::print_os_info_brief(outputStream* st)
+{
+    os::Serenity::print_distro_info(st);
+
+    os::Posix::print_uname_info(st);
+
+    os::Serenity::print_libversion_info(st);
+}
+
+void os::print_os_info(outputStream* st)
+{
+    st->print_cr("OS:");
+
+    os::Serenity::print_distro_info(st);
+
+    os::Posix::print_uname_info(st);
+
+    os::Serenity::print_uptime_info(st);
+
+    // Print warning if unsafe chroot environment detected
+    if (unsafe_chroot_detected) {
+        st->print_cr("WARNING!! %s", unstable_chroot_error);
+    }
+
+    os::Serenity::print_libversion_info(st);
+
+    os::Posix::print_rlimit_info(st);
+
+    os::Posix::print_load_average(st);
+    st->cr();
+
+    os::Serenity::print_system_memory_info(st);
+    st->cr();
+
+    os::Serenity::print_process_memory_info(st);
+    st->cr();
+
+    os::Serenity::print_proc_sys_info(st);
+    st->cr();
+
+    if (os::Serenity::print_ld_preload_file(st)) {
+        st->cr();
+    }
+
+    if (os::Serenity::print_container_info(st)) {
+        st->cr();
+    }
+
+    VM_Version::print_platform_virtualization_info(st);
+
+    os::Serenity::print_steal_info(st);
+}
+
+// Try to identify popular distros.
+// Most Serenity distributions have a /etc/XXX-release file, which contains
+// the OS version string. Newer Serenity distributions have a /etc/lsb-release
+// file that also contains the OS version string. Some have more than one
+// /etc/XXX-release file (e.g. Mandrake has both /etc/mandrake-release and
+// /etc/redhat-release.), so the order is important.
+// Any Serenity that is based on Redhat (i.e. Oracle, Mandrake, Sun JDS...) have
+// their own specific XXX-release file as well as a redhat-release file.
+// Because of this the XXX-release file needs to be searched for before the
+// redhat-release file.
+// Since Red Hat and SuSE have an lsb-release file that is not very descriptive the
+// search for redhat-release / SuSE-release needs to be before lsb-release.
+// Since the lsb-release file is the new standard it needs to be searched
+// before the older style release files.
+// Searching system-release (Red Hat) and os-release (other Serenityes) are a
+// next to last resort.  The os-release file is a new standard that contains
+// distribution information and the system-release file seems to be an old
+// standard that has been replaced by the lsb-release and os-release files.
+// Searching for the debian_version file is the last resort.  It contains
+// an informative string like "6.0.6" or "wheezy/sid". Because of this
+// "Debian " is printed before the contents of the debian_version file.
+
+const char* distro_files[] = {
+    "/etc/oracle-release",
+    "/etc/mandriva-release",
+    "/etc/mandrake-release",
+    "/etc/sun-release",
+    "/etc/redhat-release",
+    "/etc/SuSE-release",
+    "/etc/lsb-release",
+    "/etc/turbolinux-release",
+    "/etc/gentoo-release",
+    "/etc/ltib-release",
+    "/etc/angstrom-version",
+    "/etc/system-release",
+    "/etc/os-release",
+    NULL
+};
+
+void os::Serenity::print_distro_info(outputStream* st)
+{
+    for (int i = 0;; i++) {
+        const char* file = distro_files[i];
+        if (file == NULL) {
+            break; // done
+        }
+        // If file prints, we found it.
+        if (_print_ascii_file(file, st)) {
+            return;
+        }
+    }
+
+    if (file_exists("/etc/debian_version")) {
+        st->print("Debian ");
+        _print_ascii_file("/etc/debian_version", st);
+    } else {
+        st->print_cr("Serenity");
+    }
+}
+
+static void parse_os_info_helper(FILE* fp, char* distro, size_t length, bool get_first_line)
+{
+    char buf[256];
+    while (fgets(buf, sizeof(buf), fp)) {
+        // Edit out extra stuff in expected format
+        if (strstr(buf, "DISTRIB_DESCRIPTION=") != NULL || strstr(buf, "PRETTY_NAME=") != NULL) {
+            char* ptr = strstr(buf, "\""); // the name is in quotes
+            if (ptr != NULL) {
+                ptr++; // go beyond first quote
+                char* nl = strchr(ptr, '\"');
+                if (nl != NULL)
+                    *nl = '\0';
+                strncpy(distro, ptr, length);
+            } else {
+                ptr = strstr(buf, "=");
+                ptr++; // go beyond equals then
+                char* nl = strchr(ptr, '\n');
+                if (nl != NULL)
+                    *nl = '\0';
+                strncpy(distro, ptr, length);
+            }
+            return;
+        } else if (get_first_line) {
+            char* nl = strchr(buf, '\n');
+            if (nl != NULL)
+                *nl = '\0';
+            strncpy(distro, buf, length);
+            return;
+        }
+    }
+    // print last line and close
+    char* nl = strchr(buf, '\n');
+    if (nl != NULL)
+        *nl = '\0';
+    strncpy(distro, buf, length);
+}
+
+static void parse_os_info(char* distro, size_t length, const char* file)
+{
+    FILE* fp = fopen(file, "r");
+    if (fp != NULL) {
+        // if suse format, print out first line
+        bool get_first_line = (strcmp(file, "/etc/SuSE-release") == 0);
+        parse_os_info_helper(fp, distro, length, get_first_line);
+        fclose(fp);
+    }
+}
+
+void os::get_summary_os_info(char* buf, size_t buflen)
+{
+    for (int i = 0;; i++) {
+        const char* file = distro_files[i];
+        if (file == NULL) {
+            break; // ran out of distro_files
+        }
+        if (file_exists(file)) {
+            parse_os_info(buf, buflen, file);
+            return;
+        }
+    }
+    // special case for debian
+    if (file_exists("/etc/debian_version")) {
+        strncpy(buf, "Debian ", buflen);
+        if (buflen > 7) {
+            parse_os_info(&buf[7], buflen - 7, "/etc/debian_version");
+        }
+    } else {
+        strncpy(buf, "Serenity", buflen);
+    }
+}
+
+void os::Serenity::print_libversion_info(outputStream* st)
+{
+    // libc, pthread
+    st->print("libc: ");
+    st->print("%s ", os::Serenity::libc_version());
+    st->print("%s ", os::Serenity::libpthread_version());
+    st->cr();
+}
+
+void os::Serenity::print_proc_sys_info(outputStream* st)
+{
+    _print_ascii_file_h("/proc/sys/kernel/threads-max (system-wide limit on the number of threads)",
+        "/proc/sys/kernel/threads-max", st);
+    _print_ascii_file_h("/proc/sys/vm/max_map_count (maximum number of memory map areas a process may have)",
+        "/proc/sys/vm/max_map_count", st);
+    _print_ascii_file_h("/proc/sys/kernel/pid_max (system-wide limit on number of process identifiers)",
+        "/proc/sys/kernel/pid_max", st);
+}
+
+void os::Serenity::print_system_memory_info(outputStream* st)
+{
+    _print_ascii_file_h("/proc/meminfo", "/proc/meminfo", st, false);
+    st->cr();
+
+    // some information regarding THPs; for details see
+    // https://www.kernel.org/doc/Documentation/vm/transhuge.txt
+    _print_ascii_file_h("/sys/kernel/mm/transparent_hugepage/enabled",
+        "/sys/kernel/mm/transparent_hugepage/enabled", st);
+    _print_ascii_file_h("/sys/kernel/mm/transparent_hugepage/defrag (defrag/compaction efforts parameter)",
+        "/sys/kernel/mm/transparent_hugepage/defrag", st);
+}
+
+void os::Serenity::print_process_memory_info(outputStream* st)
+{
+
+    st->print_cr("Process Memory:");
+
+    // Print virtual and resident set size; peak values; swap; and for
+    //  rss its components if the kernel is recent enough.
+    ssize_t vmsize = -1, vmpeak = -1, vmswap = -1,
+            vmrss = -1, vmhwm = -1, rssanon = -1, rssfile = -1, rssshmem = -1;
+    const int num_values = 8;
+    int num_found = 0;
+    FILE* f = ::fopen("/proc/self/status", "r");
+    char buf[256];
+    if (f != NULL) {
+        while (::fgets(buf, sizeof(buf), f) != NULL && num_found < num_values) {
+            if ((vmsize == -1 && sscanf(buf, "VmSize: " SSIZE_FORMAT " kB", &vmsize) == 1) || (vmpeak == -1 && sscanf(buf, "VmPeak: " SSIZE_FORMAT " kB", &vmpeak) == 1) || (vmswap == -1 && sscanf(buf, "VmSwap: " SSIZE_FORMAT " kB", &vmswap) == 1) || (vmhwm == -1 && sscanf(buf, "VmHWM: " SSIZE_FORMAT " kB", &vmhwm) == 1) || (vmrss == -1 && sscanf(buf, "VmRSS: " SSIZE_FORMAT " kB", &vmrss) == 1) || (rssanon == -1 && sscanf(buf, "RssAnon: " SSIZE_FORMAT " kB", &rssanon) == 1) || (rssfile == -1 && sscanf(buf, "RssFile: " SSIZE_FORMAT " kB", &rssfile) == 1) || (rssshmem == -1 && sscanf(buf, "RssShmem: " SSIZE_FORMAT " kB", &rssshmem) == 1)) {
+                num_found++;
+            }
+        }
+        fclose(f);
+
+        st->print_cr("Virtual Size: " SSIZE_FORMAT "K (peak: " SSIZE_FORMAT "K)", vmsize, vmpeak);
+        st->print("Resident Set Size: " SSIZE_FORMAT "K (peak: " SSIZE_FORMAT "K)", vmrss, vmhwm);
+        if (rssanon != -1) { // requires kernel >= 4.5
+            st->print(" (anon: " SSIZE_FORMAT "K, file: " SSIZE_FORMAT "K, shmem: " SSIZE_FORMAT "K)",
+                rssanon, rssfile, rssshmem);
+        }
+        st->cr();
+        if (vmswap != -1) { // requires kernel >= 2.6.34
+            st->print_cr("Swapped out: " SSIZE_FORMAT "K", vmswap);
+        }
+    } else {
+        st->print_cr("Could not open /proc/self/status to get process memory related information");
+    }
+
+    // Print glibc outstanding allocations.
+    // (note: there is no implementation of mallinfo for muslc)
+#ifdef __GLIBC__
+    size_t total_allocated = 0;
+    bool might_have_wrapped = false;
+    if (_mallinfo2 != NULL) {
+        struct glibc_mallinfo2 mi = _mallinfo2();
+        total_allocated = mi.uordblks;
+    } else if (_mallinfo != NULL) {
+        // mallinfo is an old API. Member names mean next to nothing and, beyond that, are int.
+        // So values may have wrapped around. Still useful enough to see how much glibc thinks
+        // we allocated.
+        struct glibc_mallinfo mi = _mallinfo();
+        total_allocated = (size_t)(unsigned)mi.uordblks;
+        // Since mallinfo members are int, glibc values may have wrapped. Warn about this.
+        might_have_wrapped = (vmrss * K) > UINT_MAX && (vmrss * K) > (total_allocated + UINT_MAX);
+    }
+    if (_mallinfo2 != NULL || _mallinfo != NULL) {
+        st->print_cr("C-Heap outstanding allocations: " SIZE_FORMAT "K%s",
+            total_allocated / K,
+            might_have_wrapped ? " (may have wrapped)" : "");
+    }
+#endif // __GLIBC__
+}
+
+bool os::Serenity::print_ld_preload_file(outputStream* st)
+{
+    return _print_ascii_file("/etc/ld.so.preload", st, "/etc/ld.so.preload:");
+}
+
+void os::Serenity::print_uptime_info(outputStream* st)
+{
+    struct sysinfo sinfo;
+    int ret = sysinfo(&sinfo);
+    if (ret == 0) {
+        os::print_dhm(st, "OS uptime:", (long)sinfo.uptime);
+    }
+}
+
+bool os::Serenity::print_container_info(outputStream* st)
+{
+    if (!OSContainer::is_containerized()) {
+        return false;
+    }
+
+    st->print_cr("container (cgroup) information:");
+
+    const char* p_ct = OSContainer::container_type();
+    st->print_cr("container_type: %s", p_ct != NULL ? p_ct : "not supported");
+
+    char* p = OSContainer::cpu_cpuset_cpus();
+    st->print_cr("cpu_cpuset_cpus: %s", p != NULL ? p : "not supported");
+    free(p);
+
+    p = OSContainer::cpu_cpuset_memory_nodes();
+    st->print_cr("cpu_memory_nodes: %s", p != NULL ? p : "not supported");
+    free(p);
+
+    int i = OSContainer::active_processor_count();
+    st->print("active_processor_count: ");
+    if (i > 0) {
+        st->print_cr("%d", i);
+    } else {
+        st->print_cr("not supported");
+    }
+
+    i = OSContainer::cpu_quota();
+    st->print("cpu_quota: ");
+    if (i > 0) {
+        st->print_cr("%d", i);
+    } else {
+        st->print_cr("%s", i == OSCONTAINER_ERROR ? "not supported" : "no quota");
+    }
+
+    i = OSContainer::cpu_period();
+    st->print("cpu_period: ");
+    if (i > 0) {
+        st->print_cr("%d", i);
+    } else {
+        st->print_cr("%s", i == OSCONTAINER_ERROR ? "not supported" : "no period");
+    }
+
+    i = OSContainer::cpu_shares();
+    st->print("cpu_shares: ");
+    if (i > 0) {
+        st->print_cr("%d", i);
+    } else {
+        st->print_cr("%s", i == OSCONTAINER_ERROR ? "not supported" : "no shares");
+    }
+
+    jlong j = OSContainer::memory_limit_in_bytes();
+    st->print("memory_limit_in_bytes: ");
+    if (j > 0) {
+        st->print_cr(JLONG_FORMAT, j);
+    } else {
+        st->print_cr("%s", j == OSCONTAINER_ERROR ? "not supported" : "unlimited");
+    }
+
+    j = OSContainer::memory_and_swap_limit_in_bytes();
+    st->print("memory_and_swap_limit_in_bytes: ");
+    if (j > 0) {
+        st->print_cr(JLONG_FORMAT, j);
+    } else {
+        st->print_cr("%s", j == OSCONTAINER_ERROR ? "not supported" : "unlimited");
+    }
+
+    j = OSContainer::memory_soft_limit_in_bytes();
+    st->print("memory_soft_limit_in_bytes: ");
+    if (j > 0) {
+        st->print_cr(JLONG_FORMAT, j);
+    } else {
+        st->print_cr("%s", j == OSCONTAINER_ERROR ? "not supported" : "unlimited");
+    }
+
+    j = OSContainer::OSContainer::memory_usage_in_bytes();
+    st->print("memory_usage_in_bytes: ");
+    if (j > 0) {
+        st->print_cr(JLONG_FORMAT, j);
+    } else {
+        st->print_cr("%s", j == OSCONTAINER_ERROR ? "not supported" : "unlimited");
+    }
+
+    j = OSContainer::OSContainer::memory_max_usage_in_bytes();
+    st->print("memory_max_usage_in_bytes: ");
+    if (j > 0) {
+        st->print_cr(JLONG_FORMAT, j);
+    } else {
+        st->print_cr("%s", j == OSCONTAINER_ERROR ? "not supported" : "unlimited");
+    }
+
+    return true;
+}
+
+void os::Serenity::print_steal_info(outputStream* st)
+{
+    if (has_initial_tick_info) {
+        CPUPerfTicks pticks;
+        bool res = os::Serenity::get_tick_information(&pticks, -1);
+
+        if (res && pticks.has_steal_ticks) {
+            uint64_t steal_ticks_difference = pticks.steal - initial_steal_ticks;
+            uint64_t total_ticks_difference = pticks.total - initial_total_ticks;
+            double steal_ticks_perc = 0.0;
+            if (total_ticks_difference != 0) {
+                steal_ticks_perc = (double)steal_ticks_difference / total_ticks_difference;
+            }
+            st->print_cr("Steal ticks since vm start: " UINT64_FORMAT, steal_ticks_difference);
+            st->print_cr("Steal ticks percentage since vm start:%7.3f", steal_ticks_perc);
+        }
+    }
+}
+
+void os::print_memory_info(outputStream* st)
+{
+
+    st->print("Memory:");
+    st->print(" %dk page", os::vm_page_size() >> 10);
+
+    // values in struct sysinfo are "unsigned long"
+    struct sysinfo si;
+    sysinfo(&si);
+
+    st->print(", physical " UINT64_FORMAT "k",
+        os::physical_memory() >> 10);
+    st->print("(" UINT64_FORMAT "k free)",
+        os::available_memory() >> 10);
+    st->print(", swap " UINT64_FORMAT "k",
+        ((jlong)si.totalswap * si.mem_unit) >> 10);
+    st->print("(" UINT64_FORMAT "k free)",
+        ((jlong)si.freeswap * si.mem_unit) >> 10);
+    st->cr();
+}
+
+// Print the first "model name" line and the first "flags" line
+// that we find and nothing more. We assume "model name" comes
+// before "flags" so if we find a second "model name", then the
+// "flags" field is considered missing.
+static bool print_model_name_and_flags(outputStream* st, char* buf, size_t buflen)
+{
+#if defined(IA32) || defined(AMD64)
+    // Other platforms have less repetitive cpuinfo files
+    FILE* fp = fopen("/proc/cpuinfo", "r");
+    if (fp) {
+        bool model_name_printed = false;
+        while (!feof(fp)) {
+            if (fgets(buf, buflen, fp)) {
+                // Assume model name comes before flags
+                if (strstr(buf, "model name") != NULL) {
+                    if (!model_name_printed) {
+                        st->print_raw("CPU Model and flags from /proc/cpuinfo:\n");
+                        st->print_raw(buf);
+                        model_name_printed = true;
+                    } else {
+                        // model name printed but not flags?  Odd, just return
+                        fclose(fp);
+                        return true;
+                    }
+                }
+                // print the flags line too
+                if (strstr(buf, "flags") != NULL) {
+                    st->print_raw(buf);
+                    fclose(fp);
+                    return true;
+                }
+            }
+        }
+        fclose(fp);
+    }
+#endif // x86 platforms
+    return false;
+}
+
+// additional information about CPU e.g. available frequency ranges
+static void print_sys_devices_cpu_info(outputStream* st, char* buf, size_t buflen)
+{
+    _print_ascii_file_h("Online cpus", "/sys/devices/system/cpu/online", st);
+    _print_ascii_file_h("Offline cpus", "/sys/devices/system/cpu/offline", st);
+
+    if (ExtensiveErrorReports) {
+        // cache related info (cpu 0, should be similar for other CPUs)
+        for (unsigned int i = 0; i < 10; i++) { // handle max. 10 cache entries
+            char hbuf_level[60];
+            char hbuf_type[60];
+            char hbuf_size[60];
+            char hbuf_coherency_line_size[80];
+            snprintf(hbuf_level, 60, "/sys/devices/system/cpu/cpu0/cache/index%u/level", i);
+            snprintf(hbuf_type, 60, "/sys/devices/system/cpu/cpu0/cache/index%u/type", i);
+            snprintf(hbuf_size, 60, "/sys/devices/system/cpu/cpu0/cache/index%u/size", i);
+            snprintf(hbuf_coherency_line_size, 80, "/sys/devices/system/cpu/cpu0/cache/index%u/coherency_line_size", i);
+            if (file_exists(hbuf_level)) {
+                _print_ascii_file_h("cache level", hbuf_level, st);
+                _print_ascii_file_h("cache type", hbuf_type, st);
+                _print_ascii_file_h("cache size", hbuf_size, st);
+                _print_ascii_file_h("cache coherency line size", hbuf_coherency_line_size, st);
+            }
+        }
+    }
+
+    // we miss the cpufreq entries on Power and s390x
+#if defined(IA32) || defined(AMD64)
+    _print_ascii_file_h("BIOS frequency limitation", "/sys/devices/system/cpu/cpu0/cpufreq/bios_limit", st);
+    _print_ascii_file_h("Frequency switch latency (ns)", "/sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_transition_latency", st);
+    _print_ascii_file_h("Available cpu frequencies", "/sys/devices/system/cpu/cpu0/cpufreq/scaling_available_frequencies", st);
+    // min and max should be in the Available range but still print them (not all info might be available for all kernels)
+    if (ExtensiveErrorReports) {
+        _print_ascii_file_h("Maximum cpu frequency", "/sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_max_freq", st);
+        _print_ascii_file_h("Minimum cpu frequency", "/sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_min_freq", st);
+        _print_ascii_file_h("Current cpu frequency", "/sys/devices/system/cpu/cpu0/cpufreq/scaling_cur_freq", st);
+    }
+    // governors are power schemes, see https://wiki.archlinux.org/index.php/CPU_frequency_scaling
+    if (ExtensiveErrorReports) {
+        _print_ascii_file_h("Available governors", "/sys/devices/system/cpu/cpu0/cpufreq/scaling_available_governors", st);
+    }
+    _print_ascii_file_h("Current governor", "/sys/devices/system/cpu/cpu0/cpufreq/scaling_governor", st);
+    // Core performance boost, see https://www.kernel.org/doc/Documentation/cpu-freq/boost.txt
+    // Raise operating frequency of some cores in a multi-core package if certain conditions apply, e.g.
+    // whole chip is not fully utilized
+    _print_ascii_file_h("Core performance/turbo boost", "/sys/devices/system/cpu/cpufreq/boost", st);
+#endif
+}
+
+void os::pd_print_cpu_info(outputStream* st, char* buf, size_t buflen)
+{
+    // Only print the model name if the platform provides this as a summary
+    if (!print_model_name_and_flags(st, buf, buflen)) {
+        _print_ascii_file_h("/proc/cpuinfo", "/proc/cpuinfo", st, false);
+    }
+    st->cr();
+    print_sys_devices_cpu_info(st, buf, buflen);
+}
+
+#if defined(AMD64) || defined(IA32) || defined(X32)
+const char* search_string = "model name";
+#elif defined(M68K)
+const char* search_string = "CPU";
+#elif defined(PPC64)
+const char* search_string = "cpu";
+#elif defined(S390)
+const char* search_string = "machine =";
+#elif defined(SPARC)
+const char* search_string = "cpu";
+#else
+const char* search_string = "Processor";
+#endif
+
+// Parses the cpuinfo file for string representing the model name.
+void os::get_summary_cpu_info(char* cpuinfo, size_t length)
+{
+    FILE* fp = fopen("/proc/cpuinfo", "r");
+    if (fp != NULL) {
+        while (!feof(fp)) {
+            char buf[256];
+            if (fgets(buf, sizeof(buf), fp)) {
+                char* start = strstr(buf, search_string);
+                if (start != NULL) {
+                    char* ptr = start + strlen(search_string);
+                    char* end = buf + strlen(buf);
+                    while (ptr != end) {
+                        // skip whitespace and colon for the rest of the name.
+                        if (*ptr != ' ' && *ptr != '\t' && *ptr != ':') {
+                            break;
+                        }
+                        ptr++;
+                    }
+                    if (ptr != end) {
+                        // reasonable string, get rid of newline and keep the rest
+                        char* nl = strchr(buf, '\n');
+                        if (nl != NULL)
+                            *nl = '\0';
+                        strncpy(cpuinfo, ptr, length);
+                        fclose(fp);
+                        return;
+                    }
+                }
+            }
+        }
+        fclose(fp);
+    }
+    // cpuinfo not found or parsing failed, just print generic string.  The entire
+    // /proc/cpuinfo file will be printed later in the file (or enough of it for x86)
+#if defined(AARCH64)
+    strncpy(cpuinfo, "AArch64", length);
+#elif defined(AMD64)
+    strncpy(cpuinfo, "x86_64", length);
+#elif defined(ARM) // Order wrt. AARCH64 is relevant!
+    strncpy(cpuinfo, "ARM", length);
+#elif defined(IA32)
+    strncpy(cpuinfo, "x86_32", length);
+#elif defined(IA64)
+    strncpy(cpuinfo, "IA64", length);
+#elif defined(PPC)
+    strncpy(cpuinfo, "PPC64", length);
+#elif defined(S390)
+    strncpy(cpuinfo, "S390", length);
+#elif defined(SPARC)
+    strncpy(cpuinfo, "sparcv9", length);
+#elif defined(ZERO_LIBARCH)
+    strncpy(cpuinfo, ZERO_LIBARCH, length);
+#else
+    strncpy(cpuinfo, "unknown", length);
+#endif
+}
+
+static char saved_jvm_path[MAXPATHLEN] = { 0 };
+
+// Find the full path to the current module, libjvm.so
+void os::jvm_path(char* buf, jint buflen)
+{
+    // Error checking.
+    if (buflen < MAXPATHLEN) {
+        assert(false, "must use a large-enough buffer");
+        buf[0] = '\0';
+        return;
+    }
+    // Lazy resolve the path to current module.
+    if (saved_jvm_path[0] != 0) {
+        strcpy(buf, saved_jvm_path);
+        return;
+    }
+
+    char dli_fname[MAXPATHLEN];
+    dli_fname[0] = '\0';
+    bool ret = dll_address_to_library_name(
+        CAST_FROM_FN_PTR(address, os::jvm_path),
+        dli_fname, sizeof(dli_fname), NULL);
+    assert(ret, "cannot locate libjvm");
+    char* rp = NULL;
+    if (ret && dli_fname[0] != '\0') {
+        rp = os::Posix::realpath(dli_fname, buf, buflen);
+    }
+    if (rp == NULL) {
+        return;
+    }
+
+    if (Arguments::sun_java_launcher_is_altjvm()) {
+        // Support for the java launcher's '-XXaltjvm=<path>' option. Typical
+        // value for buf is "<JAVA_HOME>/jre/lib/<vmtype>/libjvm.so".
+        // If "/jre/lib/" appears at the right place in the string, then
+        // assume we are installed in a JDK and we're done. Otherwise, check
+        // for a JAVA_HOME environment variable and fix up the path so it
+        // looks like libjvm.so is installed there (append a fake suffix
+        // hotspot/libjvm.so).
+        const char* p = buf + strlen(buf) - 1;
+        for (int count = 0; p > buf && count < 5; ++count) {
+            for (--p; p > buf && *p != '/'; --p)
+                /* empty */;
+        }
+
+        if (strncmp(p, "/jre/lib/", 9) != 0) {
+            // Look for JAVA_HOME in the environment.
+            char* java_home_var = ::getenv("JAVA_HOME");
+            if (java_home_var != NULL && java_home_var[0] != 0) {
+                char* jrelib_p;
+                int len;
+
+                // Check the current module name "libjvm.so".
+                p = strrchr(buf, '/');
+                if (p == NULL) {
+                    return;
+                }
+                assert(strstr(p, "/libjvm") == p, "invalid library name");
+
+                rp = os::Posix::realpath(java_home_var, buf, buflen);
+                if (rp == NULL) {
+                    return;
+                }
+
+                // determine if this is a legacy image or modules image
+                // modules image doesn't have "jre" subdirectory
+                len = strlen(buf);
+                assert(len < buflen, "Ran out of buffer room");
+                jrelib_p = buf + len;
+                snprintf(jrelib_p, buflen - len, "/jre/lib");
+                if (0 != access(buf, F_OK)) {
+                    snprintf(jrelib_p, buflen - len, "/lib");
+                }
+
+                if (0 == access(buf, F_OK)) {
+                    // Use current module name "libjvm.so"
+                    len = strlen(buf);
+                    snprintf(buf + len, buflen - len, "/hotspot/libjvm.so");
+                } else {
+                    // Go back to path of .so
+                    rp = os::Posix::realpath(dli_fname, buf, buflen);
+                    if (rp == NULL) {
+                        return;
+                    }
+                }
+            }
+        }
+    }
+
+    strncpy(saved_jvm_path, buf, MAXPATHLEN);
+    saved_jvm_path[MAXPATHLEN - 1] = '\0';
+}
+
+void os::print_jni_name_prefix_on(outputStream* st, int args_size)
+{
+    // no prefix required, not even "_"
+}
+
+void os::print_jni_name_suffix_on(outputStream* st, int args_size)
+{
+    // no suffix required
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// Virtual Memory
+
+int os::vm_page_size()
+{
+    // Seems redundant as all get out
+    assert(os::Serenity::page_size() != -1, "must call os::init");
+    return os::Serenity::page_size();
+}
+
+// Solaris allocates memory by pages.
+int os::vm_allocation_granularity()
+{
+    assert(os::Serenity::page_size() != -1, "must call os::init");
+    return os::Serenity::page_size();
+}
+
+// Rationale behind this function:
+//  current (Mon Apr 25 20:12:18 MSD 2005) oprofile drops samples without executable
+//  mapping for address (see lookup_dcookie() in the kernel module), thus we cannot get
+//  samples for JITted code. Here we create private executable mapping over the code cache
+//  and then we can use standard (well, almost, as mapping can change) way to provide
+//  info for the reporting script by storing timestamp and location of symbol
+void linux_wrap_code(char* base, size_t size)
+{
+    static volatile jint cnt = 0;
+
+    if (!UseOprofile) {
+        return;
+    }
+
+    char buf[PATH_MAX + 1];
+    int num = Atomic::add(&cnt, 1);
+
+    snprintf(buf, sizeof(buf), "%s/hs-vm-%d-%d",
+        os::get_temp_directory(), os::current_process_id(), num);
+    unlink(buf);
+
+    int fd = ::open(buf, O_CREAT | O_RDWR, S_IRWXU);
+
+    if (fd != -1) {
+        off_t rv = ::lseek(fd, size - 2, SEEK_SET);
+        if (rv != (off_t)-1) {
+            if (::write(fd, "", 1) == 1) {
+                mmap(base, size,
+                    PROT_READ | PROT_WRITE | PROT_EXEC,
+                    MAP_PRIVATE | MAP_FIXED | MAP_NORESERVE, fd, 0);
+            }
+        }
+        ::close(fd);
+        unlink(buf);
+    }
+}
+
+static bool recoverable_mmap_error(int err)
+{
+    // See if the error is one we can let the caller handle. This
+    // list of errno values comes from JBS-6843484. I can't find a
+    // Serenity man page that documents this specific set of errno
+    // values so while this list currently matches Solaris, it may
+    // change as we gain experience with this failure mode.
+    switch (err) {
+    case EBADF:
+    case EINVAL:
+    case ENOTSUP:
+        // let the caller deal with these errors
+        return true;
+
+    default:
+        // Any remaining errors on this OS can cause our reserved mapping
+        // to be lost. That can cause confusion where different data
+        // structures think they have the same memory mapped. The worst
+        // scenario is if both the VM and a library think they have the
+        // same memory mapped.
+        return false;
+    }
+}
+
+static void warn_fail_commit_memory(char* addr, size_t size, bool exec,
+    int err)
+{
+    warning("INFO: os::commit_memory(" PTR_FORMAT ", " SIZE_FORMAT
+            ", %d) failed; error='%s' (errno=%d)",
+        p2i(addr), size, exec,
+        os::strerror(err), err);
+}
+
+static void warn_fail_commit_memory(char* addr, size_t size,
+    size_t alignment_hint, bool exec,
+    int err)
+{
+    warning("INFO: os::commit_memory(" PTR_FORMAT ", " SIZE_FORMAT
+            ", " SIZE_FORMAT ", %d) failed; error='%s' (errno=%d)",
+        p2i(addr), size,
+        alignment_hint, exec, os::strerror(err), err);
+}
+
+// NOTE: Serenity kernel does not really reserve the pages for us.
+//       All it does is to check if there are enough free pages
+//       left at the time of mmap(). This could be a potential
+//       problem.
+int os::Serenity::commit_memory_impl(char* addr, size_t size, bool exec)
+{
+    int prot = exec ? PROT_READ | PROT_WRITE | PROT_EXEC : PROT_READ | PROT_WRITE;
+    uintptr_t res = (uintptr_t)::mmap(addr, size, prot,
+        MAP_PRIVATE | MAP_FIXED | MAP_ANONYMOUS, -1, 0);
+    if (res != (uintptr_t)MAP_FAILED) {
+        return 0;
+    }
+
+    int err = errno; // save errno from mmap() call above
+
+    if (!recoverable_mmap_error(err)) {
+        warn_fail_commit_memory(addr, size, exec, err);
+        vm_exit_out_of_memory(size, OOM_MMAP_ERROR, "committing reserved memory.");
+    }
+
+    return err;
+}
+
+bool os::pd_commit_memory(char* addr, size_t size, bool exec)
+{
+    return os::Serenity::commit_memory_impl(addr, size, exec) == 0;
+}
+
+void os::pd_commit_memory_or_exit(char* addr, size_t size, bool exec,
+    const char* mesg)
+{
+    assert(mesg != NULL, "mesg must be specified");
+    int err = os::Serenity::commit_memory_impl(addr, size, exec);
+    if (err != 0) {
+        // the caller wants all commit errors to exit with the specified mesg:
+        warn_fail_commit_memory(addr, size, exec, err);
+        vm_exit_out_of_memory(size, OOM_MMAP_ERROR, "%s", mesg);
+    }
+}
+
+// Define MAP_HUGETLB here so we can build HotSpot on old systems.
+#ifndef MAP_HUGETLB
+#    define MAP_HUGETLB 0x40000
+#endif
+
+// If mmap flags are set with MAP_HUGETLB and the system supports multiple
+// huge page sizes, flag bits [26:31] can be used to encode the log2 of the
+// desired huge page size. Otherwise, the system's default huge page size will be used.
+// See mmap(2) man page for more info (since Serenity 3.8).
+// https://lwn.net/Articles/533499/
+#ifndef MAP_HUGE_SHIFT
+#    define MAP_HUGE_SHIFT 26
+#endif
+
+// Define MADV_HUGEPAGE here so we can build HotSpot on old systems.
+#ifndef MADV_HUGEPAGE
+#    define MADV_HUGEPAGE 14
+#endif
+
+int os::Serenity::commit_memory_impl(char* addr, size_t size,
+    size_t alignment_hint, bool exec)
+{
+    int err = os::Serenity::commit_memory_impl(addr, size, exec);
+    if (err == 0) {
+        realign_memory(addr, size, alignment_hint);
+    }
+    return err;
+}
+
+bool os::pd_commit_memory(char* addr, size_t size, size_t alignment_hint,
+    bool exec)
+{
+    return os::Serenity::commit_memory_impl(addr, size, alignment_hint, exec) == 0;
+}
+
+void os::pd_commit_memory_or_exit(char* addr, size_t size,
+    size_t alignment_hint, bool exec,
+    const char* mesg)
+{
+    assert(mesg != NULL, "mesg must be specified");
+    int err = os::Serenity::commit_memory_impl(addr, size, alignment_hint, exec);
+    if (err != 0) {
+        // the caller wants all commit errors to exit with the specified mesg:
+        warn_fail_commit_memory(addr, size, alignment_hint, exec, err);
+        vm_exit_out_of_memory(size, OOM_MMAP_ERROR, "%s", mesg);
+    }
+}
+
+void os::pd_realign_memory(char* addr, size_t bytes, size_t alignment_hint)
+{
+    if (UseTransparentHugePages && alignment_hint > (size_t)vm_page_size()) {
+        // We don't check the return value: madvise(MADV_HUGEPAGE) may not
+        // be supported or the memory may already be backed by huge pages.
+        ::madvise(addr, bytes, MADV_HUGEPAGE);
+    }
+}
+
+void os::pd_free_memory(char* addr, size_t bytes, size_t alignment_hint)
+{
+    // This method works by doing an mmap over an existing mmaping and effectively discarding
+    // the existing pages. However it won't work for SHM-based large pages that cannot be
+    // uncommitted at all. We don't do anything in this case to avoid creating a segment with
+    // small pages on top of the SHM segment. This method always works for small pages, so we
+    // allow that in any case.
+    if (alignment_hint <= (size_t)os::vm_page_size() || can_commit_large_page_memory()) {
+        commit_memory(addr, bytes, alignment_hint, !ExecMem);
+    }
+}
+
+void os::numa_make_global(char*, size_t)
+{
+}
+
+// Define for numa_set_bind_policy(int). Setting the argument to 0 will set the
+// bind policy to MPOL_PREFERRED for the current thread.
+#define USE_MPOL_PREFERRED 0
+
+void os::numa_make_local(char*, size_t, int)
+{
+}
+
+bool os::numa_topology_changed() { return false; }
+
+size_t os::numa_get_groups_num()
+{
+    return 1;
+}
+
+int os::numa_get_group_id()
+{
+    return 0;
+}
+
+int os::numa_get_group_id_for_address(const void* address)
+{
+    return 0;
+}
+
+int os::Serenity::get_existing_num_nodes()
+{
+    return 1;
+}
+
+size_t os::numa_get_leaf_groups(int* ids, size_t size)
+{
+    ids[0] - 0;
+    return 1;
+}
+
+bool os::get_page_info(char* start, page_info* info)
+{
+    return false;
+}
+
+char* os::scan_pages(char* start, char* end, page_info* page_expected,
+    page_info* page_found)
+{
+    return end;
+}
+
+int os::Serenity::sched_getcpu_syscall(void)
+{
+    // FIXME!
+    return -1;
+}
+
+void os::Serenity::sched_getcpu_init()
+{
+    set_sched_getcpu(CAST_TO_FN_PTR(sched_getcpu_func_t,
+        (void*)&sched_getcpu_syscall));
+
+    if (sched_getcpu() == -1) {
+        vm_exit_during_initialization("getcpu(2) system call not supported by kernel");
+    }
+}
+
+// Something to do with the numa-aware allocator needs these symbols
+extern "C" JNIEXPORT void numa_warn(int number, char* where, ...) { }
+extern "C" JNIEXPORT void numa_error(char* where) { }
+
+size_t os::Serenity::default_guard_size(os::ThreadType thr_type)
+{
+    // Creating guard page is very expensive. Java thread has HotSpot
+    // guard pages, only enable glibc guard page for non-Java threads.
+    // (Remember: compiler thread is a Java thread, too!)
+    return ((thr_type == java_thread || thr_type == compiler_thread) ? 0 : page_size());
+}
+
+// rebuild_cpu_to_node_map() constructs a table mapping cpud id to node id.
+// The table is later used in get_node_by_cpu().
+void os::Serenity::rebuild_cpu_to_node_map()
+{
+    const size_t NCPUS = 32768; // Since the buffer size computation is very obscure
+                                // in libnuma (possible values are starting from 16,
+                                // and continuing up with every other power of 2, but less
+                                // than the maximum number of CPUs supported by kernel), and
+                                // is a subject to change (in libnuma version 2 the requirements
+                                // are more reasonable) we'll just hardcode the number they use
+                                // in the library.
+    const size_t BitsPerCLong = sizeof(long) * CHAR_BIT;
+
+    size_t cpu_num = processor_count();
+    size_t cpu_map_size = NCPUS / BitsPerCLong;
+    size_t cpu_map_valid_size = MIN2((cpu_num + BitsPerCLong - 1) / BitsPerCLong, cpu_map_size);
+
+    cpu_to_node()->clear();
+    cpu_to_node()->at_grow(cpu_num - 1);
+
+    size_t node_num = get_existing_num_nodes();
+
+    int distance = 0;
+    int closest_distance = INT_MAX;
+    int closest_node = 0;
+    unsigned long* cpu_map = NEW_C_HEAP_ARRAY(unsigned long, cpu_map_size, mtInternal);
+    for (size_t i = 0; i < node_num; i++) {
+        // Check if node is configured (not a memory-less node). If it is not, find
+        // the closest configured node. Check also if node is bound, i.e. it's allowed
+        // to allocate memory from the node. If it's not allowed, map cpus in that node
+        // to the closest node from which memory allocation is allowed.
+        if (!is_node_in_configured_nodes(nindex_to_node()->at(i)) || !is_node_in_bound_nodes(nindex_to_node()->at(i))) {
+            closest_distance = INT_MAX;
+            // Check distance from all remaining nodes in the system. Ignore distance
+            // from itself, from another non-configured node, and from another non-bound
+            // node.
+            for (size_t m = 0; m < node_num; m++) {
+                if (m != i && is_node_in_configured_nodes(nindex_to_node()->at(m)) && is_node_in_bound_nodes(nindex_to_node()->at(m))) {
+                    distance = numa_distance(nindex_to_node()->at(i), nindex_to_node()->at(m));
+                    // If a closest node is found, update. There is always at least one
+                    // configured and bound node in the system so there is always at least
+                    // one node close.
+                    if (distance != 0 && distance < closest_distance) {
+                        closest_distance = distance;
+                        closest_node = nindex_to_node()->at(m);
+                    }
+                }
+            }
+        } else {
+            // Current node is already a configured node.
+            closest_node = nindex_to_node()->at(i);
+        }
+
+        // Get cpus from the original node and map them to the closest node. If node
+        // is a configured node (not a memory-less node), then original node and
+        // closest node are the same.
+        if (numa_node_to_cpus(nindex_to_node()->at(i), cpu_map, cpu_map_size * sizeof(unsigned long)) != -1) {
+            for (size_t j = 0; j < cpu_map_valid_size; j++) {
+                if (cpu_map[j] != 0) {
+                    for (size_t k = 0; k < BitsPerCLong; k++) {
+                        if (cpu_map[j] & (1UL << k)) {
+                            int cpu_index = j * BitsPerCLong + k;
+
+#ifndef PRODUCT
+                            if (UseDebuggerErgo1 && cpu_index >= (int)cpu_num) {
+                                // Some debuggers limit the processor count without
+                                // intercepting the NUMA APIs. Just fake the values.
+                                cpu_index = 0;
+                            }
+#endif
+
+                            cpu_to_node()->at_put(cpu_index, closest_node);
+                        }
+                    }
+                }
+            }
+        }
+    }
+    FREE_C_HEAP_ARRAY(unsigned long, cpu_map);
+}
+
+os::Serenity::sched_getcpu_func_t os::Serenity::_sched_getcpu;
+
+bool os::pd_uncommit_memory(char* addr, size_t size, bool exec)
+{
+    uintptr_t res = (uintptr_t)::mmap(addr, size, PROT_NONE,
+        MAP_PRIVATE | MAP_FIXED | MAP_NORESERVE | MAP_ANONYMOUS, -1, 0);
+    return res != (uintptr_t)MAP_FAILED;
+}
+
+static address get_stack_commited_bottom(address bottom, size_t size)
+{
+    address nbot = bottom;
+    address ntop = bottom + size;
+
+    size_t page_sz = os::vm_page_size();
+    unsigned pages = size / page_sz;
+
+    unsigned char vec[1];
+    unsigned imin = 1, imax = pages + 1, imid;
+    int mincore_return_value = 0;
+
+    assert(imin <= imax, "Unexpected page size");
+
+    while (imin < imax) {
+        imid = (imax + imin) / 2;
+        nbot = ntop - (imid * page_sz);
+
+        // Use a trick with mincore to check whether the page is mapped or not.
+        // mincore sets vec to 1 if page resides in memory and to 0 if page
+        // is swapped output but if page we are asking for is unmapped
+        // it returns -1,ENOMEM
+        mincore_return_value = mincore(nbot, page_sz, vec);
+
+        if (mincore_return_value == -1) {
+            // Page is not mapped go up
+            // to find first mapped page
+            if (errno != EAGAIN) {
+                assert(errno == ENOMEM, "Unexpected mincore errno");
+                imax = imid;
+            }
+        } else {
+            // Page is mapped go down
+            // to find first not mapped page
+            imin = imid + 1;
+        }
+    }
+
+    nbot = nbot + page_sz;
+
+    // Adjust stack bottom one page up if last checked page is not mapped
+    if (mincore_return_value == -1) {
+        nbot = nbot + page_sz;
+    }
+
+    return nbot;
+}
+
+bool os::committed_in_range(address start, size_t size, address& committed_start, size_t& committed_size)
+{
+    int mincore_return_value;
+    const size_t stripe = 1024; // query this many pages each time
+    unsigned char vec[stripe + 1];
+    // set a guard
+    vec[stripe] = 'X';
+
+    const size_t page_sz = os::vm_page_size();
+    size_t pages = size / page_sz;
+
+    assert(is_aligned(start, page_sz), "Start address must be page aligned");
+    assert(is_aligned(size, page_sz), "Size must be page aligned");
+
+    committed_start = NULL;
+
+    int loops = (pages + stripe - 1) / stripe;
+    int committed_pages = 0;
+    address loop_base = start;
+    bool found_range = false;
+
+    for (int index = 0; index < loops && !found_range; index++) {
+        assert(pages > 0, "Nothing to do");
+        int pages_to_query = (pages >= stripe) ? stripe : pages;
+        pages -= pages_to_query;
+
+        // Get stable read
+        while ((mincore_return_value = mincore(loop_base, pages_to_query * page_sz, vec)) == -1 && errno == EAGAIN)
+            ;
+
+        // During shutdown, some memory goes away without properly notifying NMT,
+        // E.g. ConcurrentGCThread/WatcherThread can exit without deleting thread object.
+        // Bailout and return as not committed for now.
+        if (mincore_return_value == -1 && errno == ENOMEM) {
+            return false;
+        }
+
+        assert(vec[stripe] == 'X', "overflow guard");
+        assert(mincore_return_value == 0, "Range must be valid");
+        // Process this stripe
+        for (int vecIdx = 0; vecIdx < pages_to_query; vecIdx++) {
+            if ((vec[vecIdx] & 0x01) == 0) { // not committed
+                // End of current contiguous region
+                if (committed_start != NULL) {
+                    found_range = true;
+                    break;
+                }
+            } else { // committed
+                // Start of region
+                if (committed_start == NULL) {
+                    committed_start = loop_base + page_sz * vecIdx;
+                }
+                committed_pages++;
+            }
+        }
+
+        loop_base += pages_to_query * page_sz;
+    }
+
+    if (committed_start != NULL) {
+        assert(committed_pages > 0, "Must have committed region");
+        assert(committed_pages <= int(size / page_sz), "Can not commit more than it has");
+        assert(committed_start >= start && committed_start < start + size, "Out of range");
+        committed_size = page_sz * committed_pages;
+        return true;
+    } else {
+        assert(committed_pages == 0, "Should not have committed region");
+        return false;
+    }
+}
+
+// Serenity uses a growable mapping for the stack, and if the mapping for
+// the stack guard pages is not removed when we detach a thread the
+// stack cannot grow beyond the pages where the stack guard was
+// mapped.  If at some point later in the process the stack expands to
+// that point, the Serenity kernel cannot expand the stack any further
+// because the guard pages are in the way, and a segfault occurs.
+//
+// However, it's essential not to split the stack region by unmapping
+// a region (leaving a hole) that's already part of the stack mapping,
+// so if the stack mapping has already grown beyond the guard pages at
+// the time we create them, we have to truncate the stack mapping.
+// So, we need to know the extent of the stack mapping when
+// create_stack_guard_pages() is called.
+
+// We only need this for stacks that are growable: at the time of
+// writing thread stacks don't use growable mappings (i.e. those
+// creeated with MAP_GROWSDOWN), and aren't marked "[stack]", so this
+// only applies to the main thread.
+
+// If the (growable) stack mapping already extends beyond the point
+// where we're going to put our guard pages, truncate the mapping at
+// that point by munmap()ping it.  This ensures that when we later
+// munmap() the guard pages we don't leave a hole in the stack
+// mapping. This only affects the main/primordial thread
+
+bool os::pd_create_stack_guard_pages(char* addr, size_t size)
+{
+    if (os::is_primordial_thread()) {
+        // As we manually grow stack up to bottom inside create_attached_thread(),
+        // it's likely that os::Serenity::initial_thread_stack_bottom is mapped and
+        // we don't need to do anything special.
+        // Check it first, before calling heavy function.
+        uintptr_t stack_extent = (uintptr_t)os::Serenity::initial_thread_stack_bottom();
+        unsigned char vec[1];
+
+        if (mincore((address)stack_extent, os::vm_page_size(), vec) == -1) {
+            // Fallback to slow path on all errors, including EAGAIN
+            stack_extent = (uintptr_t)get_stack_commited_bottom(
+                os::Serenity::initial_thread_stack_bottom(),
+                (size_t)addr - stack_extent);
+        }
+
+        if (stack_extent < (uintptr_t)addr) {
+            ::munmap((void*)stack_extent, (uintptr_t)(addr - stack_extent));
+        }
+    }
+
+    return os::commit_memory(addr, size, !ExecMem);
+}
+
+// If this is a growable mapping, remove the guard pages entirely by
+// munmap()ping them.  If not, just call uncommit_memory(). This only
+// affects the main/primordial thread, but guard against future OS changes.
+// It's safe to always unmap guard pages for primordial thread because we
+// always place it right after end of the mapped region.
+
+bool os::remove_stack_guard_pages(char* addr, size_t size)
+{
+    uintptr_t stack_extent, stack_base;
+
+    if (os::is_primordial_thread()) {
+        return ::munmap(addr, size) == 0;
+    }
+
+    return os::uncommit_memory(addr, size);
+}
+
+// 'requested_addr' is only treated as a hint, the return value may or
+// may not start from the requested address. Unlike Serenity mmap(), this
+// function returns NULL to indicate failure.
+static char* anon_mmap(char* requested_addr, size_t bytes)
+{
+    // MAP_FIXED is intentionally left out, to leave existing mappings intact.
+    const int flags = MAP_PRIVATE | MAP_NORESERVE | MAP_ANONYMOUS;
+
+    // Map reserved/uncommitted pages PROT_NONE so we fail early if we
+    // touch an uncommitted page. Otherwise, the read/write might
+    // succeed if we have enough swap space to back the physical page.
+    char* addr = (char*)::mmap(requested_addr, bytes, PROT_NONE, flags, -1, 0);
+
+    return addr == MAP_FAILED ? NULL : addr;
+}
+
+// Allocate (using mmap, NO_RESERVE, with small pages) at either a given request address
+//   (req_addr != NULL) or with a given alignment.
+//  - bytes shall be a multiple of alignment.
+//  - req_addr can be NULL. If not NULL, it must be a multiple of alignment.
+//  - alignment sets the alignment at which memory shall be allocated.
+//     It must be a multiple of allocation granularity.
+// Returns address of memory or NULL. If req_addr was not NULL, will only return
+//  req_addr or NULL.
+static char* anon_mmap_aligned(char* req_addr, size_t bytes, size_t alignment)
+{
+    size_t extra_size = bytes;
+    if (req_addr == NULL && alignment > 0) {
+        extra_size += alignment;
+    }
+
+    char* start = anon_mmap(req_addr, extra_size);
+    if (start != NULL) {
+        if (req_addr != NULL) {
+            if (start != req_addr) {
+                ::munmap(start, extra_size);
+                start = NULL;
+            }
+        } else {
+            char* const start_aligned = align_up(start, alignment);
+            char* const end_aligned = start_aligned + bytes;
+            char* const end = start + extra_size;
+            if (start_aligned > start) {
+                ::munmap(start, start_aligned - start);
+            }
+            if (end_aligned < end) {
+                ::munmap(end_aligned, end - end_aligned);
+            }
+            start = start_aligned;
+        }
+    }
+    return start;
+}
+
+static int anon_munmap(char* addr, size_t size)
+{
+    return ::munmap(addr, size) == 0;
+}
+
+char* os::pd_reserve_memory(size_t bytes, bool exec)
+{
+    return anon_mmap(NULL, bytes);
+}
+
+bool os::pd_release_memory(char* addr, size_t size)
+{
+    return anon_munmap(addr, size);
+}
+
+#ifdef CAN_SHOW_REGISTERS_ON_ASSERT
+extern char* g_assert_poison; // assertion poison page address
+#endif
+
+static bool linux_mprotect(char* addr, size_t size, int prot)
+{
+    // Serenity wants the mprotect address argument to be page aligned.
+    char* bottom = (char*)align_down((intptr_t)addr, os::Serenity::page_size());
+
+    // According to SUSv3, mprotect() should only be used with mappings
+    // established by mmap(), and mmap() always maps whole pages. Unaligned
+    // 'addr' likely indicates problem in the VM (e.g. trying to change
+    // protection of malloc'ed or statically allocated memory). Check the
+    // caller if you hit this assert.
+    assert(addr == bottom, "sanity check");
+
+    size = align_up(pointer_delta(addr, bottom, 1) + size, os::Serenity::page_size());
+    // Don't log anything if we're executing in the poison page signal handling
+    // context. It can lead to reentrant use of other parts of the VM code.
+#ifdef CAN_SHOW_REGISTERS_ON_ASSERT
+    if (addr != g_assert_poison)
+#endif
+        Events::log(NULL, "Protecting memory [" INTPTR_FORMAT "," INTPTR_FORMAT "] with protection modes %x", p2i(bottom), p2i(bottom + size), prot);
+    return ::mprotect(bottom, size, prot) == 0;
+}
+
+// Set protections specified
+bool os::protect_memory(char* addr, size_t bytes, ProtType prot,
+    bool is_committed)
+{
+    unsigned int p = 0;
+    switch (prot) {
+    case MEM_PROT_NONE:
+        p = PROT_NONE;
+        break;
+    case MEM_PROT_READ:
+        p = PROT_READ;
+        break;
+    case MEM_PROT_RW:
+        p = PROT_READ | PROT_WRITE;
+        break;
+    case MEM_PROT_RWX:
+        p = PROT_READ | PROT_WRITE | PROT_EXEC;
+        break;
+    default:
+        ShouldNotReachHere();
+    }
+    // is_committed is unused.
+    return linux_mprotect(addr, bytes, p);
+}
+
+bool os::guard_memory(char* addr, size_t size)
+{
+    return linux_mprotect(addr, size, PROT_NONE);
+}
+
+bool os::unguard_memory(char* addr, size_t size)
+{
+    return linux_mprotect(addr, size, PROT_READ | PROT_WRITE);
+}
+
+bool os::Serenity::transparent_huge_pages_sanity_check(bool warn,
+    size_t page_size)
+{
+    bool result = false;
+    void* p = mmap(NULL, page_size * 2, PROT_READ | PROT_WRITE,
+        MAP_ANONYMOUS | MAP_PRIVATE,
+        -1, 0);
+    if (p != MAP_FAILED) {
+        void* aligned_p = align_up(p, page_size);
+
+        result = madvise(aligned_p, page_size, MADV_HUGEPAGE) == 0;
+
+        munmap(p, page_size * 2);
+    }
+
+    if (warn && !result) {
+        warning("TransparentHugePages is not supported by the operating system.");
+    }
+
+    return result;
+}
+
+int os::Serenity::hugetlbfs_page_size_flag(size_t page_size)
+{
+    if (page_size != default_large_page_size()) {
+        return (exact_log2(page_size) << MAP_HUGE_SHIFT);
+    }
+    return 0;
+}
+
+bool os::Serenity::hugetlbfs_sanity_check(bool warn, size_t page_size)
+{
+    // Include the page size flag to ensure we sanity check the correct page size.
+    int flags = MAP_ANONYMOUS | MAP_PRIVATE | MAP_HUGETLB | hugetlbfs_page_size_flag(page_size);
+    void* p = mmap(NULL, page_size, PROT_READ | PROT_WRITE, flags, -1, 0);
+
+    if (p != MAP_FAILED) {
+        // Mapping succeeded, sanity check passed.
+        munmap(p, page_size);
+        return true;
+    }
+
+    if (warn) {
+        warning("HugeTLBFS is not configured or not supported by the operating system.");
+    }
+
+    return false;
+}
+
+bool os::Serenity::shm_hugetlbfs_sanity_check(bool warn, size_t page_size)
+{
+    // Try to create a large shared memory segment.
+    int shmid = shmget(IPC_PRIVATE, page_size, SHM_HUGETLB | IPC_CREAT | SHM_R | SHM_W);
+    if (shmid == -1) {
+        // Possible reasons for shmget failure:
+        // 1. shmmax is too small for the request.
+        //    > check shmmax value: cat /proc/sys/kernel/shmmax
+        //    > increase shmmax value: echo "new_value" > /proc/sys/kernel/shmmax
+        // 2. not enough large page memory.
+        //    > check available large pages: cat /proc/meminfo
+        //    > increase amount of large pages:
+        //          sysctl -w vm.nr_hugepages=new_value
+        //    > For more information regarding large pages please refer to:
+        //      https://www.kernel.org/doc/Documentation/vm/hugetlbpage.txt
+        if (warn) {
+            warning("Large pages using UseSHM are not configured on this system.");
+        }
+        return false;
+    }
+    // Managed to create a segment, now delete it.
+    shmctl(shmid, IPC_RMID, NULL);
+    return true;
+}
+
+// From the coredump_filter documentation:
+//
+// - (bit 0) anonymous private memory
+// - (bit 1) anonymous shared memory
+// - (bit 2) file-backed private memory
+// - (bit 3) file-backed shared memory
+// - (bit 4) ELF header pages in file-backed private memory areas (it is
+//           effective only if the bit 2 is cleared)
+// - (bit 5) hugetlb private memory
+// - (bit 6) hugetlb shared memory
+// - (bit 7) dax private memory
+// - (bit 8) dax shared memory
+//
+static void set_coredump_filter(CoredumpFilterBit bit)
+{
+    FILE* f;
+    long cdm;
+
+    if ((f = fopen("/proc/self/coredump_filter", "r+")) == NULL) {
+        return;
+    }
+
+    if (fscanf(f, "%lx", &cdm) != 1) {
+        fclose(f);
+        return;
+    }
+
+    long saved_cdm = cdm;
+    rewind(f);
+    cdm |= bit;
+
+    if (cdm != saved_cdm) {
+        fprintf(f, "%#lx", cdm);
+    }
+
+    fclose(f);
+}
+
+// Large page support
+
+static size_t _large_page_size = 0;
+
+size_t os::Serenity::find_default_large_page_size()
+{
+    if (_default_large_page_size != 0) {
+        return _default_large_page_size;
+    }
+    size_t large_page_size = 0;
+
+    // large_page_size on Serenity is used to round up heap size. x86 uses either
+    // 2M or 4M page, depending on whether PAE (Physical Address Extensions)
+    // mode is enabled. AMD64/EM64T uses 2M page in 64bit mode. IA64 can use
+    // page as large as 256M.
+    //
+    // Here we try to figure out page size by parsing /proc/meminfo and looking
+    // for a line with the following format:
+    //    Hugepagesize:     2048 kB
+    //
+    // If we can't determine the value (e.g. /proc is not mounted, or the text
+    // format has been changed), we'll use the largest page size supported by
+    // the processor.
+
+#ifndef ZERO
+    large_page_size = AARCH64_ONLY(2 * M)
+        AMD64_ONLY(2 * M)
+            ARM32_ONLY(2 * M)
+                IA32_ONLY(4 * M)
+                    IA64_ONLY(256 * M)
+                        PPC_ONLY(4 * M)
+                            S390_ONLY(1 * M);
+#endif // ZERO
+
+    FILE* fp = fopen("/proc/meminfo", "r");
+    if (fp) {
+        while (!feof(fp)) {
+            int x = 0;
+            char buf[16];
+            if (fscanf(fp, "Hugepagesize: %d", &x) == 1) {
+                if (x && fgets(buf, sizeof(buf), fp) && strcmp(buf, " kB\n") == 0) {
+                    large_page_size = x * K;
+                    break;
+                }
+            } else {
+                // skip to next line
+                for (;;) {
+                    int ch = fgetc(fp);
+                    if (ch == EOF || ch == (int)'\n')
+                        break;
+                }
+            }
+        }
+        fclose(fp);
+    }
+    return large_page_size;
+}
+
+size_t os::Serenity::find_large_page_size(size_t large_page_size)
+{
+    if (_default_large_page_size == 0) {
+        _default_large_page_size = Serenity::find_default_large_page_size();
+    }
+    // We need to scan /sys/kernel/mm/hugepages
+    // to discover the available page sizes
+    const char* sys_hugepages = "/sys/kernel/mm/hugepages";
+
+    DIR* dir = opendir(sys_hugepages);
+    if (dir == NULL) {
+        return _default_large_page_size;
+    }
+
+    struct dirent* entry;
+    size_t page_size;
+    while ((entry = readdir(dir)) != NULL) {
+        if (entry->d_type == DT_DIR && sscanf(entry->d_name, "hugepages-%zukB", &page_size) == 1) {
+            // The kernel is using kB, hotspot uses bytes
+            if (large_page_size == page_size * K) {
+                closedir(dir);
+                return large_page_size;
+            }
+        }
+    }
+    closedir(dir);
+    return _default_large_page_size;
+}
+
+size_t os::Serenity::setup_large_page_size()
+{
+    _default_large_page_size = Serenity::find_default_large_page_size();
+
+    if (!FLAG_IS_DEFAULT(LargePageSizeInBytes) && LargePageSizeInBytes != _default_large_page_size) {
+        _large_page_size = find_large_page_size(LargePageSizeInBytes);
+        if (_large_page_size == _default_large_page_size) {
+            warning("Setting LargePageSizeInBytes=" SIZE_FORMAT " has no effect on this OS. Using the default large page size " SIZE_FORMAT "%s.",
+                LargePageSizeInBytes,
+                byte_size_in_proper_unit(_large_page_size), proper_unit_for_byte_size(_large_page_size));
+        }
+    } else {
+        _large_page_size = _default_large_page_size;
+    }
+
+    const size_t default_page_size = (size_t)Serenity::page_size();
+    if (_large_page_size > default_page_size) {
+        _page_sizes.add(_large_page_size);
+    }
+
+    return _large_page_size;
+}
+
+size_t os::Serenity::default_large_page_size()
+{
+    return _default_large_page_size;
+}
+
+bool os::Serenity::setup_large_page_type(size_t page_size)
+{
+    if (FLAG_IS_DEFAULT(UseHugeTLBFS) && FLAG_IS_DEFAULT(UseSHM) && FLAG_IS_DEFAULT(UseTransparentHugePages)) {
+
+        // The type of large pages has not been specified by the user.
+
+        // Try UseHugeTLBFS and then UseSHM.
+        UseHugeTLBFS = UseSHM = true;
+
+        // Don't try UseTransparentHugePages since there are known
+        // performance issues with it turned on. This might change in the future.
+        UseTransparentHugePages = false;
+    }
+
+    if (UseTransparentHugePages) {
+        bool warn_on_failure = !FLAG_IS_DEFAULT(UseTransparentHugePages);
+        if (transparent_huge_pages_sanity_check(warn_on_failure, page_size)) {
+            UseHugeTLBFS = false;
+            UseSHM = false;
+            return true;
+        }
+        UseTransparentHugePages = false;
+    }
+
+    if (UseHugeTLBFS) {
+        bool warn_on_failure = !FLAG_IS_DEFAULT(UseHugeTLBFS);
+        if (hugetlbfs_sanity_check(warn_on_failure, page_size)) {
+            UseSHM = false;
+            return true;
+        }
+        UseHugeTLBFS = false;
+    }
+
+    if (UseSHM) {
+        bool warn_on_failure = !FLAG_IS_DEFAULT(UseSHM);
+        if (shm_hugetlbfs_sanity_check(warn_on_failure, page_size)) {
+            return true;
+        }
+        UseSHM = false;
+    }
+
+    if (!FLAG_IS_DEFAULT(UseLargePages)) {
+        log_warning(pagesize)("UseLargePages disabled, no large pages configured and available on the system.");
+    }
+    return false;
+}
+
+void os::large_page_init()
+{
+    if (!UseLargePages && !UseTransparentHugePages && !UseHugeTLBFS && !UseSHM) {
+        // Not using large pages.
+        return;
+    }
+
+    if (!FLAG_IS_DEFAULT(UseLargePages) && !UseLargePages) {
+        // The user explicitly turned off large pages.
+        // Ignore the rest of the large pages flags.
+        UseTransparentHugePages = false;
+        UseHugeTLBFS = false;
+        UseSHM = false;
+        return;
+    }
+
+    size_t large_page_size = Serenity::setup_large_page_size();
+    UseLargePages = Serenity::setup_large_page_type(large_page_size);
+
+    set_coredump_filter(LARGEPAGES_BIT);
+}
+
+#ifndef SHM_HUGETLB
+#    define SHM_HUGETLB 04000
+#endif
+
+#define shm_warning_format(format, ...)                                                                                                 \
+    do {                                                                                                                                \
+        if (UseLargePages && (!FLAG_IS_DEFAULT(UseLargePages) || !FLAG_IS_DEFAULT(UseSHM) || !FLAG_IS_DEFAULT(LargePageSizeInBytes))) { \
+            warning(format, __VA_ARGS__);                                                                                               \
+        }                                                                                                                               \
+    } while (0)
+
+#define shm_warning(str) shm_warning_format("%s", str)
+
+#define shm_warning_with_errno(str)                   \
+    do {                                              \
+        int err = errno;                              \
+        shm_warning_format(str " (error = %d)", err); \
+    } while (0)
+
+static char* shmat_with_alignment(int shmid, size_t bytes, size_t alignment)
+{
+    assert(is_aligned(bytes, alignment), "Must be divisible by the alignment");
+
+    if (!is_aligned(alignment, SHMLBA)) {
+        assert(false, "Code below assumes that alignment is at least SHMLBA aligned");
+        return NULL;
+    }
+
+    // To ensure that we get 'alignment' aligned memory from shmat,
+    // we pre-reserve aligned virtual memory and then attach to that.
+
+    char* pre_reserved_addr = anon_mmap_aligned(NULL /* req_addr */, bytes, alignment);
+    if (pre_reserved_addr == NULL) {
+        // Couldn't pre-reserve aligned memory.
+        shm_warning("Failed to pre-reserve aligned memory for shmat.");
+        return NULL;
+    }
+
+    // SHM_REMAP is needed to allow shmat to map over an existing mapping.
+    char* addr = (char*)shmat(shmid, pre_reserved_addr, SHM_REMAP);
+
+    if ((intptr_t)addr == -1) {
+        int err = errno;
+        shm_warning_with_errno("Failed to attach shared memory.");
+
+        assert(err != EACCES, "Unexpected error");
+        assert(err != EIDRM, "Unexpected error");
+        assert(err != EINVAL, "Unexpected error");
+
+        // Since we don't know if the kernel unmapped the pre-reserved memory area
+        // we can't unmap it, since that would potentially unmap memory that was
+        // mapped from other threads.
+        return NULL;
+    }
+
+    return addr;
+}
+
+static char* shmat_at_address(int shmid, char* req_addr)
+{
+    if (!is_aligned(req_addr, SHMLBA)) {
+        assert(false, "Requested address needs to be SHMLBA aligned");
+        return NULL;
+    }
+
+    char* addr = (char*)shmat(shmid, req_addr, 0);
+
+    if ((intptr_t)addr == -1) {
+        shm_warning_with_errno("Failed to attach shared memory.");
+        return NULL;
+    }
+
+    return addr;
+}
+
+static char* shmat_large_pages(int shmid, size_t bytes, size_t alignment, char* req_addr)
+{
+    // If a req_addr has been provided, we assume that the caller has already aligned the address.
+    if (req_addr != NULL) {
+        assert(is_aligned(req_addr, os::large_page_size()), "Must be divisible by the large page size");
+        assert(is_aligned(req_addr, alignment), "Must be divisible by given alignment");
+        return shmat_at_address(shmid, req_addr);
+    }
+
+    // Since shmid has been setup with SHM_HUGETLB, shmat will automatically
+    // return large page size aligned memory addresses when req_addr == NULL.
+    // However, if the alignment is larger than the large page size, we have
+    // to manually ensure that the memory returned is 'alignment' aligned.
+    if (alignment > os::large_page_size()) {
+        assert(is_aligned(alignment, os::large_page_size()), "Must be divisible by the large page size");
+        return shmat_with_alignment(shmid, bytes, alignment);
+    } else {
+        return shmat_at_address(shmid, NULL);
+    }
+}
+
+char* os::Serenity::reserve_memory_special_shm(size_t bytes, size_t alignment,
+    char* req_addr, bool exec)
+{
+    // "exec" is passed in but not used.  Creating the shared image for
+    // the code cache doesn't have an SHM_X executable permission to check.
+    assert(UseLargePages && UseSHM, "only for SHM large pages");
+    assert(is_aligned(req_addr, os::large_page_size()), "Unaligned address");
+    assert(is_aligned(req_addr, alignment), "Unaligned address");
+
+    if (!is_aligned(bytes, os::large_page_size())) {
+        return NULL; // Fallback to small pages.
+    }
+
+    // Create a large shared memory region to attach to based on size.
+    // Currently, size is the total size of the heap.
+    int shmid = shmget(IPC_PRIVATE, bytes, SHM_HUGETLB | IPC_CREAT | SHM_R | SHM_W);
+    if (shmid == -1) {
+        // Possible reasons for shmget failure:
+        // 1. shmmax is too small for the request.
+        //    > check shmmax value: cat /proc/sys/kernel/shmmax
+        //    > increase shmmax value: echo "new_value" > /proc/sys/kernel/shmmax
+        // 2. not enough large page memory.
+        //    > check available large pages: cat /proc/meminfo
+        //    > increase amount of large pages:
+        //          sysctl -w vm.nr_hugepages=new_value
+        //    > For more information regarding large pages please refer to:
+        //      https://www.kernel.org/doc/Documentation/vm/hugetlbpage.txt
+        //      Note 1: different Serenity may use different name for this property,
+        //            e.g. on Redhat AS-3 it is "hugetlb_pool".
+        //      Note 2: it's possible there's enough physical memory available but
+        //            they are so fragmented after a long run that they can't
+        //            coalesce into large pages. Try to reserve large pages when
+        //            the system is still "fresh".
+        shm_warning_with_errno("Failed to reserve shared memory.");
+        return NULL;
+    }
+
+    // Attach to the region.
+    char* addr = shmat_large_pages(shmid, bytes, alignment, req_addr);
+
+    // Remove shmid. If shmat() is successful, the actual shared memory segment
+    // will be deleted when it's detached by shmdt() or when the process
+    // terminates. If shmat() is not successful this will remove the shared
+    // segment immediately.
+    shmctl(shmid, IPC_RMID, NULL);
+
+    return addr;
+}
+
+static void warn_on_large_pages_failure(char* req_addr, size_t bytes,
+    int error)
+{
+    assert(error == ENOMEM, "Only expect to fail if no memory is available");
+
+    bool warn_on_failure = UseLargePages && (!FLAG_IS_DEFAULT(UseLargePages) || !FLAG_IS_DEFAULT(UseHugeTLBFS) || !FLAG_IS_DEFAULT(LargePageSizeInBytes));
+
+    if (warn_on_failure) {
+        char msg[128];
+        jio_snprintf(msg, sizeof(msg), "Failed to reserve large pages memory req_addr: " PTR_FORMAT " bytes: " SIZE_FORMAT " (errno = %d).", req_addr, bytes, error);
+        warning("%s", msg);
+    }
+}
+
+char* os::Serenity::reserve_memory_special_huge_tlbfs_only(size_t bytes,
+    char* req_addr,
+    bool exec)
+{
+    assert(UseLargePages && UseHugeTLBFS, "only for Huge TLBFS large pages");
+    assert(is_aligned(bytes, os::large_page_size()), "Unaligned size");
+    assert(is_aligned(req_addr, os::large_page_size()), "Unaligned address");
+
+    int prot = exec ? PROT_READ | PROT_WRITE | PROT_EXEC : PROT_READ | PROT_WRITE;
+    int flags = MAP_PRIVATE | MAP_ANONYMOUS | MAP_HUGETLB;
+    // Ensure the correct page size flag is used when needed.
+    flags |= hugetlbfs_page_size_flag(os::large_page_size());
+
+    char* addr = (char*)::mmap(req_addr, bytes, prot, flags, -1, 0);
+
+    if (addr == MAP_FAILED) {
+        warn_on_large_pages_failure(req_addr, bytes, errno);
+        return NULL;
+    }
+
+    assert(is_aligned(addr, os::large_page_size()), "Must be");
+
+    return addr;
+}
+
+// Reserve memory using mmap(MAP_HUGETLB).
+//  - bytes shall be a multiple of alignment.
+//  - req_addr can be NULL. If not NULL, it must be a multiple of alignment.
+//  - alignment sets the alignment at which memory shall be allocated.
+//     It must be a multiple of allocation granularity.
+// Returns address of memory or NULL. If req_addr was not NULL, will only return
+//  req_addr or NULL.
+char* os::Serenity::reserve_memory_special_huge_tlbfs_mixed(size_t bytes,
+    size_t alignment,
+    char* req_addr,
+    bool exec)
+{
+    size_t large_page_size = os::large_page_size();
+    assert(bytes >= large_page_size, "Shouldn't allocate large pages for small sizes");
+
+    assert(is_aligned(req_addr, alignment), "Must be");
+    assert(is_aligned(bytes, alignment), "Must be");
+
+    // First reserve - but not commit - the address range in small pages.
+    char* const start = anon_mmap_aligned(req_addr, bytes, alignment);
+
+    if (start == NULL) {
+        return NULL;
+    }
+
+    assert(is_aligned(start, alignment), "Must be");
+
+    char* end = start + bytes;
+
+    // Find the regions of the allocated chunk that can be promoted to large pages.
+    char* lp_start = align_up(start, large_page_size);
+    char* lp_end = align_down(end, large_page_size);
+
+    size_t lp_bytes = lp_end - lp_start;
+
+    assert(is_aligned(lp_bytes, large_page_size), "Must be");
+
+    if (lp_bytes == 0) {
+        // The mapped region doesn't even span the start and the end of a large page.
+        // Fall back to allocate a non-special area.
+        ::munmap(start, end - start);
+        return NULL;
+    }
+
+    int prot = exec ? PROT_READ | PROT_WRITE | PROT_EXEC : PROT_READ | PROT_WRITE;
+    int flags = MAP_PRIVATE | MAP_ANONYMOUS | MAP_FIXED;
+    void* result;
+
+    // Commit small-paged leading area.
+    if (start != lp_start) {
+        result = ::mmap(start, lp_start - start, prot, flags, -1, 0);
+        if (result == MAP_FAILED) {
+            ::munmap(lp_start, end - lp_start);
+            return NULL;
+        }
+    }
+
+    // Commit large-paged area.
+    flags |= MAP_HUGETLB | hugetlbfs_page_size_flag(os::large_page_size());
+
+    result = ::mmap(lp_start, lp_bytes, prot, flags, -1, 0);
+    if (result == MAP_FAILED) {
+        warn_on_large_pages_failure(lp_start, lp_bytes, errno);
+        // If the mmap above fails, the large pages region will be unmapped and we
+        // have regions before and after with small pages. Release these regions.
+        //
+        // |  mapped  |  unmapped  |  mapped  |
+        // ^          ^            ^          ^
+        // start      lp_start     lp_end     end
+        //
+        ::munmap(start, lp_start - start);
+        ::munmap(lp_end, end - lp_end);
+        return NULL;
+    }
+
+    // Commit small-paged trailing area.
+    if (lp_end != end) {
+        result = ::mmap(lp_end, end - lp_end, prot,
+            MAP_PRIVATE | MAP_ANONYMOUS | MAP_FIXED,
+            -1, 0);
+        if (result == MAP_FAILED) {
+            ::munmap(start, lp_end - start);
+            return NULL;
+        }
+    }
+
+    return start;
+}
+
+char* os::Serenity::reserve_memory_special_huge_tlbfs(size_t bytes,
+    size_t alignment,
+    char* req_addr,
+    bool exec)
+{
+    assert(UseLargePages && UseHugeTLBFS, "only for Huge TLBFS large pages");
+    assert(is_aligned(req_addr, alignment), "Must be");
+    assert(is_aligned(alignment, os::vm_allocation_granularity()), "Must be");
+    assert(is_power_of_2(os::large_page_size()), "Must be");
+    assert(bytes >= os::large_page_size(), "Shouldn't allocate large pages for small sizes");
+
+    if (is_aligned(bytes, os::large_page_size()) && alignment <= os::large_page_size()) {
+        return reserve_memory_special_huge_tlbfs_only(bytes, req_addr, exec);
+    } else {
+        return reserve_memory_special_huge_tlbfs_mixed(bytes, alignment, req_addr, exec);
+    }
+}
+
+char* os::pd_reserve_memory_special(size_t bytes, size_t alignment,
+    char* req_addr, bool exec)
+{
+    assert(UseLargePages, "only for large pages");
+
+    char* addr;
+    if (UseSHM) {
+        addr = os::Serenity::reserve_memory_special_shm(bytes, alignment, req_addr, exec);
+    } else {
+        assert(UseHugeTLBFS, "must be");
+        addr = os::Serenity::reserve_memory_special_huge_tlbfs(bytes, alignment, req_addr, exec);
+    }
+
+    if (addr != NULL) {
+        if (UseNUMAInterleaving) {
+            numa_make_global(addr, bytes);
+        }
+    }
+
+    return addr;
+}
+
+bool os::Serenity::release_memory_special_shm(char* base, size_t bytes)
+{
+    // detaching the SHM segment will also delete it, see reserve_memory_special_shm()
+    return shmdt(base) == 0;
+}
+
+bool os::Serenity::release_memory_special_huge_tlbfs(char* base, size_t bytes)
+{
+    return pd_release_memory(base, bytes);
+}
+
+bool os::pd_release_memory_special(char* base, size_t bytes)
+{
+    assert(UseLargePages, "only for large pages");
+    bool res;
+
+    if (UseSHM) {
+        res = os::Serenity::release_memory_special_shm(base, bytes);
+    } else {
+        assert(UseHugeTLBFS, "must be");
+        res = os::Serenity::release_memory_special_huge_tlbfs(base, bytes);
+    }
+    return res;
+}
+
+size_t os::large_page_size()
+{
+    return _large_page_size;
+}
+
+// With SysV SHM the entire memory region must be allocated as shared
+// memory.
+// HugeTLBFS allows application to commit large page memory on demand.
+// However, when committing memory with HugeTLBFS fails, the region
+// that was supposed to be committed will lose the old reservation
+// and allow other threads to steal that memory region. Because of this
+// behavior we can't commit HugeTLBFS memory.
+bool os::can_commit_large_page_memory()
+{
+    return UseTransparentHugePages;
+}
+
+bool os::can_execute_large_page_memory()
+{
+    return UseTransparentHugePages || UseHugeTLBFS;
+}
+
+char* os::pd_attempt_map_memory_to_file_at(char* requested_addr, size_t bytes, int file_desc)
+{
+    assert(file_desc >= 0, "file_desc is not valid");
+    char* result = pd_attempt_reserve_memory_at(requested_addr, bytes, !ExecMem);
+    if (result != NULL) {
+        if (replace_existing_mapping_with_file_mapping(result, bytes, file_desc) == NULL) {
+            vm_exit_during_initialization(err_msg("Error in mapping Java heap at the given filesystem directory"));
+        }
+    }
+    return result;
+}
+
+// Reserve memory at an arbitrary address, only if that area is
+// available (and not reserved for something else).
+
+char* os::pd_attempt_reserve_memory_at(char* requested_addr, size_t bytes, bool exec)
+{
+    // Assert only that the size is a multiple of the page size, since
+    // that's all that mmap requires, and since that's all we really know
+    // about at this low abstraction level.  If we need higher alignment,
+    // we can either pass an alignment to this method or verify alignment
+    // in one of the methods further up the call chain.  See bug 5044738.
+    assert(bytes % os::vm_page_size() == 0, "reserving unexpected size block");
+
+    // Repeatedly allocate blocks until the block is allocated at the
+    // right spot.
+
+    // Serenity mmap allows caller to pass an address as hint; give it a try first,
+    // if kernel honors the hint then we can return immediately.
+    char* addr = anon_mmap(requested_addr, bytes);
+    if (addr == requested_addr) {
+        return requested_addr;
+    }
+
+    if (addr != NULL) {
+        // mmap() is successful but it fails to reserve at the requested address
+        anon_munmap(addr, bytes);
+    }
+
+    return NULL;
+}
+
+// Sleep forever; naked call to OS-specific sleep; use with CAUTION
+void os::infinite_sleep()
+{
+    while (true) {    // sleep forever ...
+        ::sleep(100); // ... 100 seconds at a time
+    }
+}
+
+// Used to convert frequent JVM_Yield() to nops
+bool os::dont_yield()
+{
+    return DontYieldALot;
+}
+
+// Serenity CFS scheduler (since 2.6.23) does not guarantee sched_yield(2) will
+// actually give up the CPU. Since skip buddy (v2.6.28):
+//
+// * Sets the yielding task as skip buddy for current CPU's run queue.
+// * Picks next from run queue, if empty, picks a skip buddy (can be the yielding task).
+// * Clears skip buddies for this run queue (yielding task no longer a skip buddy).
+//
+// An alternative is calling os::naked_short_nanosleep with a small number to avoid
+// getting re-scheduled immediately.
+//
+void os::naked_yield()
+{
+    sched_yield();
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// thread priority support
+
+// Note: Normal Serenity applications are run with SCHED_OTHER policy. SCHED_OTHER
+// only supports dynamic priority, static priority must be zero. For real-time
+// applications, Serenity supports SCHED_RR which allows static priority (1-99).
+// However, for large multi-threaded applications, SCHED_RR is not only slower
+// than SCHED_OTHER, but also very unstable (my volano tests hang hard 4 out
+// of 5 runs - Sep 2005).
+//
+// The following code actually changes the niceness of kernel-thread/LWP. It
+// has an assumption that setpriority() only modifies one kernel-thread/LWP,
+// not the entire user process, and user level threads are 1:1 mapped to kernel
+// threads. It has always been the case, but could change in the future. For
+// this reason, the code should not be used as default (ThreadPriorityPolicy=0).
+// It is only used when ThreadPriorityPolicy=1 and may require system level permission
+// (e.g., root privilege or CAP_SYS_NICE capability).
+
+int os::java_to_os_priority[CriticalPriority + 1] = {
+    19, // 0 Entry should never be used
+
+    4, // 1 MinPriority
+    3, // 2
+    2, // 3
+
+    1,  // 4
+    0,  // 5 NormPriority
+    -1, // 6
+
+    -2, // 7
+    -3, // 8
+    -4, // 9 NearMaxPriority
+
+    -5, // 10 MaxPriority
+
+    -5 // 11 CriticalPriority
+};
+
+static int prio_init()
+{
+    if (ThreadPriorityPolicy == 1) {
+        if (geteuid() != 0) {
+            if (!FLAG_IS_DEFAULT(ThreadPriorityPolicy) && !FLAG_IS_JIMAGE_RESOURCE(ThreadPriorityPolicy)) {
+                warning("-XX:ThreadPriorityPolicy=1 may require system level permission, "
+                        "e.g., being the root user. If the necessary permission is not "
+                        "possessed, changes to priority will be silently ignored.");
+            }
+        }
+    }
+    if (UseCriticalJavaThreadPriority) {
+        os::java_to_os_priority[MaxPriority] = os::java_to_os_priority[CriticalPriority];
+    }
+    return 0;
+}
+
+OSReturn os::set_native_priority(Thread* thread, int newpri)
+{
+    if (!UseThreadPriorities || ThreadPriorityPolicy == 0)
+        return OS_OK;
+
+    int ret = setpriority(PRIO_PROCESS, thread->osthread()->thread_id(), newpri);
+    return (ret == 0) ? OS_OK : OS_ERR;
+}
+
+OSReturn os::get_native_priority(const Thread* const thread,
+    int* priority_ptr)
+{
+    if (!UseThreadPriorities || ThreadPriorityPolicy == 0) {
+        *priority_ptr = java_to_os_priority[NormPriority];
+        return OS_OK;
+    }
+
+    errno = 0;
+    *priority_ptr = getpriority(PRIO_PROCESS, thread->osthread()->thread_id());
+    return (*priority_ptr != -1 || errno == 0 ? OS_OK : OS_ERR);
+}
+
+// This is the fastest way to get thread cpu time on Serenity.
+// Returns cpu time (user+sys) for any thread, not only for current.
+// POSIX compliant clocks are implemented in the kernels 2.6.16+.
+// It might work on 2.6.10+ with a special kernel/glibc patch.
+// For reference, please, see IEEE Std 1003.1-2004:
+//   http://www.unix.org/single_unix_specification
+
+jlong os::Serenity::fast_thread_cpu_time(clockid_t clockid)
+{
+    struct timespec tp;
+    int status = clock_gettime(clockid, &tp);
+    assert(status == 0, "clock_gettime error: %s", os::strerror(errno));
+    return (tp.tv_sec * NANOSECS_PER_SEC) + tp.tv_nsec;
+}
+
+// Determine if the vmid is the parent pid for a child in a PID namespace.
+// Return the namespace pid if so, otherwise -1.
+int os::Serenity::get_namespace_pid(int vmid)
+{
+    char fname[24];
+    int retpid = -1;
+
+    snprintf(fname, sizeof(fname), "/proc/%d/status", vmid);
+    FILE* fp = fopen(fname, "r");
+
+    if (fp) {
+        int pid, nspid;
+        int ret;
+        while (!feof(fp) && !ferror(fp)) {
+            ret = fscanf(fp, "NSpid: %d %d", &pid, &nspid);
+            if (ret == 1) {
+                break;
+            }
+            if (ret == 2) {
+                retpid = nspid;
+                break;
+            }
+            for (;;) {
+                int ch = fgetc(fp);
+                if (ch == EOF || ch == (int)'\n')
+                    break;
+            }
+        }
+        fclose(fp);
+    }
+    return retpid;
+}
+
+extern void report_error(char* file_name, int line_no, char* title,
+    char* format, ...);
+
+// Some linux distributions (notably: Alpine Serenity) include the
+// grsecurity in the kernel. Of particular interest from a JVM perspective
+// is PaX (https://pax.grsecurity.net/), which adds some security features
+// related to page attributes. Specifically, the MPROTECT PaX functionality
+// (https://pax.grsecurity.net/docs/mprotect.txt) prevents dynamic
+// code generation by disallowing a (previously) writable page to be
+// marked as executable. This is, of course, exactly what HotSpot does
+// for both JIT compiled method, as well as for stubs, adapters, etc.
+//
+// Instead of crashing "lazily" when trying to make a page executable,
+// this code probes for the presence of PaX and reports the failure
+// eagerly.
+static void check_pax(void)
+{
+    // Zero doesn't generate code dynamically, so no need to perform the PaX check
+#ifndef ZERO
+    size_t size = os::Serenity::page_size();
+
+    void* p = ::mmap(NULL, size, PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);
+    if (p == MAP_FAILED) {
+        log_debug(os)("os_linux.cpp: check_pax: mmap failed (%s)", os::strerror(errno));
+        vm_exit_out_of_memory(size, OOM_MMAP_ERROR, "failed to allocate memory for PaX check.");
+    }
+
+    int res = ::mprotect(p, size, PROT_WRITE | PROT_EXEC);
+    if (res == -1) {
+        log_debug(os)("os_linux.cpp: check_pax: mprotect failed (%s)", os::strerror(errno));
+        vm_exit_during_initialization(
+            "Failed to mark memory page as executable - check if grsecurity/PaX is enabled");
+    }
+
+    ::munmap(p, size);
+#endif
+}
+
+// this is called _before_ most of the global arguments have been parsed
+void os::init(void)
+{
+    char dummy; // used to get a guess on initial stack address
+
+    clock_tics_per_sec = sysconf(_SC_CLK_TCK);
+
+    Serenity::set_page_size(sysconf(_SC_PAGESIZE));
+    if (Serenity::page_size() == -1) {
+        fatal("os_linux.cpp: os::init: sysconf failed (%s)",
+            os::strerror(errno));
+    }
+    _page_sizes.add(Serenity::page_size());
+
+    Serenity::initialize_system_info();
+
+#ifdef __GLIBC__
+    Serenity::_mallinfo = CAST_TO_FN_PTR(Serenity::mallinfo_func_t, dlsym(RTLD_DEFAULT, "mallinfo"));
+    Serenity::_mallinfo2 = CAST_TO_FN_PTR(Serenity::mallinfo2_func_t, dlsym(RTLD_DEFAULT, "mallinfo2"));
+#endif // __GLIBC__
+
+    os::Serenity::CPUPerfTicks pticks;
+    bool res = os::Serenity::get_tick_information(&pticks, -1);
+
+    if (res && pticks.has_steal_ticks) {
+        has_initial_tick_info = true;
+        initial_total_ticks = pticks.total;
+        initial_steal_ticks = pticks.steal;
+    }
+
+    // _main_thread points to the thread that created/loaded the JVM.
+    Serenity::_main_thread = pthread_self();
+
+    // retrieve entry point for pthread_setname_np
+    Serenity::_pthread_setname_np = (int (*)(pthread_t, const char*))dlsym(RTLD_DEFAULT, "pthread_setname_np");
+
+    check_pax();
+
+    os::Posix::init();
+
+    initial_time_count = javaTimeNanos();
+}
+
+// To install functions for atexit system call
+extern "C" {
+static void perfMemory_exit_helper()
+{
+    perfMemory_exit();
+}
+}
+
+void os::pd_init_container_support()
+{
+    OSContainer::init();
+}
+
+// this is called _after_ the global arguments have been parsed
+jint os::init_2(void)
+{
+
+    // This could be set after os::Posix::init() but all platforms
+    // have to set it the same so we have to mirror Solaris.
+    DEBUG_ONLY(os::set_mutex_init_done();)
+
+    os::Posix::init_2();
+
+    Serenity::fast_thread_clock_init();
+
+    if (PosixSignals::init() == JNI_ERR) {
+        return JNI_ERR;
+    }
+
+    if (AdjustStackSizeForTLS) {
+        get_minstack_init();
+    }
+
+    // Check and sets minimum stack sizes against command line options
+    if (Posix::set_minimum_stack_sizes() == JNI_ERR) {
+        return JNI_ERR;
+    }
+
+#if defined(IA32) && !defined(ZERO)
+    // Need to ensure we've determined the process's initial stack to
+    // perform the workaround
+    Serenity::capture_initial_stack(JavaThread::stack_size_at_create());
+    workaround_expand_exec_shield_cs_limit();
+#else
+    suppress_primordial_thread_resolution = Arguments::created_by_java_launcher();
+    if (!suppress_primordial_thread_resolution) {
+        Serenity::capture_initial_stack(JavaThread::stack_size_at_create());
+    }
+#endif
+
+    Serenity::libpthread_init();
+    Serenity::sched_getcpu_init();
+    log_info(os)("HotSpot is running with %s, %s",
+        Serenity::libc_version(), Serenity::libpthread_version());
+
+    if (MaxFDLimit) {
+        // set the number of file descriptors to max. print out error
+        // if getrlimit/setrlimit fails but continue regardless.
+        struct rlimit nbr_files;
+        int status = getrlimit(RLIMIT_NOFILE, &nbr_files);
+        if (status != 0) {
+            log_info(os)("os::init_2 getrlimit failed: %s", os::strerror(errno));
+        } else {
+            nbr_files.rlim_cur = nbr_files.rlim_max;
+            status = setrlimit(RLIMIT_NOFILE, &nbr_files);
+            if (status != 0) {
+                log_info(os)("os::init_2 setrlimit failed: %s", os::strerror(errno));
+            }
+        }
+    }
+
+    // at-exit methods are called in the reverse order of their registration.
+    // atexit functions are called on return from main or as a result of a
+    // call to exit(3C). There can be only 32 of these functions registered
+    // and atexit() does not set errno.
+
+    if (PerfAllowAtExitRegistration) {
+        // only register atexit functions if PerfAllowAtExitRegistration is set.
+        // atexit functions can be delayed until process exit time, which
+        // can be problematic for embedded VM situations. Embedded VMs should
+        // call DestroyJavaVM() to assure that VM resources are released.
+
+        // note: perfMemory_exit_helper atexit function may be removed in
+        // the future if the appropriate cleanup code can be added to the
+        // VM_Exit VMOperation's doit method.
+        if (atexit(perfMemory_exit_helper) != 0) {
+            warning("os::init_2 atexit(perfMemory_exit_helper) failed");
+        }
+    }
+
+    // initialize thread priority policy
+    prio_init();
+
+    if (!FLAG_IS_DEFAULT(AllocateHeapAt)) {
+        set_coredump_filter(DAX_SHARED_BIT);
+    }
+
+    if (DumpPrivateMappingsInCore) {
+        set_coredump_filter(FILE_BACKED_PVT_BIT);
+    }
+
+    if (DumpSharedMappingsInCore) {
+        set_coredump_filter(FILE_BACKED_SHARED_BIT);
+    }
+
+    if (DumpPerfMapAtExit && FLAG_IS_DEFAULT(UseCodeCacheFlushing)) {
+        // Disable code cache flushing to ensure the map file written at
+        // exit contains all nmethods generated during execution.
+        FLAG_SET_DEFAULT(UseCodeCacheFlushing, false);
+    }
+
+    return JNI_OK;
+}
+
+// older glibc versions don't have this macro (which expands to
+// an optimized bit-counting function) so we have to roll our own
+#ifndef CPU_COUNT
+
+static int _cpu_count(const cpu_set_t* cpus)
+{
+    int count = 0;
+    // only look up to the number of configured processors
+    for (int i = 0; i < os::processor_count(); i++) {
+        if (CPU_ISSET(i, cpus)) {
+            count++;
+        }
+    }
+    return count;
+}
+
+#    define CPU_COUNT(cpus) _cpu_count(cpus)
+
+#endif // CPU_COUNT
+
+// Get the current number of available processors for this process.
+// This value can change at any time during a process's lifetime.
+// sched_getaffinity gives an accurate answer as it accounts for cpusets.
+// If it appears there may be more than 1024 processors then we do a
+// dynamic check - see 6515172 for details.
+// If anything goes wrong we fallback to returning the number of online
+// processors - which can be greater than the number available to the process.
+int os::Serenity::active_processor_count()
+{
+    cpu_set_t cpus; // can represent at most 1024 (CPU_SETSIZE) processors
+    cpu_set_t* cpus_p = &cpus;
+    int cpus_size = sizeof(cpu_set_t);
+
+    int configured_cpus = os::processor_count(); // upper bound on available cpus
+    int cpu_count = 0;
+
+// old build platforms may not support dynamic cpu sets
+#ifdef CPU_ALLOC
+
+    // To enable easy testing of the dynamic path on different platforms we
+    // introduce a diagnostic flag: UseCpuAllocPath
+    if (configured_cpus >= CPU_SETSIZE || UseCpuAllocPath) {
+        // kernel may use a mask bigger than cpu_set_t
+        log_trace(os)("active_processor_count: using dynamic path %s"
+                      "- configured processors: %d",
+            UseCpuAllocPath ? "(forced) " : "",
+            configured_cpus);
+        cpus_p = CPU_ALLOC(configured_cpus);
+        if (cpus_p != NULL) {
+            cpus_size = CPU_ALLOC_SIZE(configured_cpus);
+            // zero it just to be safe
+            CPU_ZERO_S(cpus_size, cpus_p);
+        } else {
+            // failed to allocate so fallback to online cpus
+            int online_cpus = ::sysconf(_SC_NPROCESSORS_ONLN);
+            log_trace(os)("active_processor_count: "
+                          "CPU_ALLOC failed (%s) - using "
+                          "online processor count: %d",
+                os::strerror(errno), online_cpus);
+            return online_cpus;
+        }
+    } else {
+        log_trace(os)("active_processor_count: using static path - configured processors: %d",
+            configured_cpus);
+    }
+#else // CPU_ALLOC
+// these stubs won't be executed
+#    define CPU_COUNT_S(size, cpus) -1
+#    define CPU_FREE(cpus)
+
+    log_trace(os)("active_processor_count: only static path available - configured processors: %d",
+        configured_cpus);
+#endif // CPU_ALLOC
+
+    // pid 0 means the current thread - which we have to assume represents the process
+    if (sched_getaffinity(0, cpus_size, cpus_p) == 0) {
+        if (cpus_p != &cpus) { // can only be true when CPU_ALLOC used
+            cpu_count = CPU_COUNT_S(cpus_size, cpus_p);
+        } else {
+            cpu_count = CPU_COUNT(cpus_p);
+        }
+        log_trace(os)("active_processor_count: sched_getaffinity processor count: %d", cpu_count);
+    } else {
+        cpu_count = ::sysconf(_SC_NPROCESSORS_ONLN);
+        warning("sched_getaffinity failed (%s)- using online processor count (%d) "
+                "which may exceed available processors",
+            os::strerror(errno), cpu_count);
+    }
+
+    if (cpus_p != &cpus) { // can only be true when CPU_ALLOC used
+        CPU_FREE(cpus_p);
+    }
+
+    assert(cpu_count > 0 && cpu_count <= os::processor_count(), "sanity check");
+    return cpu_count;
+}
+
+// Determine the active processor count from one of
+// three different sources:
+//
+// 1. User option -XX:ActiveProcessorCount
+// 2. kernel os calls (sched_getaffinity or sysconf(_SC_NPROCESSORS_ONLN)
+// 3. extracted from cgroup cpu subsystem (shares and quotas)
+//
+// Option 1, if specified, will always override.
+// If the cgroup subsystem is active and configured, we
+// will return the min of the cgroup and option 2 results.
+// This is required since tools, such as numactl, that
+// alter cpu affinity do not update cgroup subsystem
+// cpuset configuration files.
+int os::active_processor_count()
+{
+    // User has overridden the number of active processors
+    if (ActiveProcessorCount > 0) {
+        log_trace(os)("active_processor_count: "
+                      "active processor count set by user : %d",
+            ActiveProcessorCount);
+        return ActiveProcessorCount;
+    }
+
+    int active_cpus;
+    if (OSContainer::is_containerized()) {
+        active_cpus = OSContainer::active_processor_count();
+        log_trace(os)("active_processor_count: determined by OSContainer: %d",
+            active_cpus);
+    } else {
+        active_cpus = os::Serenity::active_processor_count();
+    }
+
+    return active_cpus;
+}
+
+static bool should_warn_invalid_processor_id()
+{
+    if (os::processor_count() == 1) {
+        // Don't warn if we only have one processor
+        return false;
+    }
+
+    static volatile int warn_once = 1;
+
+    if (Atomic::load(&warn_once) == 0 || Atomic::xchg(&warn_once, 0) == 0) {
+        // Don't warn more than once
+        return false;
+    }
+
+    return true;
+}
+
+uint os::processor_id()
+{
+    const int id = Serenity::sched_getcpu();
+
+    if (id < processor_count()) {
+        return (uint)id;
+    }
+
+    // Some environments (e.g. openvz containers and the rr debugger) incorrectly
+    // report a processor id that is higher than the number of processors available.
+    // This is problematic, for example, when implementing CPU-local data structures,
+    // where the processor id is used to index into an array of length processor_count().
+    // If this happens we return 0 here. This is is safe since we always have at least
+    // one processor, but it's not optimal for performance if we're actually executing
+    // in an environment with more than one processor.
+    if (should_warn_invalid_processor_id()) {
+        log_warning(os)("Invalid processor id reported by the operating system "
+                        "(got processor id %d, valid processor id range is 0-%d)",
+            id, processor_count() - 1);
+        log_warning(os)("Falling back to assuming processor id is 0. "
+                        "This could have a negative impact on performance.");
+    }
+
+    return 0;
+}
+
+void os::set_native_thread_name(const char* name)
+{
+    if (Serenity::_pthread_setname_np) {
+        char buf[16]; // according to glibc manpage, 16 chars incl. '/0'
+        snprintf(buf, sizeof(buf), "%s", name);
+        buf[sizeof(buf) - 1] = '\0';
+        const int rc = Serenity::_pthread_setname_np(pthread_self(), buf);
+        // ERANGE should not happen; all other errors should just be ignored.
+        assert(rc != ERANGE, "pthread_setname_np failed");
+    }
+}
+
+bool os::bind_to_processor(uint processor_id)
+{
+    // Not yet implemented.
+    return false;
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// debug support
+
+bool os::find(address addr, outputStream* st)
+{
+    Dl_info dlinfo;
+    memset(&dlinfo, 0, sizeof(dlinfo));
+    if (dladdr(addr, &dlinfo) != 0) {
+        st->print(PTR_FORMAT ": ", p2i(addr));
+        if (dlinfo.dli_sname != NULL && dlinfo.dli_saddr != NULL) {
+            st->print("%s+" PTR_FORMAT, dlinfo.dli_sname,
+                p2i(addr) - p2i(dlinfo.dli_saddr));
+        } else if (dlinfo.dli_fbase != NULL) {
+            st->print("<offset " PTR_FORMAT ">", p2i(addr) - p2i(dlinfo.dli_fbase));
+        } else {
+            st->print("<absolute address>");
+        }
+        if (dlinfo.dli_fname != NULL) {
+            st->print(" in %s", dlinfo.dli_fname);
+        }
+        if (dlinfo.dli_fbase != NULL) {
+            st->print(" at " PTR_FORMAT, p2i(dlinfo.dli_fbase));
+        }
+        st->cr();
+
+        if (Verbose) {
+            // decode some bytes around the PC
+            address begin = clamp_address_in_page(addr - 40, addr, os::vm_page_size());
+            address end = clamp_address_in_page(addr + 40, addr, os::vm_page_size());
+            address lowest = (address)dlinfo.dli_sname;
+            if (!lowest)
+                lowest = (address)dlinfo.dli_fbase;
+            if (begin < lowest)
+                begin = lowest;
+            Dl_info dlinfo2;
+            if (dladdr(end, &dlinfo2) != 0 && dlinfo2.dli_saddr != dlinfo.dli_saddr
+                && end > dlinfo2.dli_saddr && dlinfo2.dli_saddr > begin) {
+                end = (address)dlinfo2.dli_saddr;
+            }
+            Disassembler::decode(begin, end, st);
+        }
+        return true;
+    }
+    return false;
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// misc
+
+// This does not do anything on Serenity. This is basically a hook for being
+// able to use structured exception handling (thread-local exception filters)
+// on, e.g., Win32.
+void os::os_exception_wrapper(java_call_t f, JavaValue* value, const methodHandle& method,
+    JavaCallArguments* args, Thread* thread)
+{
+    f(value, method, args, thread);
+}
+
+void os::print_statistics()
+{
+}
+
+bool os::message_box(const char* title, const char* message)
+{
+    int i;
+    fdStream err(defaultStream::error_fd());
+    for (i = 0; i < 78; i++)
+        err.print_raw("=");
+    err.cr();
+    err.print_raw_cr(title);
+    for (i = 0; i < 78; i++)
+        err.print_raw("-");
+    err.cr();
+    err.print_raw_cr(message);
+    for (i = 0; i < 78; i++)
+        err.print_raw("=");
+    err.cr();
+
+    char buf[16];
+    // Prevent process from exiting upon "read error" without consuming all CPU
+    while (::read(0, buf, sizeof(buf)) <= 0) {
+        ::sleep(100);
+    }
+
+    return buf[0] == 'y' || buf[0] == 'Y';
+}
+
+// Is a (classpath) directory empty?
+bool os::dir_is_empty(const char* path)
+{
+    DIR* dir = NULL;
+    struct dirent* ptr;
+
+    dir = opendir(path);
+    if (dir == NULL)
+        return true;
+
+    // Scan the directory
+    bool result = true;
+    while (result && (ptr = readdir(dir)) != NULL) {
+        if (strcmp(ptr->d_name, ".") != 0 && strcmp(ptr->d_name, "..") != 0) {
+            result = false;
+        }
+    }
+    closedir(dir);
+    return result;
+}
+
+// This code originates from JDK's sysOpen and open64_w
+// from src/solaris/hpi/src/system_md.c
+
+int os::open(const char* path, int oflag, int mode)
+{
+    if (strlen(path) > MAX_PATH - 1) {
+        errno = ENAMETOOLONG;
+        return -1;
+    }
+
+    // All file descriptors that are opened in the Java process and not
+    // specifically destined for a subprocess should have the close-on-exec
+    // flag set.  If we don't set it, then careless 3rd party native code
+    // might fork and exec without closing all appropriate file descriptors
+    // (e.g. as we do in closeDescriptors in UNIXProcess.c), and this in
+    // turn might:
+    //
+    // - cause end-of-file to fail to be detected on some file
+    //   descriptors, resulting in mysterious hangs, or
+    //
+    // - might cause an fopen in the subprocess to fail on a system
+    //   suffering from bug 1085341.
+    //
+    // (Yes, the default setting of the close-on-exec flag is a Unix
+    // design flaw)
+    //
+    // See:
+    // 1085341: 32-bit stdio routines should support file descriptors >255
+    // 4843136: (process) pipe file descriptor from Runtime.exec not being closed
+    // 6339493: (process) Runtime.exec does not close all file descriptors on Solaris 9
+    //
+    // Modern Serenity kernels (after 2.6.23 2007) support O_CLOEXEC with open().
+    // O_CLOEXEC is preferable to using FD_CLOEXEC on an open file descriptor
+    // because it saves a system call and removes a small window where the flag
+    // is unset.  On ancient Serenity kernels the O_CLOEXEC flag will be ignored
+    // and we fall back to using FD_CLOEXEC (see below).
+#ifdef O_CLOEXEC
+    oflag |= O_CLOEXEC;
+#endif
+
+    int fd = ::open64(path, oflag, mode);
+    if (fd == -1)
+        return -1;
+
+    //If the open succeeded, the file might still be a directory
+    {
+        struct stat64 buf64;
+        int ret = ::fstat64(fd, &buf64);
+        int st_mode = buf64.st_mode;
+
+        if (ret != -1) {
+            if ((st_mode & S_IFMT) == S_IFDIR) {
+                errno = EISDIR;
+                ::close(fd);
+                return -1;
+            }
+        } else {
+            ::close(fd);
+            return -1;
+        }
+    }
+
+#ifdef FD_CLOEXEC
+    // Validate that the use of the O_CLOEXEC flag on open above worked.
+    // With recent kernels, we will perform this check exactly once.
+    static sig_atomic_t O_CLOEXEC_is_known_to_work = 0;
+    if (!O_CLOEXEC_is_known_to_work) {
+        int flags = ::fcntl(fd, F_GETFD);
+        if (flags != -1) {
+            if ((flags & FD_CLOEXEC) != 0)
+                O_CLOEXEC_is_known_to_work = 1;
+            else
+                ::fcntl(fd, F_SETFD, flags | FD_CLOEXEC);
+        }
+    }
+#endif
+
+    return fd;
+}
+
+// create binary file, rewriting existing file if required
+int os::create_binary_file(const char* path, bool rewrite_existing)
+{
+    int oflags = O_WRONLY | O_CREAT;
+    if (!rewrite_existing) {
+        oflags |= O_EXCL;
+    }
+    return ::open64(path, oflags, S_IREAD | S_IWRITE);
+}
+
+// return current position of file pointer
+jlong os::current_file_offset(int fd)
+{
+    return (jlong)::lseek64(fd, (off64_t)0, SEEK_CUR);
+}
+
+// move file pointer to the specified offset
+jlong os::seek_to_file_offset(int fd, jlong offset)
+{
+    return (jlong)::lseek64(fd, (off64_t)offset, SEEK_SET);
+}
+
+// This code originates from JDK's sysAvailable
+// from src/solaris/hpi/src/native_threads/src/sys_api_td.c
+
+int os::available(int fd, jlong* bytes)
+{
+    jlong cur, end;
+    int mode;
+    struct stat64 buf64;
+
+    if (::fstat64(fd, &buf64) >= 0) {
+        mode = buf64.st_mode;
+        if (S_ISCHR(mode) || S_ISFIFO(mode) || S_ISSOCK(mode)) {
+            int n;
+            if (::ioctl(fd, FIONREAD, &n) >= 0) {
+                *bytes = n;
+                return 1;
+            }
+        }
+    }
+    if ((cur = ::lseek64(fd, 0L, SEEK_CUR)) == -1) {
+        return 0;
+    } else if ((end = ::lseek64(fd, 0L, SEEK_END)) == -1) {
+        return 0;
+    } else if (::lseek64(fd, cur, SEEK_SET) == -1) {
+        return 0;
+    }
+    *bytes = end - cur;
+    return 1;
+}
+
+// Map a block of memory.
+char* os::pd_map_memory(int fd, const char* file_name, size_t file_offset,
+    char* addr, size_t bytes, bool read_only,
+    bool allow_exec)
+{
+    int prot;
+    int flags = MAP_PRIVATE;
+
+    if (read_only) {
+        prot = PROT_READ;
+    } else {
+        prot = PROT_READ | PROT_WRITE;
+    }
+
+    if (allow_exec) {
+        prot |= PROT_EXEC;
+    }
+
+    if (addr != NULL) {
+        flags |= MAP_FIXED;
+    }
+
+    char* mapped_address = (char*)mmap(addr, (size_t)bytes, prot, flags,
+        fd, file_offset);
+    if (mapped_address == MAP_FAILED) {
+        return NULL;
+    }
+    return mapped_address;
+}
+
+// Remap a block of memory.
+char* os::pd_remap_memory(int fd, const char* file_name, size_t file_offset,
+    char* addr, size_t bytes, bool read_only,
+    bool allow_exec)
+{
+    // same as map_memory() on this OS
+    return os::map_memory(fd, file_name, file_offset, addr, bytes, read_only,
+        allow_exec);
+}
+
+// Unmap a block of memory.
+bool os::pd_unmap_memory(char* addr, size_t bytes)
+{
+    return munmap(addr, bytes) == 0;
+}
+
+static jlong slow_thread_cpu_time(Thread* thread, bool user_sys_cpu_time);
+
+static jlong fast_cpu_time(Thread* thread)
+{
+    clockid_t clockid;
+    int rc = os::Serenity::pthread_getcpuclockid(thread->osthread()->pthread_id(),
+        &clockid);
+    if (rc == 0) {
+        return os::Serenity::fast_thread_cpu_time(clockid);
+    } else {
+        // It's possible to encounter a terminated native thread that failed
+        // to detach itself from the VM - which should result in ESRCH.
+        assert_status(rc == ESRCH, rc, "pthread_getcpuclockid failed");
+        return -1;
+    }
+}
+
+// current_thread_cpu_time(bool) and thread_cpu_time(Thread*, bool)
+// are used by JVM M&M and JVMTI to get user+sys or user CPU time
+// of a thread.
+//
+// current_thread_cpu_time() and thread_cpu_time(Thread*) returns
+// the fast estimate available on the platform.
+
+jlong os::current_thread_cpu_time()
+{
+    if (os::Serenity::supports_fast_thread_cpu_time()) {
+        return os::Serenity::fast_thread_cpu_time(CLOCK_THREAD_CPUTIME_ID);
+    } else {
+        // return user + sys since the cost is the same
+        return slow_thread_cpu_time(Thread::current(), true /* user + sys */);
+    }
+}
+
+jlong os::thread_cpu_time(Thread* thread)
+{
+    // consistent with what current_thread_cpu_time() returns
+    if (os::Serenity::supports_fast_thread_cpu_time()) {
+        return fast_cpu_time(thread);
+    } else {
+        return slow_thread_cpu_time(thread, true /* user + sys */);
+    }
+}
+
+jlong os::current_thread_cpu_time(bool user_sys_cpu_time)
+{
+    if (user_sys_cpu_time && os::Serenity::supports_fast_thread_cpu_time()) {
+        return os::Serenity::fast_thread_cpu_time(CLOCK_THREAD_CPUTIME_ID);
+    } else {
+        return slow_thread_cpu_time(Thread::current(), user_sys_cpu_time);
+    }
+}
+
+jlong os::thread_cpu_time(Thread* thread, bool user_sys_cpu_time)
+{
+    if (user_sys_cpu_time && os::Serenity::supports_fast_thread_cpu_time()) {
+        return fast_cpu_time(thread);
+    } else {
+        return slow_thread_cpu_time(thread, user_sys_cpu_time);
+    }
+}
+
+//  -1 on error.
+static jlong slow_thread_cpu_time(Thread* thread, bool user_sys_cpu_time)
+{
+    pid_t tid = thread->osthread()->thread_id();
+    char* s;
+    char stat[2048];
+    int statlen;
+    char proc_name[64];
+    int count;
+    long sys_time, user_time;
+    char cdummy;
+    int idummy;
+    long ldummy;
+    FILE* fp;
+
+    snprintf(proc_name, 64, "/proc/self/task/%d/stat", tid);
+    fp = fopen(proc_name, "r");
+    if (fp == NULL)
+        return -1;
+    statlen = fread(stat, 1, 2047, fp);
+    stat[statlen] = '\0';
+    fclose(fp);
+
+    // Skip pid and the command string. Note that we could be dealing with
+    // weird command names, e.g. user could decide to rename java launcher
+    // to "java 1.4.2 :)", then the stat file would look like
+    //                1234 (java 1.4.2 :)) R ... ...
+    // We don't really need to know the command string, just find the last
+    // occurrence of ")" and then start parsing from there. See bug 4726580.
+    s = strrchr(stat, ')');
+    if (s == NULL)
+        return -1;
+
+    // Skip blank chars
+    do {
+        s++;
+    } while (s && isspace(*s));
+
+    count = sscanf(s, "%c %d %d %d %d %d %lu %lu %lu %lu %lu %lu %lu",
+        &cdummy, &idummy, &idummy, &idummy, &idummy, &idummy,
+        &ldummy, &ldummy, &ldummy, &ldummy, &ldummy,
+        &user_time, &sys_time);
+    if (count != 13)
+        return -1;
+    if (user_sys_cpu_time) {
+        return ((jlong)sys_time + (jlong)user_time) * (1000000000 / clock_tics_per_sec);
+    } else {
+        return (jlong)user_time * (1000000000 / clock_tics_per_sec);
+    }
+}
+
+void os::current_thread_cpu_time_info(jvmtiTimerInfo* info_ptr)
+{
+    info_ptr->max_value = ALL_64_BITS;      // will not wrap in less than 64 bits
+    info_ptr->may_skip_backward = false;    // elapsed time not wall time
+    info_ptr->may_skip_forward = false;     // elapsed time not wall time
+    info_ptr->kind = JVMTI_TIMER_TOTAL_CPU; // user+system time is returned
+}
+
+void os::thread_cpu_time_info(jvmtiTimerInfo* info_ptr)
+{
+    info_ptr->max_value = ALL_64_BITS;      // will not wrap in less than 64 bits
+    info_ptr->may_skip_backward = false;    // elapsed time not wall time
+    info_ptr->may_skip_forward = false;     // elapsed time not wall time
+    info_ptr->kind = JVMTI_TIMER_TOTAL_CPU; // user+system time is returned
+}
+
+bool os::is_thread_cpu_time_supported()
+{
+    return true;
+}
+
+// System loadavg support.  Returns -1 if load average cannot be obtained.
+// Serenity doesn't yet have a (official) notion of processor sets,
+// so just return the system wide load average.
+int os::loadavg(double loadavg[], int nelem)
+{
+    return ::getloadavg(loadavg, nelem);
+}
+
+void os::pause()
+{
+    char filename[MAX_PATH];
+    if (PauseAtStartupFile && PauseAtStartupFile[0]) {
+        jio_snprintf(filename, MAX_PATH, "%s", PauseAtStartupFile);
+    } else {
+        jio_snprintf(filename, MAX_PATH, "./vm.paused.%d", current_process_id());
+    }
+
+    int fd = ::open(filename, O_WRONLY | O_CREAT | O_TRUNC, 0666);
+    if (fd != -1) {
+        struct stat buf;
+        ::close(fd);
+        while (::stat(filename, &buf) == 0) {
+            (void)::poll(NULL, 0, 100);
+        }
+    } else {
+        jio_fprintf(stderr,
+            "Could not open pause file '%s', continuing immediately.\n", filename);
+    }
+}
+
+// Get the default path to the core file
+// Returns the length of the string
+int os::get_core_path(char* buffer, size_t bufferSize)
+{
+    /*
+   * Max length of /proc/sys/kernel/core_pattern is 128 characters.
+   * See https://www.kernel.org/doc/Documentation/sysctl/kernel.txt
+   */
+    const int core_pattern_len = 129;
+    char core_pattern[core_pattern_len] = { 0 };
+
+    int core_pattern_file = ::open("/proc/sys/kernel/core_pattern", O_RDONLY);
+    if (core_pattern_file == -1) {
+        return -1;
+    }
+
+    ssize_t ret = ::read(core_pattern_file, core_pattern, core_pattern_len);
+    ::close(core_pattern_file);
+    if (ret <= 0 || ret >= core_pattern_len || core_pattern[0] == '\n') {
+        return -1;
+    }
+    if (core_pattern[ret - 1] == '\n') {
+        core_pattern[ret - 1] = '\0';
+    } else {
+        core_pattern[ret] = '\0';
+    }
+
+    // Replace the %p in the core pattern with the process id. NOTE: we do this
+    // only if the pattern doesn't start with "|", and we support only one %p in
+    // the pattern.
+    char* pid_pos = strstr(core_pattern, "%p");
+    const char* tail = (pid_pos != NULL) ? (pid_pos + 2) : ""; // skip over the "%p"
+    int written;
+
+    if (core_pattern[0] == '/') {
+        if (pid_pos != NULL) {
+            *pid_pos = '\0';
+            written = jio_snprintf(buffer, bufferSize, "%s%d%s", core_pattern,
+                current_process_id(), tail);
+        } else {
+            written = jio_snprintf(buffer, bufferSize, "%s", core_pattern);
+        }
+    } else {
+        char cwd[PATH_MAX];
+
+        const char* p = get_current_directory(cwd, PATH_MAX);
+        if (p == NULL) {
+            return -1;
+        }
+
+        if (core_pattern[0] == '|') {
+            written = jio_snprintf(buffer, bufferSize,
+                "\"%s\" (or dumping to %s/core.%d)",
+                &core_pattern[1], p, current_process_id());
+        } else if (pid_pos != NULL) {
+            *pid_pos = '\0';
+            written = jio_snprintf(buffer, bufferSize, "%s/%s%d%s", p, core_pattern,
+                current_process_id(), tail);
+        } else {
+            written = jio_snprintf(buffer, bufferSize, "%s/%s", p, core_pattern);
+        }
+    }
+
+    if (written < 0) {
+        return -1;
+    }
+
+    if (((size_t)written < bufferSize) && (pid_pos == NULL) && (core_pattern[0] != '|')) {
+        int core_uses_pid_file = ::open("/proc/sys/kernel/core_uses_pid", O_RDONLY);
+
+        if (core_uses_pid_file != -1) {
+            char core_uses_pid = 0;
+            ssize_t ret = ::read(core_uses_pid_file, &core_uses_pid, 1);
+            ::close(core_uses_pid_file);
+
+            if (core_uses_pid == '1') {
+                jio_snprintf(buffer + written, bufferSize - written,
+                    ".%d", current_process_id());
+            }
+        }
+    }
+
+    return strlen(buffer);
+}
+
+bool os::start_debugging(char* buf, int buflen)
+{
+    int len = (int)strlen(buf);
+    char* p = &buf[len];
+
+    jio_snprintf(p, buflen - len,
+        "\n\n"
+        "Do you want to debug the problem?\n\n"
+        "To debug, run 'gdb /proc/%d/exe %d'; then switch to thread " UINTX_FORMAT " (" INTPTR_FORMAT ")\n"
+        "Enter 'yes' to launch gdb automatically (PATH must include gdb)\n"
+        "Otherwise, press RETURN to abort...",
+        os::current_process_id(), os::current_process_id(),
+        os::current_thread_id(), os::current_thread_id());
+
+    bool yes = os::message_box("Unexpected Error", buf);
+
+    if (yes) {
+        // yes, user asked VM to launch debugger
+        jio_snprintf(buf, sizeof(char) * buflen, "gdb /proc/%d/exe %d",
+            os::current_process_id(), os::current_process_id());
+
+        os::fork_and_exec(buf);
+        yes = false;
+    }
+    return yes;
+}
+
+// Java/Compiler thread:
+//
+//   Low memory addresses
+// P0 +------------------------+
+//    |                        |\  Java thread created by VM does not have glibc
+//    |    glibc guard page    | - guard page, attached Java thread usually has
+//    |                        |/  1 glibc guard page.
+// P1 +------------------------+ Thread::stack_base() - Thread::stack_size()
+//    |                        |\
+//    |  HotSpot Guard Pages   | - red, yellow and reserved pages
+//    |                        |/
+//    +------------------------+ StackOverflow::stack_reserved_zone_base()
+//    |                        |\
+//    |      Normal Stack      | -
+//    |                        |/
+// P2 +------------------------+ Thread::stack_base()
+//
+// Non-Java thread:
+//
+//   Low memory addresses
+// P0 +------------------------+
+//    |                        |\
+//    |  glibc guard page      | - usually 1 page
+//    |                        |/
+// P1 +------------------------+ Thread::stack_base() - Thread::stack_size()
+//    |                        |\
+//    |      Normal Stack      | -
+//    |                        |/
+// P2 +------------------------+ Thread::stack_base()
+//
+// ** P1 (aka bottom) and size (P2 = P1 - size) are the address and stack size
+//    returned from pthread_attr_getstack().
+// ** Due to NPTL implementation error, linux takes the glibc guard page out
+//    of the stack size given in pthread_attr. We work around this for
+//    threads created by the VM. (We adapt bottom to be P1 and size accordingly.)
+//
+#ifndef ZERO
+static void current_stack_region(address* bottom, size_t* size)
+{
+    if (os::is_primordial_thread()) {
+        // primordial thread needs special handling because pthread_getattr_np()
+        // may return bogus value.
+        *bottom = os::Serenity::initial_thread_stack_bottom();
+        *size = os::Serenity::initial_thread_stack_size();
+    } else {
+        pthread_attr_t attr;
+
+        int rslt = pthread_getattr_np(pthread_self(), &attr);
+
+        // JVM needs to know exact stack location, abort if it fails
+        if (rslt != 0) {
+            if (rslt == ENOMEM) {
+                vm_exit_out_of_memory(0, OOM_MMAP_ERROR, "pthread_getattr_np");
+            } else {
+                fatal("pthread_getattr_np failed with error = %d", rslt);
+            }
+        }
+
+        if (pthread_attr_getstack(&attr, (void**)bottom, size) != 0) {
+            fatal("Cannot locate current stack attributes!");
+        }
+
+        // Work around NPTL stack guard error.
+        size_t guard_size = 0;
+        rslt = pthread_attr_getguardsize(&attr, &guard_size);
+        if (rslt != 0) {
+            fatal("pthread_attr_getguardsize failed with error = %d", rslt);
+        }
+        *bottom += guard_size;
+        *size -= guard_size;
+
+        pthread_attr_destroy(&attr);
+    }
+    assert(os::current_stack_pointer() >= *bottom && os::current_stack_pointer() < *bottom + *size, "just checking");
+}
+
+address os::current_stack_base()
+{
+    address bottom;
+    size_t size;
+    current_stack_region(&bottom, &size);
+    return (bottom + size);
+}
+
+size_t os::current_stack_size()
+{
+    // This stack size includes the usable stack and HotSpot guard pages
+    // (for the threads that have Hotspot guard pages).
+    address bottom;
+    size_t size;
+    current_stack_region(&bottom, &size);
+    return size;
+}
+#endif
+
+static inline struct timespec get_mtime(const char* filename)
+{
+    struct stat st;
+    int ret = os::stat(filename, &st);
+    assert(ret == 0, "failed to stat() file '%s': %s", filename, os::strerror(errno));
+    return st.st_mtim;
+}
+
+int os::compare_file_modified_times(const char* file1, const char* file2)
+{
+    struct timespec filetime1 = get_mtime(file1);
+    struct timespec filetime2 = get_mtime(file2);
+    int diff = filetime1.tv_sec - filetime2.tv_sec;
+    if (diff == 0) {
+        return filetime1.tv_nsec - filetime2.tv_nsec;
+    }
+    return diff;
+}
+
+bool os::supports_map_sync()
+{
+    return true;
+}
+
+void os::print_memory_mappings(char* addr, size_t bytes, outputStream* st)
+{
+    unsigned long long start = (unsigned long long)addr;
+    unsigned long long end = start + bytes;
+    FILE* f = ::fopen("/proc/self/maps", "r");
+    int num_found = 0;
+    if (f != NULL) {
+        st->print("Range [%llx-%llx) contains: ", start, end);
+        char line[512];
+        while (fgets(line, sizeof(line), f) == line) {
+            unsigned long long a1 = 0;
+            unsigned long long a2 = 0;
+            if (::sscanf(line, "%llx-%llx", &a1, &a2) == 2) {
+                // Lets print out every range which touches ours.
+                if ((a1 >= start && a1 < end) || // left leg in
+                    (a2 >= start && a2 < end) || // right leg in
+                    (a1 < start && a2 >= end)) { // superimposition
+                    num_found++;
+                    st->print("%s", line); // line includes \n
+                }
+            }
+        }
+        ::fclose(f);
+        if (num_found == 0) {
+            st->print("nothing.");
+        }
+        st->cr();
+    }
+}
diff --git a/src/hotspot/os/serenity/os_serenity.hpp b/src/hotspot/os/serenity/os_serenity.hpp
new file mode 100644
index 00000000000..4b3d86d5785
--- /dev/null
+++ b/src/hotspot/os/serenity/os_serenity.hpp
@@ -0,0 +1,165 @@
+/*
+ * Copyright (c) 1999, 2021, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#ifndef OS_SERENITY_OS_SERENITY_HPP
+#define OS_SERENITY_OS_SERENITY_HPP
+
+// Information about the protection of the page at address '0' on this os.
+static bool zero_page_read_protected() { return true; }
+
+class Serenity {
+    friend class CgroupSubsystem;
+    friend class os;
+    friend class OSContainer;
+    friend class TestReserveMemorySpecial;
+
+    static int (*_pthread_getcpuclockid)(pthread_t, clockid_t*);
+    static int (*_pthread_setname_np)(pthread_t, const char*);
+
+    static address _initial_thread_stack_bottom;
+    static uintptr_t _initial_thread_stack_size;
+
+    static const char* _libc_version;
+    static const char* _libpthread_version;
+
+    static bool _supports_fast_thread_cpu_time;
+
+    static size_t _default_large_page_size;
+
+protected:
+    static julong _physical_memory;
+    static pthread_t _main_thread;
+    static int _page_size;
+
+    static julong available_memory();
+    static julong physical_memory() { return _physical_memory; }
+    static void set_physical_memory(julong phys_mem) { _physical_memory = phys_mem; }
+    static int active_processor_count();
+
+    static void initialize_system_info();
+
+    static int commit_memory_impl(char* addr, size_t bytes, bool exec);
+    static int commit_memory_impl(char* addr, size_t bytes,
+        size_t alignment_hint, bool exec);
+
+    static void set_libc_version(const char* s) { _libc_version = s; }
+    static void set_libpthread_version(const char* s) { _libpthread_version = s; }
+
+    static size_t default_large_page_size();
+    static size_t find_default_large_page_size();
+    static size_t find_large_page_size(size_t page_size);
+
+    static char* reserve_memory_special_shm(size_t bytes, size_t alignment, char* req_addr, bool exec);
+
+    static bool release_memory_special_impl(char* base, size_t bytes);
+    static bool release_memory_special_shm(char* base, size_t bytes);
+
+    static void print_process_memory_info(outputStream* st);
+    static void print_system_memory_info(outputStream* st);
+    static bool print_container_info(outputStream* st);
+    static void print_steal_info(outputStream* st);
+    static void print_distro_info(outputStream* st);
+    static void print_libversion_info(outputStream* st);
+    static void print_proc_sys_info(outputStream* st);
+    static bool print_ld_preload_file(outputStream* st);
+    static void print_uptime_info(outputStream* st);
+
+public:
+    struct CPUPerfTicks {
+        uint64_t used;
+        uint64_t usedKernel;
+        uint64_t total;
+        uint64_t steal;
+        bool has_steal_ticks;
+    };
+
+    // which_logical_cpu=-1 returns accumulated ticks for all cpus.
+    static bool get_tick_information(CPUPerfTicks* pticks, int which_logical_cpu);
+    static bool _stack_is_executable;
+    static void* dlopen_helper(const char* name, char* ebuf, int ebuflen);
+    static void* dll_load_in_vmthread(const char* name, char* ebuf, int ebuflen);
+
+    static void init_thread_fpu_state();
+    static int get_fpu_control_word();
+    static void set_fpu_control_word(int fpu_control);
+    static pthread_t main_thread(void) { return _main_thread; }
+    // returns kernel thread id (similar to LWP id on Solaris), which can be
+    // used to access /proc
+    static pid_t gettid();
+
+    static address initial_thread_stack_bottom(void) { return _initial_thread_stack_bottom; }
+    static uintptr_t initial_thread_stack_size(void) { return _initial_thread_stack_size; }
+
+    static int page_size(void) { return _page_size; }
+    static void set_page_size(int val) { _page_size = val; }
+
+    static void libpthread_init();
+    static void sched_getcpu_init();
+    // Return default guard size for the specified thread type
+    static size_t default_guard_size(os::ThreadType thr_type);
+
+    static void capture_initial_stack(size_t max_size);
+
+    // Stack overflow handling
+    static bool manually_expand_stack(JavaThread* t, address addr);
+
+    // fast POSIX clocks support
+    static void fast_thread_clock_init(void);
+
+    static int pthread_getcpuclockid(pthread_t tid, clockid_t* clock_id)
+    {
+        return _pthread_getcpuclockid ? _pthread_getcpuclockid(tid, clock_id) : -1;
+    }
+
+    static bool supports_fast_thread_cpu_time()
+    {
+        return _supports_fast_thread_cpu_time;
+    }
+
+    static jlong fast_thread_cpu_time(clockid_t clockid);
+
+    // Determine if the vmid is the parent pid for a child in a PID namespace.
+    // Return the namespace pid if so, otherwise -1.
+    static int get_namespace_pid(int vmid);
+
+    // Stack repair handling
+
+    // none present
+
+private:
+    static void expand_stack_to(address bottom);
+
+    typedef int (*sched_getcpu_func_t)(void);
+
+    static sched_getcpu_func_t _sched_getcpu;
+
+    static void set_sched_getcpu(sched_getcpu_func_t func) { _sched_getcpu = func; }
+    static int sched_getcpu_syscall(void);
+
+public:
+    static int sched_getcpu() { return _sched_getcpu != NULL ? _sched_getcpu() : -1; }
+    static int numa_available() { return -1; }
+};
+
+#endif // OS_SERENITY_OS_SERENITY_HPP
diff --git a/src/hotspot/os/serenity/os_serenity.inline.hpp b/src/hotspot/os/serenity/os_serenity.inline.hpp
new file mode 100644
index 00000000000..49e66f7aa5d
--- /dev/null
+++ b/src/hotspot/os/serenity/os_serenity.inline.hpp
@@ -0,0 +1,44 @@
+/*
+ * Copyright (c) 1999, 2021, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#ifndef OS_SERENITY_OS_SERENITY_INLINE_HPP
+#define OS_SERENITY_OS_SERENITY_INLINE_HPP
+
+#include "runtime/os.hpp"
+#include "os_posix.inline.hpp"
+
+inline bool os::uses_stack_guard_pages() {
+  return true;
+}
+
+inline bool os::must_commit_stack_guard_pages() {
+  assert(uses_stack_guard_pages(), "sanity check");
+  return true;
+}
+
+// Bang the shadow pages if they need to be touched to be mapped.
+inline void os::map_stack_shadow_pages(address sp) {
+}
+
+#endif // OS_SERENITY_OS_SERENITY_INLINE_HPP
diff --git a/src/hotspot/os/serenity/os_share_serenity.hpp b/src/hotspot/os/serenity/os_share_serenity.hpp
new file mode 100644
index 00000000000..bc752ccf2c1
--- /dev/null
+++ b/src/hotspot/os/serenity/os_share_serenity.hpp
@@ -0,0 +1,36 @@
+/*
+ * Copyright (c) 1999, 2019, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#ifndef OS_SERENITY_OS_SHARE_SERENITY_HPP
+#define OS_SERENITY_OS_SHARE_SERENITY_HPP
+
+// misc
+void handle_unexpected_exception(Thread* thread, int sig, siginfo_t* info, address pc, address adjusted_pc);
+#ifndef PRODUCT
+void continue_with_dump(void);
+#endif
+
+#define PROCFILE_LENGTH 128
+
+#endif // OS_SERENITY_OS_SHARE_SERENITY_HPP
diff --git a/src/hotspot/os/serenity/threadCritical_serenity.cpp b/src/hotspot/os/serenity/threadCritical_serenity.cpp
new file mode 100644
index 00000000000..71c51df599d
--- /dev/null
+++ b/src/hotspot/os/serenity/threadCritical_serenity.cpp
@@ -0,0 +1,61 @@
+/*
+ * Copyright (c) 2001, 2010, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#include "precompiled.hpp"
+#include "runtime/thread.inline.hpp"
+#include "runtime/threadCritical.hpp"
+
+// put OS-includes here
+# include <pthread.h>
+
+//
+// See threadCritical.hpp for details of this class.
+//
+
+static pthread_t             tc_owner = 0;
+static pthread_mutex_t       tc_mutex = PTHREAD_MUTEX_INITIALIZER;
+static int                   tc_count = 0;
+
+ThreadCritical::ThreadCritical() {
+  pthread_t self = pthread_self();
+  if (self != tc_owner) {
+    int ret = pthread_mutex_lock(&tc_mutex);
+    guarantee(ret == 0, "fatal error with pthread_mutex_lock()");
+    assert(tc_count == 0, "Lock acquired with illegal reentry count.");
+    tc_owner = self;
+  }
+  tc_count++;
+}
+
+ThreadCritical::~ThreadCritical() {
+  assert(tc_owner == pthread_self(), "must have correct owner");
+  assert(tc_count > 0, "must have correct count");
+
+  tc_count--;
+  if (tc_count == 0) {
+    tc_owner = 0;
+    int ret = pthread_mutex_unlock(&tc_mutex);
+    guarantee(ret == 0, "fatal error with pthread_mutex_unlock()");
+  }
+}
diff --git a/src/hotspot/os/serenity/vmStructs_serenity.hpp b/src/hotspot/os/serenity/vmStructs_serenity.hpp
new file mode 100644
index 00000000000..73d6adbdb73
--- /dev/null
+++ b/src/hotspot/os/serenity/vmStructs_serenity.hpp
@@ -0,0 +1,45 @@
+/*
+ * Copyright (c) 2015, 2019, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#ifndef OS_SERENITY_VMSTRUCTS_SERENITY_HPP
+#define OS_SERENITY_VMSTRUCTS_SERENITY_HPP
+
+#include <dlfcn.h>
+
+// These are the OS-specific fields, types and integer
+// constants required by the Serviceability Agent. This file is
+// referenced by vmStructs.cpp.
+
+#define VM_STRUCTS_OS(nonstatic_field, static_field, unchecked_nonstatic_field, volatile_nonstatic_field, nonproduct_nonstatic_field, c2_nonstatic_field, unchecked_c1_static_field, unchecked_c2_static_field)
+
+#define VM_TYPES_OS(declare_type, declare_toplevel_type, declare_oop_type, declare_integer_type, declare_unsigned_integer_type, declare_c1_toplevel_type, declare_c2_type, declare_c2_toplevel_type)
+
+#define VM_INT_CONSTANTS_OS(declare_constant, declare_preprocessor_constant, declare_c1_constant, declare_c2_constant, declare_c2_preprocessor_constant)
+
+#define VM_LONG_CONSTANTS_OS(declare_constant, declare_preprocessor_constant, declare_c1_constant, declare_c2_constant, declare_c2_preprocessor_constant)
+
+#define VM_ADDRESSES_OS(declare_address, declare_preprocessor_address, declare_function) \
+  declare_preprocessor_address("RTLD_DEFAULT", RTLD_DEFAULT)
+
+#endif // OS_SERENITY_VMSTRUCTS_SERENITY_HPP
diff --git a/src/hotspot/os/serenity/waitBarrier_serenity.cpp b/src/hotspot/os/serenity/waitBarrier_serenity.cpp
new file mode 100644
index 00000000000..491f9e6b866
--- /dev/null
+++ b/src/hotspot/os/serenity/waitBarrier_serenity.cpp
@@ -0,0 +1,80 @@
+/*
+ * Copyright (c) 2019, 2020, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#include "precompiled.hpp"
+#include "runtime/orderAccess.hpp"
+#include "runtime/os.hpp"
+#include "waitBarrier_linux.hpp"
+#include <sys/syscall.h>
+#include <linux/futex.h>
+
+#define check_with_errno(check_type, cond, msg)                             \
+  do {                                                                      \
+    int err = errno;                                                        \
+    check_type(cond, "%s: error='%s' (errno=%s)", msg, os::strerror(err),   \
+               os::errno_name(err));                                        \
+} while (false)
+
+#define guarantee_with_errno(cond, msg) check_with_errno(guarantee, cond, msg)
+
+static int futex(volatile int *addr, int futex_op, int op_arg) {
+  return syscall(SYS_futex, addr, futex_op, op_arg, NULL, NULL, 0);
+}
+
+void LinuxWaitBarrier::arm(int barrier_tag) {
+  assert(_futex_barrier == 0, "Should not be already armed: "
+         "_futex_barrier=%d", _futex_barrier);
+  _futex_barrier = barrier_tag;
+  OrderAccess::fence();
+}
+
+void LinuxWaitBarrier::disarm() {
+  assert(_futex_barrier != 0, "Should be armed/non-zero.");
+  _futex_barrier = 0;
+  int s = futex(&_futex_barrier,
+                FUTEX_WAKE_PRIVATE,
+                INT_MAX /* wake a max of this many threads */);
+  guarantee_with_errno(s > -1, "futex FUTEX_WAKE failed");
+}
+
+void LinuxWaitBarrier::wait(int barrier_tag) {
+  assert(barrier_tag != 0, "Trying to wait on disarmed value");
+  if (barrier_tag == 0 ||
+      barrier_tag != _futex_barrier) {
+    OrderAccess::fence();
+    return;
+  }
+  do {
+    int s = futex(&_futex_barrier,
+                  FUTEX_WAIT_PRIVATE,
+                  barrier_tag /* should be this tag */);
+    guarantee_with_errno((s == 0) ||
+                         (s == -1 && errno == EAGAIN) ||
+                         (s == -1 && errno == EINTR),
+                         "futex FUTEX_WAIT failed");
+    // Return value 0: woken up, but re-check in case of spurious wakeup.
+    // Error EINTR: woken by signal, so re-check and re-wait if necessary.
+    // Error EAGAIN: we are already disarmed and so will pass the check.
+  } while (barrier_tag == _futex_barrier);
+}
diff --git a/src/hotspot/os/serenity/waitBarrier_serenity.hpp b/src/hotspot/os/serenity/waitBarrier_serenity.hpp
new file mode 100644
index 00000000000..0ea206d78f3
--- /dev/null
+++ b/src/hotspot/os/serenity/waitBarrier_serenity.hpp
@@ -0,0 +1,47 @@
+/*
+ * Copyright (c) 2019, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#ifndef OS_SERENITY_WAITBARRIER_SERENITY_HPP
+#define OS_SERENITY_WAITBARRIER_SERENITY_HPP
+
+#include "memory/allocation.hpp"
+#include "utilities/globalDefinitions.hpp"
+
+class LinuxWaitBarrier : public CHeapObj<mtInternal> {
+  volatile int _futex_barrier;
+
+  NONCOPYABLE(LinuxWaitBarrier);
+
+ public:
+  LinuxWaitBarrier() : _futex_barrier(0) {};
+  ~LinuxWaitBarrier() {};
+
+  const char* description() { return "futex"; }
+
+  void arm(int barrier_tag);
+  void disarm();
+  void wait(int barrier_tag);
+};
+
+#endif // OS_SERENITY_WAITBARRIER_SERENITY_HPP
diff --git a/src/hotspot/os_cpu/serenity_x86/assembler_serenity_x86.cpp b/src/hotspot/os_cpu/serenity_x86/assembler_serenity_x86.cpp
new file mode 100644
index 00000000000..dd20ea833c8
--- /dev/null
+++ b/src/hotspot/os_cpu/serenity_x86/assembler_serenity_x86.cpp
@@ -0,0 +1,32 @@
+/*
+ * Copyright (c) 1999, 2015, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#include "precompiled.hpp"
+#include "asm/macroAssembler.hpp"
+#include "asm/macroAssembler.inline.hpp"
+#include "runtime/os.hpp"
+
+void MacroAssembler::int3() {
+  call(RuntimeAddress(CAST_FROM_FN_PTR(address, os::breakpoint)));
+}
diff --git a/src/hotspot/os_cpu/serenity_x86/atomic_serenity_x86.hpp b/src/hotspot/os_cpu/serenity_x86/atomic_serenity_x86.hpp
new file mode 100644
index 00000000000..20d74906953
--- /dev/null
+++ b/src/hotspot/os_cpu/serenity_x86/atomic_serenity_x86.hpp
@@ -0,0 +1,225 @@
+/*
+ * Copyright (c) 1999, 2019, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#ifndef OS_CPU_SERENITY_X86_ATOMIC_SERENITY_X86_HPP
+#define OS_CPU_SERENITY_X86_ATOMIC_SERENITY_X86_HPP
+
+// Implementation of class atomic
+
+template<size_t byte_size>
+struct Atomic::PlatformAdd {
+  template<typename D, typename I>
+  D fetch_and_add(D volatile* dest, I add_value, atomic_memory_order order) const;
+
+  template<typename D, typename I>
+  D add_and_fetch(D volatile* dest, I add_value, atomic_memory_order order) const {
+    return fetch_and_add(dest, add_value, order) + add_value;
+  }
+};
+
+template<>
+template<typename D, typename I>
+inline D Atomic::PlatformAdd<4>::fetch_and_add(D volatile* dest, I add_value,
+                                               atomic_memory_order order) const {
+  STATIC_ASSERT(4 == sizeof(I));
+  STATIC_ASSERT(4 == sizeof(D));
+  D old_value;
+  __asm__ volatile (  "lock xaddl %0,(%2)"
+                    : "=r" (old_value)
+                    : "0" (add_value), "r" (dest)
+                    : "cc", "memory");
+  return old_value;
+}
+
+template<>
+template<typename T>
+inline T Atomic::PlatformXchg<4>::operator()(T volatile* dest,
+                                             T exchange_value,
+                                             atomic_memory_order order) const {
+  STATIC_ASSERT(4 == sizeof(T));
+  __asm__ volatile (  "xchgl (%2),%0"
+                    : "=r" (exchange_value)
+                    : "0" (exchange_value), "r" (dest)
+                    : "memory");
+  return exchange_value;
+}
+
+template<>
+template<typename T>
+inline T Atomic::PlatformCmpxchg<1>::operator()(T volatile* dest,
+                                                T compare_value,
+                                                T exchange_value,
+                                                atomic_memory_order /* order */) const {
+  STATIC_ASSERT(1 == sizeof(T));
+  __asm__ volatile ("lock cmpxchgb %1,(%3)"
+                    : "=a" (exchange_value)
+                    : "q" (exchange_value), "a" (compare_value), "r" (dest)
+                    : "cc", "memory");
+  return exchange_value;
+}
+
+template<>
+template<typename T>
+inline T Atomic::PlatformCmpxchg<4>::operator()(T volatile* dest,
+                                                T compare_value,
+                                                T exchange_value,
+                                                atomic_memory_order /* order */) const {
+  STATIC_ASSERT(4 == sizeof(T));
+  __asm__ volatile ("lock cmpxchgl %1,(%3)"
+                    : "=a" (exchange_value)
+                    : "r" (exchange_value), "a" (compare_value), "r" (dest)
+                    : "cc", "memory");
+  return exchange_value;
+}
+
+#ifdef AMD64
+
+template<>
+template<typename D, typename I>
+inline D Atomic::PlatformAdd<8>::fetch_and_add(D volatile* dest, I add_value,
+                                               atomic_memory_order order) const {
+  STATIC_ASSERT(8 == sizeof(I));
+  STATIC_ASSERT(8 == sizeof(D));
+  D old_value;
+  __asm__ __volatile__ ("lock xaddq %0,(%2)"
+                        : "=r" (old_value)
+                        : "0" (add_value), "r" (dest)
+                        : "cc", "memory");
+  return old_value;
+}
+
+template<>
+template<typename T>
+inline T Atomic::PlatformXchg<8>::operator()(T volatile* dest, T exchange_value,
+                                             atomic_memory_order order) const {
+  STATIC_ASSERT(8 == sizeof(T));
+  __asm__ __volatile__ ("xchgq (%2),%0"
+                        : "=r" (exchange_value)
+                        : "0" (exchange_value), "r" (dest)
+                        : "memory");
+  return exchange_value;
+}
+
+template<>
+template<typename T>
+inline T Atomic::PlatformCmpxchg<8>::operator()(T volatile* dest,
+                                                T compare_value,
+                                                T exchange_value,
+                                                atomic_memory_order /* order */) const {
+  STATIC_ASSERT(8 == sizeof(T));
+  __asm__ __volatile__ ("lock cmpxchgq %1,(%3)"
+                        : "=a" (exchange_value)
+                        : "r" (exchange_value), "a" (compare_value), "r" (dest)
+                        : "cc", "memory");
+  return exchange_value;
+}
+
+#else // !AMD64
+
+extern "C" {
+  // defined in linux_x86.s
+  int64_t _Atomic_cmpxchg_long(int64_t, volatile int64_t*, int64_t);
+  void _Atomic_move_long(const volatile int64_t* src, volatile int64_t* dst);
+}
+
+template<>
+template<typename T>
+inline T Atomic::PlatformCmpxchg<8>::operator()(T volatile* dest,
+                                                T compare_value,
+                                                T exchange_value,
+                                                atomic_memory_order order) const {
+  STATIC_ASSERT(8 == sizeof(T));
+  return cmpxchg_using_helper<int64_t>(_Atomic_cmpxchg_long, dest, compare_value, exchange_value);
+}
+
+template<>
+template<typename T>
+inline T Atomic::PlatformLoad<8>::operator()(T const volatile* src) const {
+  STATIC_ASSERT(8 == sizeof(T));
+  volatile int64_t dest;
+  _Atomic_move_long(reinterpret_cast<const volatile int64_t*>(src), reinterpret_cast<volatile int64_t*>(&dest));
+  return PrimitiveConversions::cast<T>(dest);
+}
+
+template<>
+template<typename T>
+inline void Atomic::PlatformStore<8>::operator()(T volatile* dest,
+                                                 T store_value) const {
+  STATIC_ASSERT(8 == sizeof(T));
+  _Atomic_move_long(reinterpret_cast<const volatile int64_t*>(&store_value), reinterpret_cast<volatile int64_t*>(dest));
+}
+
+#endif // AMD64
+
+template<>
+struct Atomic::PlatformOrderedStore<1, RELEASE_X_FENCE>
+{
+  template <typename T>
+  void operator()(volatile T* p, T v) const {
+    __asm__ volatile (  "xchgb (%2),%0"
+                      : "=q" (v)
+                      : "0" (v), "r" (p)
+                      : "memory");
+  }
+};
+
+template<>
+struct Atomic::PlatformOrderedStore<2, RELEASE_X_FENCE>
+{
+  template <typename T>
+  void operator()(volatile T* p, T v) const {
+    __asm__ volatile (  "xchgw (%2),%0"
+                      : "=r" (v)
+                      : "0" (v), "r" (p)
+                      : "memory");
+  }
+};
+
+template<>
+struct Atomic::PlatformOrderedStore<4, RELEASE_X_FENCE>
+{
+  template <typename T>
+  void operator()(volatile T* p, T v) const {
+    __asm__ volatile (  "xchgl (%2),%0"
+                      : "=r" (v)
+                      : "0" (v), "r" (p)
+                      : "memory");
+  }
+};
+
+#ifdef AMD64
+template<>
+struct Atomic::PlatformOrderedStore<8, RELEASE_X_FENCE>
+{
+  template <typename T>
+  void operator()(volatile T* p, T v) const {
+    __asm__ volatile (  "xchgq (%2), %0"
+                      : "=r" (v)
+                      : "0" (v), "r" (p)
+                      : "memory");
+  }
+};
+#endif // AMD64
+
+#endif // OS_CPU_SERENITY_X86_ATOMIC_SERENITY_X86_HPP
diff --git a/src/hotspot/os_cpu/serenity_x86/bytes_serenity_x86.inline.hpp b/src/hotspot/os_cpu/serenity_x86/bytes_serenity_x86.inline.hpp
new file mode 100644
index 00000000000..74b930cca35
--- /dev/null
+++ b/src/hotspot/os_cpu/serenity_x86/bytes_serenity_x86.inline.hpp
@@ -0,0 +1,79 @@
+/*
+ * Copyright (c) 1999, 2020, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#ifndef OS_CPU_SERENITY_X86_BYTES_SERENITY_X86_INLINE_HPP
+#define OS_CPU_SERENITY_X86_BYTES_SERENITY_X86_INLINE_HPP
+
+#include <byteswap.h>
+
+// Efficient swapping of data bytes from Java byte
+// ordering to native byte ordering and vice versa.
+inline u2   Bytes::swap_u2(u2 x) {
+#ifdef AMD64
+  return bswap_16(x);
+#else
+  u2 ret;
+  __asm__ __volatile__ (
+    "movw %0, %%ax;"
+    "xchg %%al, %%ah;"
+    "movw %%ax, %0"
+    :"=r" (ret)      // output : register 0 => ret
+    :"0"  (x)        // input  : x => register 0
+    :"ax", "0"       // clobbered registers
+  );
+  return ret;
+#endif // AMD64
+}
+
+inline u4   Bytes::swap_u4(u4 x) {
+#ifdef AMD64
+  return bswap_32(x);
+#else
+  u4 ret;
+  __asm__ __volatile__ (
+    "bswap %0"
+    :"=r" (ret)      // output : register 0 => ret
+    :"0"  (x)        // input  : x => register 0
+    :"0"             // clobbered register
+  );
+  return ret;
+#endif // AMD64
+}
+
+#ifdef AMD64
+inline u8 Bytes::swap_u8(u8 x) {
+  return bswap_64(x);
+}
+#else
+// Helper function for swap_u8
+inline u8   Bytes::swap_u8_base(u4 x, u4 y) {
+  return (((u8)swap_u4(x))<<32) | swap_u4(y);
+}
+
+inline u8 Bytes::swap_u8(u8 x) {
+  return swap_u8_base(*(u4*)&x, *(((u4*)&x)+1));
+}
+#endif // !AMD64
+
+#endif // OS_CPU_SERENITY_X86_BYTES_SERENITY_X86_INLINE_HPP
diff --git a/src/hotspot/os_cpu/serenity_x86/copy_serenity_x86.inline.hpp b/src/hotspot/os_cpu/serenity_x86/copy_serenity_x86.inline.hpp
new file mode 100644
index 00000000000..e2a9b34ad41
--- /dev/null
+++ b/src/hotspot/os_cpu/serenity_x86/copy_serenity_x86.inline.hpp
@@ -0,0 +1,309 @@
+/*
+ * Copyright (c) 2003, 2019, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#ifndef OS_CPU_SERENITY_X86_COPY_SERENITY_X86_INLINE_HPP
+#define OS_CPU_SERENITY_X86_COPY_SERENITY_X86_INLINE_HPP
+
+static void pd_conjoint_words(const HeapWord* from, HeapWord* to, size_t count) {
+#ifdef AMD64
+  (void)memmove(to, from, count * HeapWordSize);
+#else
+  // Includes a zero-count check.
+  intx temp = 0;
+  __asm__ volatile("        testl   %6,%6         ;"
+                   "        jz      7f            ;"
+                   "        cmpl    %4,%5         ;"
+                   "        leal    -4(%4,%6,4),%3;"
+                   "        jbe     1f            ;"
+                   "        cmpl    %7,%5         ;"
+                   "        jbe     4f            ;"
+                   "1:      cmpl    $32,%6        ;"
+                   "        ja      3f            ;"
+                   "        subl    %4,%1         ;"
+                   "2:      movl    (%4),%3       ;"
+                   "        movl    %7,(%5,%4,1)  ;"
+                   "        addl    $4,%0         ;"
+                   "        subl    $1,%2          ;"
+                   "        jnz     2b            ;"
+                   "        jmp     7f            ;"
+                   "3:      rep;    smovl         ;"
+                   "        jmp     7f            ;"
+                   "4:      cmpl    $32,%2        ;"
+                   "        movl    %7,%0         ;"
+                   "        leal    -4(%5,%6,4),%1;"
+                   "        ja      6f            ;"
+                   "        subl    %4,%1         ;"
+                   "5:      movl    (%4),%3       ;"
+                   "        movl    %7,(%5,%4,1)  ;"
+                   "        subl    $4,%0         ;"
+                   "        subl    $1,%2          ;"
+                   "        jnz     5b            ;"
+                   "        jmp     7f            ;"
+                   "6:      std                   ;"
+                   "        rep;    smovl         ;"
+                   "        cld                   ;"
+                   "7:      nop                    "
+                   : "=S" (from), "=D" (to), "=c" (count), "=r" (temp)
+                   : "0"  (from), "1"  (to), "2"  (count), "3"  (temp)
+                   : "memory", "flags");
+#endif // AMD64
+}
+
+static void pd_disjoint_words(const HeapWord* from, HeapWord* to, size_t count) {
+#ifdef AMD64
+  switch (count) {
+  case 8:  to[7] = from[7];
+  case 7:  to[6] = from[6];
+  case 6:  to[5] = from[5];
+  case 5:  to[4] = from[4];
+  case 4:  to[3] = from[3];
+  case 3:  to[2] = from[2];
+  case 2:  to[1] = from[1];
+  case 1:  to[0] = from[0];
+  case 0:  break;
+  default:
+    (void)memcpy(to, from, count * HeapWordSize);
+    break;
+  }
+#else
+  // Includes a zero-count check.
+  intx temp = 0;
+  __asm__ volatile("        testl   %6,%6       ;"
+                   "        jz      3f          ;"
+                   "        cmpl    $32,%6      ;"
+                   "        ja      2f          ;"
+                   "        subl    %4,%1       ;"
+                   "1:      movl    (%4),%3     ;"
+                   "        movl    %7,(%5,%4,1);"
+                   "        addl    $4,%0       ;"
+                   "        subl    $1,%2        ;"
+                   "        jnz     1b          ;"
+                   "        jmp     3f          ;"
+                   "2:      rep;    smovl       ;"
+                   "3:      nop                  "
+                   : "=S" (from), "=D" (to), "=c" (count), "=r" (temp)
+                   : "0"  (from), "1"  (to), "2"  (count), "3"  (temp)
+                   : "memory", "cc");
+#endif // AMD64
+}
+
+static void pd_disjoint_words_atomic(const HeapWord* from, HeapWord* to, size_t count) {
+#ifdef AMD64
+  switch (count) {
+  case 8:  to[7] = from[7];
+  case 7:  to[6] = from[6];
+  case 6:  to[5] = from[5];
+  case 5:  to[4] = from[4];
+  case 4:  to[3] = from[3];
+  case 3:  to[2] = from[2];
+  case 2:  to[1] = from[1];
+  case 1:  to[0] = from[0];
+  case 0:  break;
+  default:
+    while (count-- > 0) {
+      *to++ = *from++;
+    }
+    break;
+  }
+#else
+  // pd_disjoint_words is word-atomic in this implementation.
+  pd_disjoint_words(from, to, count);
+#endif // AMD64
+}
+
+static void pd_aligned_conjoint_words(const HeapWord* from, HeapWord* to, size_t count) {
+  pd_conjoint_words(from, to, count);
+}
+
+static void pd_aligned_disjoint_words(const HeapWord* from, HeapWord* to, size_t count) {
+  pd_disjoint_words(from, to, count);
+}
+
+static void pd_conjoint_bytes(const void* from, void* to, size_t count) {
+#ifdef AMD64
+  (void)memmove(to, from, count);
+#else
+  // Includes a zero-count check.
+  intx temp = 0;
+  __asm__ volatile("        testl   %6,%6          ;"
+                   "        jz      13f            ;"
+                   "        cmpl    %4,%5          ;"
+                   "        leal    -1(%4,%6),%3   ;"
+                   "        jbe     1f             ;"
+                   "        cmpl    %7,%5          ;"
+                   "        jbe     8f             ;"
+                   "1:      cmpl    $3,%6          ;"
+                   "        jbe     6f             ;"
+                   "        movl    %6,%3          ;"
+                   "        movl    $4,%2          ;"
+                   "        subl    %4,%2          ;"
+                   "        andl    $3,%2          ;"
+                   "        jz      2f             ;"
+                   "        subl    %6,%3          ;"
+                   "        rep;    smovb          ;"
+                   "2:      movl    %7,%2          ;"
+                   "        shrl    $2,%2          ;"
+                   "        jz      5f             ;"
+                   "        cmpl    $32,%2         ;"
+                   "        ja      4f             ;"
+                   "        subl    %4,%1          ;"
+                   "3:      movl    (%4),%%edx     ;"
+                   "        movl    %%edx,(%5,%4,1);"
+                   "        addl    $4,%0          ;"
+                   "        subl    $1,%2           ;"
+                   "        jnz     3b             ;"
+                   "        addl    %4,%1          ;"
+                   "        jmp     5f             ;"
+                   "4:      rep;    smovl          ;"
+                   "5:      movl    %7,%2          ;"
+                   "        andl    $3,%2          ;"
+                   "        jz      13f            ;"
+                   "6:      xorl    %7,%3          ;"
+                   "7:      movb    (%4,%7,1),%%dl ;"
+                   "        movb    %%dl,(%5,%7,1) ;"
+                   "        addl    $1,%3          ;"
+                   "        subl    $1,%2           ;"
+                   "        jnz     7b             ;"
+                   "        jmp     13f            ;"
+                   "8:      std                    ;"
+                   "        cmpl    $12,%2         ;"
+                   "        ja      9f             ;"
+                   "        movl    %7,%0          ;"
+                   "        leal    -1(%6,%5),%1   ;"
+                   "        jmp     11f            ;"
+                   "9:      xchgl   %3,%2          ;"
+                   "        movl    %6,%0          ;"
+                   "        addl    $1,%2          ;"
+                   "        leal    -1(%7,%5),%1   ;"
+                   "        andl    $3,%2          ;"
+                   "        jz      10f            ;"
+                   "        subl    %6,%3          ;"
+                   "        rep;    smovb          ;"
+                   "10:     movl    %7,%2          ;"
+                   "        subl    $3,%0          ;"
+                   "        shrl    $2,%2          ;"
+                   "        subl    $3,%1          ;"
+                   "        rep;    smovl          ;"
+                   "        andl    $3,%3          ;"
+                   "        jz      12f            ;"
+                   "        movl    %7,%2          ;"
+                   "        addl    $3,%0          ;"
+                   "        addl    $3,%1          ;"
+                   "11:     rep;    smovb          ;"
+                   "12:     cld                    ;"
+                   "13:     nop                    ;"
+                   : "=S" (from), "=D" (to), "=c" (count), "=r" (temp)
+                   : "0"  (from), "1"  (to), "2"  (count), "3"  (temp)
+                   : "memory", "flags", "%edx");
+#endif // AMD64
+}
+
+static void pd_conjoint_bytes_atomic(const void* from, void* to, size_t count) {
+  pd_conjoint_bytes(from, to, count);
+}
+
+static void pd_conjoint_jshorts_atomic(const jshort* from, jshort* to, size_t count) {
+  _Copy_conjoint_jshorts_atomic(from, to, count);
+}
+
+static void pd_conjoint_jints_atomic(const jint* from, jint* to, size_t count) {
+#ifdef AMD64
+  _Copy_conjoint_jints_atomic(from, to, count);
+#else
+  assert(HeapWordSize == BytesPerInt, "heapwords and jints must be the same size");
+  // pd_conjoint_words is word-atomic in this implementation.
+  pd_conjoint_words((const HeapWord*)from, (HeapWord*)to, count);
+#endif // AMD64
+}
+
+static void pd_conjoint_jlongs_atomic(const jlong* from, jlong* to, size_t count) {
+#ifdef AMD64
+  _Copy_conjoint_jlongs_atomic(from, to, count);
+#else
+  // Guarantee use of fild/fistp or xmm regs via some asm code, because compilers won't.
+  if (from > to) {
+    while (count-- > 0) {
+      __asm__ volatile("fildll (%0); fistpll (%1)"
+                       :
+                       : "r" (from), "r" (to)
+                       : "memory" );
+      ++from;
+      ++to;
+    }
+  } else {
+    while (count-- > 0) {
+      __asm__ volatile("fildll (%0,%2,8); fistpll (%1,%2,8)"
+                       :
+                       : "r" (from), "r" (to), "r" (count)
+                       : "memory" );
+    }
+  }
+#endif // AMD64
+}
+
+static void pd_conjoint_oops_atomic(const oop* from, oop* to, size_t count) {
+#ifdef AMD64
+  assert(BytesPerLong == BytesPerOop, "jlongs and oops must be the same size");
+  _Copy_conjoint_jlongs_atomic((const jlong*)from, (jlong*)to, count);
+#else
+  assert(HeapWordSize == BytesPerOop, "heapwords and oops must be the same size");
+  // pd_conjoint_words is word-atomic in this implementation.
+  pd_conjoint_words((const HeapWord*)from, (HeapWord*)to, count);
+#endif // AMD64
+}
+
+static void pd_arrayof_conjoint_bytes(const HeapWord* from, HeapWord* to, size_t count) {
+  _Copy_arrayof_conjoint_bytes(from, to, count);
+}
+
+static void pd_arrayof_conjoint_jshorts(const HeapWord* from, HeapWord* to, size_t count) {
+  _Copy_arrayof_conjoint_jshorts(from, to, count);
+}
+
+static void pd_arrayof_conjoint_jints(const HeapWord* from, HeapWord* to, size_t count) {
+#ifdef AMD64
+   _Copy_arrayof_conjoint_jints(from, to, count);
+#else
+  pd_conjoint_jints_atomic((const jint*)from, (jint*)to, count);
+#endif // AMD64
+}
+
+static void pd_arrayof_conjoint_jlongs(const HeapWord* from, HeapWord* to, size_t count) {
+#ifdef AMD64
+  _Copy_arrayof_conjoint_jlongs(from, to, count);
+#else
+  pd_conjoint_jlongs_atomic((const jlong*)from, (jlong*)to, count);
+#endif // AMD64
+}
+
+static void pd_arrayof_conjoint_oops(const HeapWord* from, HeapWord* to, size_t count) {
+#ifdef AMD64
+  assert(BytesPerLong == BytesPerOop, "jlongs and oops must be the same size");
+  _Copy_arrayof_conjoint_jlongs(from, to, count);
+#else
+  pd_conjoint_oops_atomic((const oop*)from, (oop*)to, count);
+#endif // AMD64
+}
+
+#endif // OS_CPU_SERENITY_X86_COPY_SERENITY_X86_INLINE_HPP
diff --git a/src/hotspot/os_cpu/serenity_x86/gc/z/zSyscall_serenity_x86.hpp b/src/hotspot/os_cpu/serenity_x86/gc/z/zSyscall_serenity_x86.hpp
new file mode 100644
index 00000000000..15209cd4308
--- /dev/null
+++ b/src/hotspot/os_cpu/serenity_x86/gc/z/zSyscall_serenity_x86.hpp
@@ -0,0 +1,40 @@
+/*
+ * Copyright (c) 2019, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ */
+
+#ifndef OS_CPU_SERENITY_X86_GC_Z_ZSYSCALL_SERENITY_X86_HPP
+#define OS_CPU_SERENITY_X86_GC_Z_ZSYSCALL_SERENITY_X86_HPP
+
+#include <sys/syscall.h>
+
+//
+// Support for building on older Linux systems
+//
+
+#ifndef SYS_memfd_create
+#define SYS_memfd_create     319
+#endif
+#ifndef SYS_fallocate
+#define SYS_fallocate        285
+#endif
+
+#endif // OS_CPU_SERENITY_X86_GC_Z_ZSYSCALL_SERENITY_X86_HPP
diff --git a/src/hotspot/os_cpu/serenity_x86/globals_serenity_x86.hpp b/src/hotspot/os_cpu/serenity_x86/globals_serenity_x86.hpp
new file mode 100644
index 00000000000..6bec2c25b18
--- /dev/null
+++ b/src/hotspot/os_cpu/serenity_x86/globals_serenity_x86.hpp
@@ -0,0 +1,50 @@
+/*
+ * Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#ifndef OS_CPU_SERENITY_X86_GLOBALS_SERENITY_X86_HPP
+#define OS_CPU_SERENITY_X86_GLOBALS_SERENITY_X86_HPP
+
+// Sets the default values for platform dependent flags used by the runtime system.
+// (see globals.hpp)
+
+define_pd_global(bool, DontYieldALot,            false);
+#ifdef AMD64
+define_pd_global(intx, CompilerThreadStackSize,  1024);
+define_pd_global(intx, ThreadStackSize,          1024); // 0 => use system default
+define_pd_global(intx, VMThreadStackSize,        1024);
+#else
+define_pd_global(intx, CompilerThreadStackSize,  512);
+// ThreadStackSize 320 allows a couple of test cases to run while
+// keeping the number of threads that can be created high.  System
+// default ThreadStackSize appears to be 512 which is too big.
+define_pd_global(intx, ThreadStackSize,          320);
+define_pd_global(intx, VMThreadStackSize,        512);
+#endif // AMD64
+
+define_pd_global(size_t, JVMInvokeMethodSlack,   8192);
+
+// Used on 64 bit platforms for UseCompressedOops base address
+define_pd_global(size_t, HeapBaseMinAddress,     2*G);
+
+#endif // OS_CPU_SERENITY_X86_GLOBALS_SERENITY_X86_HPP
diff --git a/src/hotspot/os_cpu/serenity_x86/linux_x86_32.s b/src/hotspot/os_cpu/serenity_x86/linux_x86_32.s
new file mode 100644
index 00000000000..b1961848451
--- /dev/null
+++ b/src/hotspot/os_cpu/serenity_x86/linux_x86_32.s
@@ -0,0 +1,645 @@
+#
+# Copyright (c) 2004, 2017, Oracle and/or its affiliates. All rights reserved.
+# DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+#
+# This code is free software; you can redistribute it and/or modify it
+# under the terms of the GNU General Public License version 2 only, as
+# published by the Free Software Foundation.
+#
+# This code is distributed in the hope that it will be useful, but WITHOUT
+# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+# version 2 for more details (a copy is included in the LICENSE file that
+# accompanied this code).
+#
+# You should have received a copy of the GNU General Public License version
+# 2 along with this work; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+#
+# Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+# or visit www.oracle.com if you need additional information or have any
+# questions.
+#
+
+
+        # NOTE WELL!  The _Copy functions are called directly
+	# from server-compiler-generated code via CallLeafNoFP,
+	# which means that they *must* either not use floating
+	# point or use it in the same manner as does the server
+	# compiler.
+
+        .globl _Copy_conjoint_bytes
+        .globl _Copy_arrayof_conjoint_bytes
+        .globl _Copy_conjoint_jshorts_atomic
+	.globl _Copy_arrayof_conjoint_jshorts
+        .globl _Copy_conjoint_jints_atomic
+        .globl _Copy_arrayof_conjoint_jints
+	.globl _Copy_conjoint_jlongs_atomic
+	.globl _mmx_Copy_arrayof_conjoint_jshorts
+
+        .globl _Atomic_cmpxchg_long
+        .globl _Atomic_move_long
+
+	.text
+
+        .globl  SpinPause
+	.type   SpinPause,@function
+        .p2align 4,,15
+SpinPause:
+        rep
+        nop
+        movl    $1, %eax
+        ret
+
+        # Support for void Copy::conjoint_bytes(void* from,
+        #                                       void* to,
+        #                                       size_t count)
+        .p2align 4,,15
+	.type    _Copy_conjoint_bytes,@function
+_Copy_conjoint_bytes:
+        pushl    %esi
+        movl     4+12(%esp),%ecx      # count
+        pushl    %edi
+        movl     8+ 4(%esp),%esi      # from
+        movl     8+ 8(%esp),%edi      # to
+        cmpl     %esi,%edi
+        leal     -1(%esi,%ecx),%eax   # from + count - 1
+        jbe      cb_CopyRight
+        cmpl     %eax,%edi
+        jbe      cb_CopyLeft
+        # copy from low to high
+cb_CopyRight:
+        cmpl     $3,%ecx
+        jbe      5f                   # <= 3 bytes
+        # align source address at dword address boundary
+        movl     %ecx,%eax            # original count
+        movl     $4,%ecx
+        subl     %esi,%ecx
+        andl     $3,%ecx              # prefix byte count
+        jz       1f                   # no prefix
+        subl     %ecx,%eax            # byte count less prefix
+        # copy prefix
+        subl     %esi,%edi
+0:      movb     (%esi),%dl
+        movb     %dl,(%edi,%esi,1)
+        addl     $1,%esi
+        subl     $1,%ecx
+        jnz      0b
+        addl     %esi,%edi
+1:      movl     %eax,%ecx            # byte count less prefix
+        shrl     $2,%ecx              # dword count
+        jz       4f                   # no dwords to move
+        cmpl     $32,%ecx
+        jbe      2f                   # <= 32 dwords
+        # copy aligned dwords
+        rep;     smovl
+        jmp      4f
+        # copy aligned dwords
+2:      subl     %esi,%edi
+        .p2align 4,,15
+3:      movl     (%esi),%edx
+        movl     %edx,(%edi,%esi,1)
+        addl     $4,%esi
+        subl     $1,%ecx
+        jnz      3b
+        addl     %esi,%edi
+4:      movl     %eax,%ecx            # byte count less prefix
+5:      andl     $3,%ecx              # suffix byte count
+        jz       7f                   # no suffix
+        # copy suffix
+        xorl     %eax,%eax
+6:      movb     (%esi,%eax,1),%dl
+        movb     %dl,(%edi,%eax,1)
+        addl     $1,%eax
+        subl     $1,%ecx
+        jnz      6b
+7:      popl     %edi
+        popl     %esi
+        ret
+        # copy from high to low
+cb_CopyLeft:
+        std
+        leal     -4(%edi,%ecx),%edi   # to + count - 4
+        movl     %eax,%esi            # from + count - 1
+        movl     %ecx,%eax
+        subl     $3,%esi              # from + count - 4
+        cmpl     $3,%ecx
+        jbe      5f                   # <= 3 bytes
+1:      shrl     $2,%ecx              # dword count
+        jz       4f                   # no dwords to move
+        cmpl     $32,%ecx
+        ja       3f                   # > 32 dwords
+        # copy dwords, aligned or not
+        subl     %esi,%edi
+        .p2align 4,,15
+2:      movl     (%esi),%edx
+        movl     %edx,(%edi,%esi,1)
+        subl     $4,%esi
+        subl     $1,%ecx
+        jnz      2b
+        addl     %esi,%edi
+        jmp      4f
+        # copy dwords, aligned or not
+3:      rep;     smovl
+4:      movl     %eax,%ecx            # byte count
+5:      andl     $3,%ecx              # suffix byte count
+        jz       7f                   # no suffix
+        # copy suffix
+        subl     %esi,%edi
+        addl     $3,%esi
+6:      movb     (%esi),%dl
+        movb     %dl,(%edi,%esi,1)
+	subl     $1,%esi
+        subl     $1,%ecx
+        jnz      6b
+7:      cld
+        popl     %edi
+        popl     %esi
+        ret
+
+        # Support for void Copy::arrayof_conjoint_bytes(void* from,
+        #                                               void* to,
+        #                                               size_t count)
+        #
+        # Same as _Copy_conjoint_bytes, except no source alignment check.
+        .p2align 4,,15
+	.type    _Copy_arrayof_conjoint_bytes,@function
+_Copy_arrayof_conjoint_bytes:
+        pushl    %esi
+        movl     4+12(%esp),%ecx      # count
+        pushl    %edi
+        movl     8+ 4(%esp),%esi      # from
+        movl     8+ 8(%esp),%edi      # to
+        cmpl     %esi,%edi
+        leal     -1(%esi,%ecx),%eax   # from + count - 1
+        jbe      acb_CopyRight
+        cmpl     %eax,%edi
+        jbe      acb_CopyLeft
+        # copy from low to high
+acb_CopyRight:
+        cmpl     $3,%ecx
+        jbe      5f
+1:      movl     %ecx,%eax
+        shrl     $2,%ecx
+        jz       4f
+        cmpl     $32,%ecx
+        ja       3f
+        # copy aligned dwords
+        subl     %esi,%edi
+        .p2align 4,,15
+2:      movl     (%esi),%edx
+        movl     %edx,(%edi,%esi,1)
+        addl     $4,%esi
+        subl     $1,%ecx
+        jnz      2b
+        addl     %esi,%edi
+        jmp      4f
+        # copy aligned dwords
+3:      rep;     smovl
+4:      movl     %eax,%ecx
+5:      andl     $3,%ecx
+        jz       7f
+        # copy suffix
+        xorl     %eax,%eax
+6:      movb     (%esi,%eax,1),%dl
+        movb     %dl,(%edi,%eax,1)
+        addl     $1,%eax
+        subl     $1,%ecx
+        jnz      6b
+7:      popl     %edi
+        popl     %esi
+        ret
+acb_CopyLeft:
+        std
+        leal     -4(%edi,%ecx),%edi   # to + count - 4
+        movl     %eax,%esi            # from + count - 1
+        movl     %ecx,%eax
+        subl     $3,%esi              # from + count - 4
+        cmpl     $3,%ecx
+        jbe      5f
+1:      shrl     $2,%ecx
+        jz       4f
+        cmpl     $32,%ecx
+        jbe      2f                   # <= 32 dwords
+        rep;     smovl
+        jmp      4f
+	.space 8
+2:      subl     %esi,%edi
+        .p2align 4,,15
+3:      movl     (%esi),%edx
+        movl     %edx,(%edi,%esi,1)
+        subl     $4,%esi
+        subl     $1,%ecx
+        jnz      3b
+        addl     %esi,%edi
+4:      movl     %eax,%ecx
+5:      andl     $3,%ecx
+        jz       7f
+        subl     %esi,%edi
+        addl     $3,%esi
+6:      movb     (%esi),%dl
+        movb     %dl,(%edi,%esi,1)
+	subl     $1,%esi
+        subl     $1,%ecx
+        jnz      6b
+7:      cld
+        popl     %edi
+        popl     %esi
+        ret
+
+        # Support for void Copy::conjoint_jshorts_atomic(void* from,
+        #                                                void* to,
+        #                                                size_t count)
+        .p2align 4,,15
+	.type    _Copy_conjoint_jshorts_atomic,@function
+_Copy_conjoint_jshorts_atomic:
+        pushl    %esi
+        movl     4+12(%esp),%ecx      # count
+        pushl    %edi
+        movl     8+ 4(%esp),%esi      # from
+        movl     8+ 8(%esp),%edi      # to
+        cmpl     %esi,%edi
+        leal     -2(%esi,%ecx,2),%eax # from + count*2 - 2
+        jbe      cs_CopyRight
+        cmpl     %eax,%edi
+        jbe      cs_CopyLeft
+        # copy from low to high
+cs_CopyRight:
+        # align source address at dword address boundary
+        movl     %esi,%eax            # original from
+        andl     $3,%eax              # either 0 or 2
+        jz       1f                   # no prefix
+        # copy prefix
+        subl     $1,%ecx
+        jl       5f                   # zero count
+        movw     (%esi),%dx
+        movw     %dx,(%edi)
+        addl     %eax,%esi            # %eax == 2
+        addl     %eax,%edi
+1:      movl     %ecx,%eax            # word count less prefix
+        sarl     %ecx                 # dword count
+        jz       4f                   # no dwords to move
+        cmpl     $32,%ecx
+        jbe      2f                   # <= 32 dwords
+        # copy aligned dwords
+        rep;     smovl
+        jmp      4f
+        # copy aligned dwords
+2:      subl     %esi,%edi
+        .p2align 4,,15
+3:      movl     (%esi),%edx
+        movl     %edx,(%edi,%esi,1)
+        addl     $4,%esi
+        subl     $1,%ecx
+        jnz      3b
+        addl     %esi,%edi
+4:      andl     $1,%eax              # suffix count
+        jz       5f                   # no suffix
+        # copy suffix
+        movw     (%esi),%dx
+        movw     %dx,(%edi)
+5:      popl     %edi
+        popl     %esi
+        ret
+        # copy from high to low
+cs_CopyLeft:
+        std
+        leal     -4(%edi,%ecx,2),%edi # to + count*2 - 4
+        movl     %eax,%esi            # from + count*2 - 2
+        movl     %ecx,%eax
+        subl     $2,%esi              # from + count*2 - 4
+1:      sarl     %ecx                 # dword count
+        jz       4f                   # no dwords to move
+        cmpl     $32,%ecx
+        ja       3f                   # > 32 dwords
+        subl     %esi,%edi
+        .p2align 4,,15
+2:      movl     (%esi),%edx
+        movl     %edx,(%edi,%esi,1)
+        subl     $4,%esi
+        subl     $1,%ecx
+        jnz      2b
+        addl     %esi,%edi
+        jmp      4f
+3:      rep;     smovl
+4:      andl     $1,%eax              # suffix count
+        jz       5f                   # no suffix
+        # copy suffix
+        addl     $2,%esi
+        addl     $2,%edi
+        movw     (%esi),%dx
+        movw     %dx,(%edi)
+5:      cld
+        popl     %edi
+        popl     %esi
+        ret
+
+        # Support for void Copy::arrayof_conjoint_jshorts(void* from,
+        #                                                 void* to,
+        #                                                 size_t count)
+        .p2align 4,,15
+	.type    _Copy_arrayof_conjoint_jshorts,@function
+_Copy_arrayof_conjoint_jshorts:
+        pushl    %esi
+        movl     4+12(%esp),%ecx      # count
+        pushl    %edi
+        movl     8+ 4(%esp),%esi      # from
+        movl     8+ 8(%esp),%edi      # to
+        cmpl     %esi,%edi
+        leal     -2(%esi,%ecx,2),%eax # from + count*2 - 2
+        jbe      acs_CopyRight
+        cmpl     %eax,%edi
+        jbe      acs_CopyLeft
+acs_CopyRight:
+        movl     %ecx,%eax            # word count
+        sarl     %ecx                 # dword count
+        jz       4f                   # no dwords to move
+        cmpl     $32,%ecx
+        jbe      2f                   # <= 32 dwords
+        # copy aligned dwords
+        rep;     smovl
+        jmp      4f
+        # copy aligned dwords
+        .space 5
+2:      subl     %esi,%edi
+        .p2align 4,,15
+3:      movl     (%esi),%edx
+        movl     %edx,(%edi,%esi,1)
+        addl     $4,%esi
+        subl     $1,%ecx
+        jnz      3b
+        addl     %esi,%edi
+4:      andl     $1,%eax              # suffix count
+        jz       5f                   # no suffix
+        # copy suffix
+        movw     (%esi),%dx
+        movw     %dx,(%edi)
+5:      popl     %edi
+        popl     %esi
+        ret
+acs_CopyLeft:
+        std
+        leal     -4(%edi,%ecx,2),%edi # to + count*2 - 4
+        movl     %eax,%esi            # from + count*2 - 2
+        movl     %ecx,%eax
+        subl     $2,%esi              # from + count*2 - 4
+        sarl     %ecx                 # dword count
+        jz       4f                   # no dwords to move
+        cmpl     $32,%ecx
+        ja       3f                   # > 32 dwords
+        subl     %esi,%edi
+        .p2align 4,,15
+2:      movl     (%esi),%edx
+        movl     %edx,(%edi,%esi,1)
+        subl     $4,%esi
+        subl     $1,%ecx
+        jnz      2b
+        addl     %esi,%edi
+        jmp      4f
+3:      rep;     smovl
+4:      andl     $1,%eax              # suffix count
+        jz       5f                   # no suffix
+        # copy suffix
+        addl     $2,%esi
+        addl     $2,%edi
+        movw     (%esi),%dx
+        movw     %dx,(%edi)
+5:      cld
+        popl     %edi
+        popl     %esi
+        ret
+
+        # Support for void Copy::conjoint_jints_atomic(void* from,
+        #                                              void* to,
+        #                                              size_t count)
+        # Equivalent to
+        #   arrayof_conjoint_jints
+        .p2align 4,,15
+	.type    _Copy_conjoint_jints_atomic,@function
+	.type    _Copy_arrayof_conjoint_jints,@function
+_Copy_conjoint_jints_atomic:
+_Copy_arrayof_conjoint_jints:
+        pushl    %esi
+        movl     4+12(%esp),%ecx      # count
+        pushl    %edi
+        movl     8+ 4(%esp),%esi      # from
+        movl     8+ 8(%esp),%edi      # to
+        cmpl     %esi,%edi
+        leal     -4(%esi,%ecx,4),%eax # from + count*4 - 4
+        jbe      ci_CopyRight
+        cmpl     %eax,%edi
+        jbe      ci_CopyLeft
+ci_CopyRight:
+        cmpl     $32,%ecx
+        jbe      2f                   # <= 32 dwords
+        rep;     smovl
+        popl     %edi
+        popl     %esi
+        ret
+        .space 10
+2:      subl     %esi,%edi
+        jmp      4f
+        .p2align 4,,15
+3:      movl     (%esi),%edx
+        movl     %edx,(%edi,%esi,1)
+        addl     $4,%esi
+4:      subl     $1,%ecx
+        jge      3b
+        popl     %edi
+        popl     %esi
+        ret
+ci_CopyLeft:
+        std
+        leal     -4(%edi,%ecx,4),%edi # to + count*4 - 4
+        cmpl     $32,%ecx
+        ja       4f                   # > 32 dwords
+        subl     %eax,%edi            # eax == from + count*4 - 4
+        jmp      3f
+        .p2align 4,,15
+2:      movl     (%eax),%edx
+        movl     %edx,(%edi,%eax,1)
+        subl     $4,%eax
+3:      subl     $1,%ecx
+        jge      2b
+        cld
+        popl     %edi
+        popl     %esi
+        ret
+4:      movl     %eax,%esi            # from + count*4 - 4
+        rep;     smovl
+        cld
+        popl     %edi
+        popl     %esi
+        ret
+
+        # Support for void Copy::conjoint_jlongs_atomic(jlong* from,
+        #                                               jlong* to,
+        #                                               size_t count)
+        #
+        # 32-bit
+        #
+        # count treated as signed
+        #
+        # if (from > to) {
+        #   while (--count >= 0) {
+        #     *to++ = *from++;
+        #   }
+        # } else {
+        #   while (--count >= 0) {
+        #     to[count] = from[count];
+        #   }
+        # }
+        .p2align 4,,15
+	.type    _Copy_conjoint_jlongs_atomic,@function
+_Copy_conjoint_jlongs_atomic:
+        movl     4+8(%esp),%ecx       # count
+        movl     4+0(%esp),%eax       # from
+        movl     4+4(%esp),%edx       # to
+        cmpl     %eax,%edx
+        jae      cla_CopyLeft
+cla_CopyRight:
+        subl     %eax,%edx
+        jmp      2f
+        .p2align 4,,15
+1:      fildll   (%eax)
+        fistpll  (%edx,%eax,1)
+        addl     $8,%eax
+2:      subl     $1,%ecx
+        jge      1b
+        ret
+        .p2align 4,,15
+3:      fildll   (%eax,%ecx,8)
+        fistpll  (%edx,%ecx,8)
+cla_CopyLeft:
+        subl     $1,%ecx
+        jge      3b
+        ret
+
+        # Support for void Copy::arrayof_conjoint_jshorts(void* from,
+        #                                                 void* to,
+        #                                                 size_t count)
+        .p2align 4,,15
+	.type    _mmx_Copy_arrayof_conjoint_jshorts,@function
+_mmx_Copy_arrayof_conjoint_jshorts:
+        pushl    %esi
+        movl     4+12(%esp),%ecx
+        pushl    %edi
+        movl     8+ 4(%esp),%esi
+        movl     8+ 8(%esp),%edi
+        cmpl     %esi,%edi
+        leal     -2(%esi,%ecx,2),%eax
+        jbe      mmx_acs_CopyRight
+        cmpl     %eax,%edi
+        jbe      mmx_acs_CopyLeft
+mmx_acs_CopyRight:
+        movl     %ecx,%eax
+        sarl     %ecx
+        je       5f
+        cmpl     $33,%ecx
+        jae      3f
+1:      subl     %esi,%edi
+        .p2align 4,,15
+2:      movl     (%esi),%edx
+        movl     %edx,(%edi,%esi,1)
+        addl     $4,%esi
+        subl     $1,%ecx
+        jnz      2b
+        addl     %esi,%edi
+        jmp      5f
+3:      smovl # align to 8 bytes, we know we are 4 byte aligned to start
+        subl     $1,%ecx
+4:      .p2align 4,,15
+        movq     0(%esi),%mm0
+        addl     $64,%edi
+        movq     8(%esi),%mm1
+        subl     $16,%ecx
+        movq     16(%esi),%mm2
+        movq     %mm0,-64(%edi)
+        movq     24(%esi),%mm0
+        movq     %mm1,-56(%edi)
+        movq     32(%esi),%mm1
+        movq     %mm2,-48(%edi)
+        movq     40(%esi),%mm2
+        movq     %mm0,-40(%edi)
+        movq     48(%esi),%mm0
+        movq     %mm1,-32(%edi)
+        movq     56(%esi),%mm1
+        movq     %mm2,-24(%edi)
+        movq     %mm0,-16(%edi)
+        addl     $64,%esi
+        movq     %mm1,-8(%edi)
+        cmpl     $16,%ecx
+        jge      4b
+        emms
+	testl    %ecx,%ecx
+	ja       1b
+5:      andl     $1,%eax
+        je       7f
+6:      movw     (%esi),%dx
+        movw     %dx,(%edi)
+7:	popl     %edi
+        popl     %esi
+        ret
+mmx_acs_CopyLeft:
+        std
+        leal     -4(%edi,%ecx,2),%edi
+        movl     %eax,%esi
+        movl     %ecx,%eax
+        subl     $2,%esi
+        sarl     %ecx
+        je       4f
+        cmpl     $32,%ecx
+        ja       3f
+        subl     %esi,%edi
+        .p2align 4,,15
+2:      movl     (%esi),%edx
+        movl     %edx,(%edi,%esi,1)
+        subl     $4,%esi
+        subl     $1,%ecx
+        jnz      2b
+        addl     %esi,%edi
+        jmp      4f
+3:      rep;     smovl
+4:      andl     $1,%eax
+        je       6f
+        addl     $2,%esi
+        addl     $2,%edi
+5:      movw     (%esi),%dx
+        movw     %dx,(%edi)
+6:      cld
+        popl     %edi
+        popl     %esi
+        ret
+
+
+        # Support for jlong Atomic::cmpxchg(volatile jlong* dest,
+        #                                   jlong compare_value,
+        #                                   jlong exchange_value)
+        #
+        .p2align 4,,15
+	.type    _Atomic_cmpxchg_long,@function
+_Atomic_cmpxchg_long:
+                                   #  8(%esp) : return PC
+        pushl    %ebx              #  4(%esp) : old %ebx
+        pushl    %edi              #  0(%esp) : old %edi
+        movl     12(%esp), %ebx    # 12(%esp) : exchange_value (low)
+        movl     16(%esp), %ecx    # 16(%esp) : exchange_value (high)
+        movl     24(%esp), %eax    # 24(%esp) : compare_value (low)
+        movl     28(%esp), %edx    # 28(%esp) : compare_value (high)
+        movl     20(%esp), %edi    # 20(%esp) : dest
+        lock cmpxchg8b (%edi)
+        popl     %edi
+        popl     %ebx
+        ret
+
+
+        # Support for jlong Atomic::load and Atomic::store.
+        # void _Atomic_move_long(const volatile jlong* src, volatile jlong* dst)
+        .p2align 4,,15
+	.type    _Atomic_move_long,@function
+_Atomic_move_long:
+        movl     4(%esp), %eax   # src
+        fildll    (%eax)
+        movl     8(%esp), %eax   # dest
+        fistpll   (%eax)
+        ret
diff --git a/src/hotspot/os_cpu/serenity_x86/linux_x86_64.s b/src/hotspot/os_cpu/serenity_x86/linux_x86_64.s
new file mode 100644
index 00000000000..89d98cb5837
--- /dev/null
+++ b/src/hotspot/os_cpu/serenity_x86/linux_x86_64.s
@@ -0,0 +1,380 @@
+# 
+# Copyright (c) 2004, 2013, Oracle and/or its affiliates. All rights reserved.
+# DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+#
+# This code is free software; you can redistribute it and/or modify it
+# under the terms of the GNU General Public License version 2 only, as
+# published by the Free Software Foundation.
+#
+# This code is distributed in the hope that it will be useful, but WITHOUT
+# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+# version 2 for more details (a copy is included in the LICENSE file that
+# accompanied this code).
+#
+# You should have received a copy of the GNU General Public License version
+# 2 along with this work; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+#
+# Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+# or visit www.oracle.com if you need additional information or have any
+# questions.
+#
+
+
+        # NOTE WELL!  The _Copy functions are called directly
+	# from server-compiler-generated code via CallLeafNoFP,
+	# which means that they *must* either not use floating
+	# point or use it in the same manner as does the server
+	# compiler.
+	
+        .globl _Copy_arrayof_conjoint_bytes
+	.globl _Copy_arrayof_conjoint_jshorts
+        .globl _Copy_conjoint_jshorts_atomic
+        .globl _Copy_arrayof_conjoint_jints
+        .globl _Copy_conjoint_jints_atomic
+        .globl _Copy_arrayof_conjoint_jlongs
+        .globl _Copy_conjoint_jlongs_atomic
+
+	.text
+
+        .globl SpinPause
+        .align 16
+        .type  SpinPause,@function
+SpinPause:
+        rep
+        nop
+        movq   $1, %rax
+        ret
+
+        # Support for void Copy::arrayof_conjoint_bytes(void* from,
+        #                                               void* to,
+        #                                               size_t count)
+        # rdi - from
+        # rsi - to
+        # rdx - count, treated as ssize_t
+        #
+        .p2align 4,,15
+	.type    _Copy_arrayof_conjoint_bytes,@function
+_Copy_arrayof_conjoint_bytes:
+        movq     %rdx,%r8             # byte count
+        shrq     $3,%rdx              # qword count
+        cmpq     %rdi,%rsi
+        leaq     -1(%rdi,%r8,1),%rax  # from + bcount*1 - 1
+        jbe      acb_CopyRight
+        cmpq     %rax,%rsi
+        jbe      acb_CopyLeft 
+acb_CopyRight:
+        leaq     -8(%rdi,%rdx,8),%rax # from + qcount*8 - 8
+        leaq     -8(%rsi,%rdx,8),%rcx # to + qcount*8 - 8
+        negq     %rdx
+        jmp      7f
+        .p2align 4,,15
+1:      movq     8(%rax,%rdx,8),%rsi
+        movq     %rsi,8(%rcx,%rdx,8)
+        addq     $1,%rdx
+        jnz      1b
+2:      testq    $4,%r8               # check for trailing dword
+        jz       3f
+        movl     8(%rax),%esi         # copy trailing dword
+        movl     %esi,8(%rcx)
+        addq     $4,%rax
+        addq     $4,%rcx              # original %rsi is trashed, so we
+                                      #  can't use it as a base register
+3:      testq    $2,%r8               # check for trailing word
+        jz       4f
+        movw     8(%rax),%si          # copy trailing word
+        movw     %si,8(%rcx)
+        addq     $2,%rcx
+4:      testq    $1,%r8               # check for trailing byte
+        jz       5f
+        movb     -1(%rdi,%r8,1),%al   # copy trailing byte
+        movb     %al,8(%rcx)
+5:      ret
+        .p2align 4,,15
+6:      movq     -24(%rax,%rdx,8),%rsi
+        movq     %rsi,-24(%rcx,%rdx,8)
+        movq     -16(%rax,%rdx,8),%rsi
+        movq     %rsi,-16(%rcx,%rdx,8)
+        movq     -8(%rax,%rdx,8),%rsi
+        movq     %rsi,-8(%rcx,%rdx,8)
+        movq     (%rax,%rdx,8),%rsi
+        movq     %rsi,(%rcx,%rdx,8)
+7:      addq     $4,%rdx
+        jle      6b
+        subq     $4,%rdx
+        jl       1b
+        jmp      2b
+acb_CopyLeft:
+        testq    $1,%r8               # check for trailing byte
+        jz       1f
+        movb     -1(%rdi,%r8,1),%cl   # copy trailing byte
+        movb     %cl,-1(%rsi,%r8,1)
+        subq     $1,%r8               # adjust for possible trailing word
+1:      testq    $2,%r8               # check for trailing word
+        jz       2f
+        movw     -2(%rdi,%r8,1),%cx   # copy trailing word
+        movw     %cx,-2(%rsi,%r8,1)
+2:      testq    $4,%r8               # check for trailing dword
+        jz       5f
+        movl     (%rdi,%rdx,8),%ecx   # copy trailing dword
+        movl     %ecx,(%rsi,%rdx,8)
+        jmp      5f
+        .p2align 4,,15
+3:      movq     -8(%rdi,%rdx,8),%rcx
+        movq     %rcx,-8(%rsi,%rdx,8)
+        subq     $1,%rdx
+        jnz      3b
+        ret
+        .p2align 4,,15
+4:      movq     24(%rdi,%rdx,8),%rcx
+        movq     %rcx,24(%rsi,%rdx,8)
+        movq     16(%rdi,%rdx,8),%rcx
+        movq     %rcx,16(%rsi,%rdx,8)
+        movq     8(%rdi,%rdx,8),%rcx
+        movq     %rcx,8(%rsi,%rdx,8)
+        movq     (%rdi,%rdx,8),%rcx
+        movq     %rcx,(%rsi,%rdx,8)
+5:      subq     $4,%rdx
+        jge      4b
+        addq     $4,%rdx
+        jg       3b
+        ret
+
+        # Support for void Copy::arrayof_conjoint_jshorts(void* from,
+        #                                                 void* to,
+        #                                                 size_t count)
+        # Equivalent to
+        #   conjoint_jshorts_atomic
+        #
+        # If 'from' and/or 'to' are aligned on 4- or 2-byte boundaries, we
+        # let the hardware handle it.  The tow or four words within dwords
+        # or qwords that span cache line boundaries will still be loaded
+        # and stored atomically.
+        #
+        # rdi - from
+        # rsi - to
+        # rdx - count, treated as ssize_t
+        #
+        .p2align 4,,15
+	.type    _Copy_arrayof_conjoint_jshorts,@function
+	.type    _Copy_conjoint_jshorts_atomic,@function
+_Copy_arrayof_conjoint_jshorts:
+_Copy_conjoint_jshorts_atomic:
+        movq     %rdx,%r8             # word count
+        shrq     $2,%rdx              # qword count
+        cmpq     %rdi,%rsi
+        leaq     -2(%rdi,%r8,2),%rax  # from + wcount*2 - 2
+        jbe      acs_CopyRight
+        cmpq     %rax,%rsi
+        jbe      acs_CopyLeft 
+acs_CopyRight:
+        leaq     -8(%rdi,%rdx,8),%rax # from + qcount*8 - 8
+        leaq     -8(%rsi,%rdx,8),%rcx # to + qcount*8 - 8
+        negq     %rdx
+        jmp      6f
+1:      movq     8(%rax,%rdx,8),%rsi
+        movq     %rsi,8(%rcx,%rdx,8)
+        addq     $1,%rdx
+        jnz      1b
+2:      testq    $2,%r8               # check for trailing dword
+        jz       3f
+        movl     8(%rax),%esi         # copy trailing dword
+        movl     %esi,8(%rcx)
+        addq     $4,%rcx              # original %rsi is trashed, so we
+                                      #  can't use it as a base register
+3:      testq    $1,%r8               # check for trailing word
+        jz       4f
+        movw     -2(%rdi,%r8,2),%si   # copy trailing word
+        movw     %si,8(%rcx)
+4:      ret
+        .p2align 4,,15
+5:      movq     -24(%rax,%rdx,8),%rsi
+        movq     %rsi,-24(%rcx,%rdx,8)
+        movq     -16(%rax,%rdx,8),%rsi
+        movq     %rsi,-16(%rcx,%rdx,8)
+        movq     -8(%rax,%rdx,8),%rsi
+        movq     %rsi,-8(%rcx,%rdx,8)
+        movq     (%rax,%rdx,8),%rsi
+        movq     %rsi,(%rcx,%rdx,8)
+6:      addq     $4,%rdx
+        jle      5b
+        subq     $4,%rdx
+        jl       1b
+        jmp      2b
+acs_CopyLeft:
+        testq    $1,%r8               # check for trailing word
+        jz       1f
+        movw     -2(%rdi,%r8,2),%cx   # copy trailing word
+        movw     %cx,-2(%rsi,%r8,2)
+1:      testq    $2,%r8               # check for trailing dword
+        jz       4f
+        movl     (%rdi,%rdx,8),%ecx   # copy trailing dword
+        movl     %ecx,(%rsi,%rdx,8)
+        jmp      4f
+2:      movq     -8(%rdi,%rdx,8),%rcx
+        movq     %rcx,-8(%rsi,%rdx,8)
+        subq     $1,%rdx
+        jnz      2b
+        ret
+        .p2align 4,,15
+3:      movq     24(%rdi,%rdx,8),%rcx
+        movq     %rcx,24(%rsi,%rdx,8)
+        movq     16(%rdi,%rdx,8),%rcx
+        movq     %rcx,16(%rsi,%rdx,8)
+        movq     8(%rdi,%rdx,8),%rcx
+        movq     %rcx,8(%rsi,%rdx,8)
+        movq     (%rdi,%rdx,8),%rcx
+        movq     %rcx,(%rsi,%rdx,8)
+4:      subq     $4,%rdx
+        jge      3b
+        addq     $4,%rdx
+        jg       2b
+        ret
+
+        # Support for void Copy::arrayof_conjoint_jints(jint* from,
+        #                                               jint* to,
+        #                                               size_t count)
+        # Equivalent to
+        #   conjoint_jints_atomic
+        #
+        # If 'from' and/or 'to' are aligned on 4-byte boundaries, we let
+        # the hardware handle it.  The two dwords within qwords that span
+        # cache line boundaries will still be loaded and stored atomically.
+        #
+        # rdi - from
+        # rsi - to
+        # rdx - count, treated as ssize_t
+        #
+        .p2align 4,,15
+	.type    _Copy_arrayof_conjoint_jints,@function
+	.type    _Copy_conjoint_jints_atomic,@function
+_Copy_arrayof_conjoint_jints:
+_Copy_conjoint_jints_atomic:
+        movq     %rdx,%r8             # dword count
+        shrq     %rdx                 # qword count
+        cmpq     %rdi,%rsi
+        leaq     -4(%rdi,%r8,4),%rax  # from + dcount*4 - 4
+        jbe      aci_CopyRight
+        cmpq     %rax,%rsi
+        jbe      aci_CopyLeft 
+aci_CopyRight:
+        leaq     -8(%rdi,%rdx,8),%rax # from + qcount*8 - 8
+        leaq     -8(%rsi,%rdx,8),%rcx # to + qcount*8 - 8
+        negq     %rdx
+        jmp      5f
+        .p2align 4,,15
+1:      movq     8(%rax,%rdx,8),%rsi
+        movq     %rsi,8(%rcx,%rdx,8)
+        addq     $1,%rdx
+        jnz       1b
+2:      testq    $1,%r8               # check for trailing dword
+        jz       3f
+        movl     8(%rax),%esi         # copy trailing dword
+        movl     %esi,8(%rcx)
+3:      ret
+        .p2align 4,,15
+4:      movq     -24(%rax,%rdx,8),%rsi
+        movq     %rsi,-24(%rcx,%rdx,8)
+        movq     -16(%rax,%rdx,8),%rsi
+        movq     %rsi,-16(%rcx,%rdx,8)
+        movq     -8(%rax,%rdx,8),%rsi
+        movq     %rsi,-8(%rcx,%rdx,8)
+        movq     (%rax,%rdx,8),%rsi
+        movq     %rsi,(%rcx,%rdx,8)
+5:      addq     $4,%rdx
+        jle      4b
+        subq     $4,%rdx
+        jl       1b
+        jmp      2b
+aci_CopyLeft:
+        testq    $1,%r8               # check for trailing dword
+        jz       3f
+        movl     -4(%rdi,%r8,4),%ecx  # copy trailing dword
+        movl     %ecx,-4(%rsi,%r8,4)
+        jmp      3f
+1:      movq     -8(%rdi,%rdx,8),%rcx
+        movq     %rcx,-8(%rsi,%rdx,8)
+        subq     $1,%rdx
+        jnz      1b
+        ret
+        .p2align 4,,15
+2:      movq     24(%rdi,%rdx,8),%rcx
+        movq     %rcx,24(%rsi,%rdx,8)
+        movq     16(%rdi,%rdx,8),%rcx
+        movq     %rcx,16(%rsi,%rdx,8)
+        movq     8(%rdi,%rdx,8),%rcx
+        movq     %rcx,8(%rsi,%rdx,8)
+        movq     (%rdi,%rdx,8),%rcx
+        movq     %rcx,(%rsi,%rdx,8)
+3:      subq     $4,%rdx
+        jge      2b
+        addq     $4,%rdx
+        jg       1b
+        ret
+
+        # Support for void Copy::arrayof_conjoint_jlongs(jlong* from,
+        #                                                jlong* to,
+        #                                                size_t count)
+        # Equivalent to
+        #   conjoint_jlongs_atomic
+        #   arrayof_conjoint_oops
+        #   conjoint_oops_atomic
+        #
+        # rdi - from
+        # rsi - to
+        # rdx - count, treated as ssize_t
+        #
+        .p2align 4,,15
+	.type    _Copy_arrayof_conjoint_jlongs,@function
+	.type    _Copy_conjoint_jlongs_atomic,@function
+_Copy_arrayof_conjoint_jlongs:
+_Copy_conjoint_jlongs_atomic:
+        cmpq     %rdi,%rsi
+        leaq     -8(%rdi,%rdx,8),%rax # from + count*8 - 8
+        jbe      acl_CopyRight
+        cmpq     %rax,%rsi
+        jbe      acl_CopyLeft 
+acl_CopyRight:
+        leaq     -8(%rsi,%rdx,8),%rcx # to + count*8 - 8
+        negq     %rdx
+        jmp      3f
+1:      movq     8(%rax,%rdx,8),%rsi
+        movq     %rsi,8(%rcx,%rdx,8)
+        addq     $1,%rdx
+        jnz      1b
+        ret
+        .p2align 4,,15
+2:      movq     -24(%rax,%rdx,8),%rsi
+        movq     %rsi,-24(%rcx,%rdx,8)
+        movq     -16(%rax,%rdx,8),%rsi
+        movq     %rsi,-16(%rcx,%rdx,8)
+        movq     -8(%rax,%rdx,8),%rsi
+        movq     %rsi,-8(%rcx,%rdx,8)
+        movq     (%rax,%rdx,8),%rsi
+        movq     %rsi,(%rcx,%rdx,8)
+3:      addq     $4,%rdx
+        jle      2b
+        subq     $4,%rdx
+        jl       1b
+        ret
+4:      movq     -8(%rdi,%rdx,8),%rcx
+        movq     %rcx,-8(%rsi,%rdx,8)
+        subq     $1,%rdx
+        jnz      4b
+        ret
+        .p2align 4,,15
+5:      movq     24(%rdi,%rdx,8),%rcx
+        movq     %rcx,24(%rsi,%rdx,8)
+        movq     16(%rdi,%rdx,8),%rcx
+        movq     %rcx,16(%rsi,%rdx,8)
+        movq     8(%rdi,%rdx,8),%rcx
+        movq     %rcx,8(%rsi,%rdx,8)
+        movq     (%rdi,%rdx,8),%rcx
+        movq     %rcx,(%rsi,%rdx,8)
+acl_CopyLeft:
+        subq     $4,%rdx
+        jge      5b
+        addq     $4,%rdx
+        jg       4b
+        ret
diff --git a/src/hotspot/os_cpu/serenity_x86/orderAccess_serenity_x86.hpp b/src/hotspot/os_cpu/serenity_x86/orderAccess_serenity_x86.hpp
new file mode 100644
index 00000000000..5cb2ab68cd3
--- /dev/null
+++ b/src/hotspot/os_cpu/serenity_x86/orderAccess_serenity_x86.hpp
@@ -0,0 +1,69 @@
+/*
+ * Copyright (c) 2003, 2020, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#ifndef OS_CPU_SERENITY_X86_ORDERACCESS_SERENITY_X86_HPP
+#define OS_CPU_SERENITY_X86_ORDERACCESS_SERENITY_X86_HPP
+
+// Included in orderAccess.hpp header file.
+
+// Compiler version last used for testing: gcc 4.8.2
+// Please update this information when this file changes
+
+// Implementation of class OrderAccess.
+
+// A compiler barrier, forcing the C++ compiler to invalidate all memory assumptions
+static inline void compiler_barrier() {
+  __asm__ volatile ("" : : : "memory");
+}
+
+inline void OrderAccess::loadload()   { compiler_barrier(); }
+inline void OrderAccess::storestore() { compiler_barrier(); }
+inline void OrderAccess::loadstore()  { compiler_barrier(); }
+inline void OrderAccess::storeload()  { fence();            }
+
+inline void OrderAccess::acquire()    { compiler_barrier(); }
+inline void OrderAccess::release()    { compiler_barrier(); }
+
+inline void OrderAccess::fence() {
+   // always use locked addl since mfence is sometimes expensive
+#ifdef AMD64
+  __asm__ volatile ("lock; addl $0,0(%%rsp)" : : : "cc", "memory");
+#else
+  __asm__ volatile ("lock; addl $0,0(%%esp)" : : : "cc", "memory");
+#endif
+  compiler_barrier();
+}
+
+inline void OrderAccess::cross_modify_fence_impl() {
+  int idx = 0;
+#ifdef AMD64
+  __asm__ volatile ("cpuid " : "+a" (idx) : : "ebx", "ecx", "edx", "memory");
+#else
+  // On some x86 systems EBX is a reserved register that cannot be
+  // clobbered, so we must protect it around the CPUID.
+  __asm__ volatile ("xchg %%esi, %%ebx; cpuid; xchg %%esi, %%ebx " : "+a" (idx) : : "esi", "ecx", "edx", "memory");
+#endif
+}
+
+#endif // OS_CPU_SERENITY_X86_ORDERACCESS_SERENITY_X86_HPP
diff --git a/src/hotspot/os_cpu/serenity_x86/os_serenity_x86.cpp b/src/hotspot/os_cpu/serenity_x86/os_serenity_x86.cpp
new file mode 100644
index 00000000000..3c5ea2983bc
--- /dev/null
+++ b/src/hotspot/os_cpu/serenity_x86/os_serenity_x86.cpp
@@ -0,0 +1,711 @@
+/*
+ * Copyright (c) 1999, 2021, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+// no precompiled headers
+#include "jvm.h"
+#include "asm/macroAssembler.hpp"
+#include "classfile/vmSymbols.hpp"
+#include "code/codeCache.hpp"
+#include "code/icBuffer.hpp"
+#include "code/vtableStubs.hpp"
+#include "interpreter/interpreter.hpp"
+#include "logging/log.hpp"
+#include "memory/allocation.inline.hpp"
+#include "os_share_linux.hpp"
+#include "prims/jniFastGetField.hpp"
+#include "prims/jvm_misc.hpp"
+#include "runtime/frame.inline.hpp"
+#include "runtime/interfaceSupport.inline.hpp"
+#include "runtime/java.hpp"
+#include "runtime/javaCalls.hpp"
+#include "runtime/mutexLocker.hpp"
+#include "runtime/osThread.hpp"
+#include "runtime/safepointMechanism.hpp"
+#include "runtime/sharedRuntime.hpp"
+#include "runtime/stubRoutines.hpp"
+#include "runtime/thread.inline.hpp"
+#include "runtime/timer.hpp"
+#include "signals_posix.hpp"
+#include "services/memTracker.hpp"
+#include "utilities/align.hpp"
+#include "utilities/debug.hpp"
+#include "utilities/events.hpp"
+#include "utilities/vmError.hpp"
+
+// put OS-includes here
+# include <sys/types.h>
+# include <sys/mman.h>
+# include <pthread.h>
+# include <signal.h>
+# include <errno.h>
+# include <dlfcn.h>
+# include <stdlib.h>
+# include <stdio.h>
+# include <unistd.h>
+# include <sys/resource.h>
+# include <pthread.h>
+# include <sys/stat.h>
+# include <sys/time.h>
+# include <sys/utsname.h>
+# include <sys/socket.h>
+# include <sys/wait.h>
+# include <pwd.h>
+# include <poll.h>
+# include <ucontext.h>
+#ifndef AMD64
+# include <fpu_control.h>
+#endif
+
+#ifdef AMD64
+#define REG_SP REG_RSP
+#define REG_PC REG_RIP
+#define REG_FP REG_RBP
+#define SPELL_REG_SP "rsp"
+#define SPELL_REG_FP "rbp"
+#else
+#define REG_SP REG_UESP
+#define REG_PC REG_EIP
+#define REG_FP REG_EBP
+#define SPELL_REG_SP "esp"
+#define SPELL_REG_FP "ebp"
+#endif // AMD64
+
+address os::current_stack_pointer() {
+  return (address)__builtin_frame_address(0);
+}
+
+char* os::non_memory_address_word() {
+  // Must never look like an address returned by reserve_memory,
+  // even in its subfields (as defined by the CPU immediate fields,
+  // if the CPU splits constants across multiple instructions).
+
+  return (char*) -1;
+}
+
+address os::Posix::ucontext_get_pc(const ucontext_t * uc) {
+  return (address)uc->uc_mcontext.gregs[REG_PC];
+}
+
+void os::Posix::ucontext_set_pc(ucontext_t * uc, address pc) {
+  uc->uc_mcontext.gregs[REG_PC] = (intptr_t)pc;
+}
+
+intptr_t* os::Linux::ucontext_get_sp(const ucontext_t * uc) {
+  return (intptr_t*)uc->uc_mcontext.gregs[REG_SP];
+}
+
+intptr_t* os::Linux::ucontext_get_fp(const ucontext_t * uc) {
+  return (intptr_t*)uc->uc_mcontext.gregs[REG_FP];
+}
+
+address os::fetch_frame_from_context(const void* ucVoid,
+                    intptr_t** ret_sp, intptr_t** ret_fp) {
+
+  address epc;
+  const ucontext_t* uc = (const ucontext_t*)ucVoid;
+
+  if (uc != NULL) {
+    epc = os::Posix::ucontext_get_pc(uc);
+    if (ret_sp) *ret_sp = os::Linux::ucontext_get_sp(uc);
+    if (ret_fp) *ret_fp = os::Linux::ucontext_get_fp(uc);
+  } else {
+    epc = NULL;
+    if (ret_sp) *ret_sp = (intptr_t *)NULL;
+    if (ret_fp) *ret_fp = (intptr_t *)NULL;
+  }
+
+  return epc;
+}
+
+frame os::fetch_frame_from_context(const void* ucVoid) {
+  intptr_t* sp;
+  intptr_t* fp;
+  address epc = fetch_frame_from_context(ucVoid, &sp, &fp);
+  return frame(sp, fp, epc);
+}
+
+frame os::fetch_compiled_frame_from_context(const void* ucVoid) {
+  const ucontext_t* uc = (const ucontext_t*)ucVoid;
+  intptr_t* fp = os::Linux::ucontext_get_fp(uc);
+  intptr_t* sp = os::Linux::ucontext_get_sp(uc);
+  return frame(sp + 1, fp, (address)*sp);
+}
+
+// By default, gcc always save frame pointer (%ebp/%rbp) on stack. It may get
+// turned off by -fomit-frame-pointer,
+frame os::get_sender_for_C_frame(frame* fr) {
+  return frame(fr->sender_sp(), fr->link(), fr->sender_pc());
+}
+
+intptr_t* _get_previous_fp() {
+#if defined(__clang__)
+  intptr_t **ebp;
+  __asm__ __volatile__ ("mov %%" SPELL_REG_FP ", %0":"=r"(ebp):);
+#else
+  register intptr_t **ebp __asm__ (SPELL_REG_FP);
+#endif
+  // ebp is for this frame (_get_previous_fp). We want the ebp for the
+  // caller of os::current_frame*(), so go up two frames. However, for
+  // optimized builds, _get_previous_fp() will be inlined, so only go
+  // up 1 frame in that case.
+#ifdef _NMT_NOINLINE_
+  return **(intptr_t***)ebp;
+#else
+  return *ebp;
+#endif
+}
+
+
+frame os::current_frame() {
+  intptr_t* fp = _get_previous_fp();
+  frame myframe((intptr_t*)os::current_stack_pointer(),
+                (intptr_t*)fp,
+                CAST_FROM_FN_PTR(address, os::current_frame));
+  if (os::is_first_C_frame(&myframe)) {
+    // stack is not walkable
+    return frame();
+  } else {
+    return os::get_sender_for_C_frame(&myframe);
+  }
+}
+
+// Utility functions
+
+// From IA32 System Programming Guide
+enum {
+  trap_page_fault = 0xE
+};
+
+bool PosixSignals::pd_hotspot_signal_handler(int sig, siginfo_t* info,
+                                             ucontext_t* uc, JavaThread* thread) {
+
+  /*
+  NOTE: does not seem to work on linux.
+  if (info == NULL || info->si_code <= 0 || info->si_code == SI_NOINFO) {
+    // can't decode this kind of signal
+    info = NULL;
+  } else {
+    assert(sig == info->si_signo, "bad siginfo");
+  }
+*/
+  // decide if this trap can be handled by a stub
+  address stub = NULL;
+
+  address pc          = NULL;
+
+  //%note os_trap_1
+  if (info != NULL && uc != NULL && thread != NULL) {
+    pc = (address) os::Posix::ucontext_get_pc(uc);
+
+#ifndef AMD64
+    // Halt if SI_KERNEL before more crashes get misdiagnosed as Java bugs
+    // This can happen in any running code (currently more frequently in
+    // interpreter code but has been seen in compiled code)
+    if (sig == SIGSEGV && info->si_addr == 0 && info->si_code == SI_KERNEL) {
+      fatal("An irrecoverable SI_KERNEL SIGSEGV has occurred due "
+            "to unstable signal handling in this distribution.");
+    }
+#endif // AMD64
+
+    // Handle ALL stack overflow variations here
+    if (sig == SIGSEGV) {
+      address addr = (address) info->si_addr;
+
+      // check if fault address is within thread stack
+      if (thread->is_in_full_stack(addr)) {
+        // stack overflow
+        if (os::Posix::handle_stack_overflow(thread, addr, pc, uc, &stub)) {
+          return true; // continue
+        }
+      }
+    }
+
+    if ((sig == SIGSEGV) && VM_Version::is_cpuinfo_segv_addr(pc)) {
+      // Verify that OS save/restore AVX registers.
+      stub = VM_Version::cpuinfo_cont_addr();
+    }
+
+    if (thread->thread_state() == _thread_in_Java) {
+      // Java thread running in Java code => find exception handler if any
+      // a fault inside compiled code, the interpreter, or a stub
+
+      if (sig == SIGSEGV && SafepointMechanism::is_poll_address((address)info->si_addr)) {
+        stub = SharedRuntime::get_poll_stub(pc);
+      } else if (sig == SIGBUS /* && info->si_code == BUS_OBJERR */) {
+        // BugId 4454115: A read from a MappedByteBuffer can fault
+        // here if the underlying file has been truncated.
+        // Do not crash the VM in such a case.
+        CodeBlob* cb = CodeCache::find_blob_unsafe(pc);
+        CompiledMethod* nm = (cb != NULL) ? cb->as_compiled_method_or_null() : NULL;
+        bool is_unsafe_arraycopy = thread->doing_unsafe_access() && UnsafeCopyMemory::contains_pc(pc);
+        if ((nm != NULL && nm->has_unsafe_access()) || is_unsafe_arraycopy) {
+          address next_pc = Assembler::locate_next_instruction(pc);
+          if (is_unsafe_arraycopy) {
+            next_pc = UnsafeCopyMemory::page_error_continue_pc(pc);
+          }
+          stub = SharedRuntime::handle_unsafe_access(thread, next_pc);
+        }
+      }
+      else
+
+#ifdef AMD64
+      if (sig == SIGFPE  &&
+          (info->si_code == FPE_INTDIV || info->si_code == FPE_FLTDIV)) {
+        stub =
+          SharedRuntime::
+          continuation_for_implicit_exception(thread,
+                                              pc,
+                                              SharedRuntime::
+                                              IMPLICIT_DIVIDE_BY_ZERO);
+#else
+      if (sig == SIGFPE /* && info->si_code == FPE_INTDIV */) {
+        // HACK: si_code does not work on linux 2.2.12-20!!!
+        int op = pc[0];
+        if (op == 0xDB) {
+          // FIST
+          // TODO: The encoding of D2I in x86_32.ad can cause an exception
+          // prior to the fist instruction if there was an invalid operation
+          // pending. We want to dismiss that exception. From the win_32
+          // side it also seems that if it really was the fist causing
+          // the exception that we do the d2i by hand with different
+          // rounding. Seems kind of weird.
+          // NOTE: that we take the exception at the NEXT floating point instruction.
+          assert(pc[0] == 0xDB, "not a FIST opcode");
+          assert(pc[1] == 0x14, "not a FIST opcode");
+          assert(pc[2] == 0x24, "not a FIST opcode");
+          return true;
+        } else if (op == 0xF7) {
+          // IDIV
+          stub = SharedRuntime::continuation_for_implicit_exception(thread, pc, SharedRuntime::IMPLICIT_DIVIDE_BY_ZERO);
+        } else {
+          // TODO: handle more cases if we are using other x86 instructions
+          //   that can generate SIGFPE signal on linux.
+          tty->print_cr("unknown opcode 0x%X with SIGFPE.", op);
+          fatal("please update this code.");
+        }
+#endif // AMD64
+      } else if (sig == SIGSEGV &&
+                 MacroAssembler::uses_implicit_null_check(info->si_addr)) {
+          // Determination of interpreter/vtable stub/compiled code null exception
+          stub = SharedRuntime::continuation_for_implicit_exception(thread, pc, SharedRuntime::IMPLICIT_NULL);
+      }
+    } else if ((thread->thread_state() == _thread_in_vm ||
+                thread->thread_state() == _thread_in_native) &&
+               (sig == SIGBUS && /* info->si_code == BUS_OBJERR && */
+               thread->doing_unsafe_access())) {
+        address next_pc = Assembler::locate_next_instruction(pc);
+        if (UnsafeCopyMemory::contains_pc(pc)) {
+          next_pc = UnsafeCopyMemory::page_error_continue_pc(pc);
+        }
+        stub = SharedRuntime::handle_unsafe_access(thread, next_pc);
+    }
+
+    // jni_fast_Get<Primitive>Field can trap at certain pc's if a GC kicks in
+    // and the heap gets shrunk before the field access.
+    if ((sig == SIGSEGV) || (sig == SIGBUS)) {
+      address addr = JNI_FastGetField::find_slowcase_pc(pc);
+      if (addr != (address)-1) {
+        stub = addr;
+      }
+    }
+  }
+
+#ifndef AMD64
+  // Execution protection violation
+  //
+  // This should be kept as the last step in the triage.  We don't
+  // have a dedicated trap number for a no-execute fault, so be
+  // conservative and allow other handlers the first shot.
+  //
+  // Note: We don't test that info->si_code == SEGV_ACCERR here.
+  // this si_code is so generic that it is almost meaningless; and
+  // the si_code for this condition may change in the future.
+  // Furthermore, a false-positive should be harmless.
+  if (UnguardOnExecutionViolation > 0 &&
+      (sig == SIGSEGV || sig == SIGBUS) &&
+      uc->uc_mcontext.gregs[REG_TRAPNO] == trap_page_fault) {
+    int page_size = os::vm_page_size();
+    address addr = (address) info->si_addr;
+    address pc = os::Posix::ucontext_get_pc(uc);
+    // Make sure the pc and the faulting address are sane.
+    //
+    // If an instruction spans a page boundary, and the page containing
+    // the beginning of the instruction is executable but the following
+    // page is not, the pc and the faulting address might be slightly
+    // different - we still want to unguard the 2nd page in this case.
+    //
+    // 15 bytes seems to be a (very) safe value for max instruction size.
+    bool pc_is_near_addr =
+      (pointer_delta((void*) addr, (void*) pc, sizeof(char)) < 15);
+    bool instr_spans_page_boundary =
+      (align_down((intptr_t) pc ^ (intptr_t) addr,
+                       (intptr_t) page_size) > 0);
+
+    if (pc == addr || (pc_is_near_addr && instr_spans_page_boundary)) {
+      static volatile address last_addr =
+        (address) os::non_memory_address_word();
+
+      // In conservative mode, don't unguard unless the address is in the VM
+      if (addr != last_addr &&
+          (UnguardOnExecutionViolation > 1 || os::address_is_in_vm(addr))) {
+
+        // Set memory to RWX and retry
+        address page_start = align_down(addr, page_size);
+        bool res = os::protect_memory((char*) page_start, page_size,
+                                      os::MEM_PROT_RWX);
+
+        log_debug(os)("Execution protection violation "
+                      "at " INTPTR_FORMAT
+                      ", unguarding " INTPTR_FORMAT ": %s, errno=%d", p2i(addr),
+                      p2i(page_start), (res ? "success" : "failed"), errno);
+        stub = pc;
+
+        // Set last_addr so if we fault again at the same address, we don't end
+        // up in an endless loop.
+        //
+        // There are two potential complications here.  Two threads trapping at
+        // the same address at the same time could cause one of the threads to
+        // think it already unguarded, and abort the VM.  Likely very rare.
+        //
+        // The other race involves two threads alternately trapping at
+        // different addresses and failing to unguard the page, resulting in
+        // an endless loop.  This condition is probably even more unlikely than
+        // the first.
+        //
+        // Although both cases could be avoided by using locks or thread local
+        // last_addr, these solutions are unnecessary complication: this
+        // handler is a best-effort safety net, not a complete solution.  It is
+        // disabled by default and should only be used as a workaround in case
+        // we missed any no-execute-unsafe VM code.
+
+        last_addr = addr;
+      }
+    }
+  }
+#endif // !AMD64
+
+  if (stub != NULL) {
+    // save all thread context in case we need to restore it
+    if (thread != NULL) thread->set_saved_exception_pc(pc);
+
+    os::Posix::ucontext_set_pc(uc, stub);
+    return true;
+  }
+
+  return false;
+}
+
+void os::Linux::init_thread_fpu_state(void) {
+#ifndef AMD64
+  // set fpu to 53 bit precision
+  set_fpu_control_word(0x27f);
+#endif // !AMD64
+}
+
+int os::Linux::get_fpu_control_word(void) {
+#ifdef AMD64
+  return 0;
+#else
+  int fpu_control;
+  _FPU_GETCW(fpu_control);
+  return fpu_control & 0xffff;
+#endif // AMD64
+}
+
+void os::Linux::set_fpu_control_word(int fpu_control) {
+#ifndef AMD64
+  _FPU_SETCW(fpu_control);
+#endif // !AMD64
+}
+
+// Check that the linux kernel version is 2.4 or higher since earlier
+// versions do not support SSE without patches.
+bool os::supports_sse() {
+#ifdef AMD64
+  return true;
+#else
+  struct utsname uts;
+  if( uname(&uts) != 0 ) return false; // uname fails?
+  char *minor_string;
+  int major = strtol(uts.release,&minor_string,10);
+  int minor = strtol(minor_string+1,NULL,10);
+  bool result = (major > 2 || (major==2 && minor >= 4));
+  log_info(os)("OS version is %d.%d, which %s support SSE/SSE2",
+               major,minor, result ? "DOES" : "does NOT");
+  return result;
+#endif // AMD64
+}
+
+juint os::cpu_microcode_revision() {
+  juint result = 0;
+  char data[2048] = {0}; // lines should fit in 2K buf
+  size_t len = sizeof(data);
+  FILE *fp = fopen("/proc/cpuinfo", "r");
+  if (fp) {
+    while (!feof(fp)) {
+      if (fgets(data, len, fp)) {
+        if (strstr(data, "microcode") != NULL) {
+          char* rev = strchr(data, ':');
+          if (rev != NULL) sscanf(rev + 1, "%x", &result);
+          break;
+        }
+      }
+    }
+    fclose(fp);
+  }
+  return result;
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// thread stack
+
+// Minimum usable stack sizes required to get to user code. Space for
+// HotSpot guard pages is added later.
+size_t os::Posix::_compiler_thread_min_stack_allowed = 48 * K;
+size_t os::Posix::_java_thread_min_stack_allowed = 40 * K;
+#ifdef _LP64
+size_t os::Posix::_vm_internal_thread_min_stack_allowed = 64 * K;
+#else
+size_t os::Posix::_vm_internal_thread_min_stack_allowed = (48 DEBUG_ONLY(+ 4)) * K;
+#endif // _LP64
+
+// return default stack size for thr_type
+size_t os::Posix::default_stack_size(os::ThreadType thr_type) {
+  // default stack size (compiler thread needs larger stack)
+#ifdef AMD64
+  size_t s = (thr_type == os::compiler_thread ? 4 * M : 1 * M);
+#else
+  size_t s = (thr_type == os::compiler_thread ? 2 * M : 512 * K);
+#endif // AMD64
+  return s;
+}
+
+/////////////////////////////////////////////////////////////////////////////
+// helper functions for fatal error handler
+
+void os::print_context(outputStream *st, const void *context) {
+  if (context == NULL) return;
+
+  const ucontext_t *uc = (const ucontext_t*)context;
+  st->print_cr("Registers:");
+#ifdef AMD64
+  st->print(  "RAX=" INTPTR_FORMAT, (intptr_t)uc->uc_mcontext.gregs[REG_RAX]);
+  st->print(", RBX=" INTPTR_FORMAT, (intptr_t)uc->uc_mcontext.gregs[REG_RBX]);
+  st->print(", RCX=" INTPTR_FORMAT, (intptr_t)uc->uc_mcontext.gregs[REG_RCX]);
+  st->print(", RDX=" INTPTR_FORMAT, (intptr_t)uc->uc_mcontext.gregs[REG_RDX]);
+  st->cr();
+  st->print(  "RSP=" INTPTR_FORMAT, (intptr_t)uc->uc_mcontext.gregs[REG_RSP]);
+  st->print(", RBP=" INTPTR_FORMAT, (intptr_t)uc->uc_mcontext.gregs[REG_RBP]);
+  st->print(", RSI=" INTPTR_FORMAT, (intptr_t)uc->uc_mcontext.gregs[REG_RSI]);
+  st->print(", RDI=" INTPTR_FORMAT, (intptr_t)uc->uc_mcontext.gregs[REG_RDI]);
+  st->cr();
+  st->print(  "R8 =" INTPTR_FORMAT, (intptr_t)uc->uc_mcontext.gregs[REG_R8]);
+  st->print(", R9 =" INTPTR_FORMAT, (intptr_t)uc->uc_mcontext.gregs[REG_R9]);
+  st->print(", R10=" INTPTR_FORMAT, (intptr_t)uc->uc_mcontext.gregs[REG_R10]);
+  st->print(", R11=" INTPTR_FORMAT, (intptr_t)uc->uc_mcontext.gregs[REG_R11]);
+  st->cr();
+  st->print(  "R12=" INTPTR_FORMAT, (intptr_t)uc->uc_mcontext.gregs[REG_R12]);
+  st->print(", R13=" INTPTR_FORMAT, (intptr_t)uc->uc_mcontext.gregs[REG_R13]);
+  st->print(", R14=" INTPTR_FORMAT, (intptr_t)uc->uc_mcontext.gregs[REG_R14]);
+  st->print(", R15=" INTPTR_FORMAT, (intptr_t)uc->uc_mcontext.gregs[REG_R15]);
+  st->cr();
+  st->print(  "RIP=" INTPTR_FORMAT, (intptr_t)uc->uc_mcontext.gregs[REG_RIP]);
+  st->print(", EFLAGS=" INTPTR_FORMAT, (intptr_t)uc->uc_mcontext.gregs[REG_EFL]);
+  st->print(", CSGSFS=" INTPTR_FORMAT, (intptr_t)uc->uc_mcontext.gregs[REG_CSGSFS]);
+  st->print(", ERR=" INTPTR_FORMAT, (intptr_t)uc->uc_mcontext.gregs[REG_ERR]);
+  st->cr();
+  st->print("  TRAPNO=" INTPTR_FORMAT, (intptr_t)uc->uc_mcontext.gregs[REG_TRAPNO]);
+#else
+  st->print(  "EAX=" INTPTR_FORMAT, uc->uc_mcontext.gregs[REG_EAX]);
+  st->print(", EBX=" INTPTR_FORMAT, uc->uc_mcontext.gregs[REG_EBX]);
+  st->print(", ECX=" INTPTR_FORMAT, uc->uc_mcontext.gregs[REG_ECX]);
+  st->print(", EDX=" INTPTR_FORMAT, uc->uc_mcontext.gregs[REG_EDX]);
+  st->cr();
+  st->print(  "ESP=" INTPTR_FORMAT, uc->uc_mcontext.gregs[REG_UESP]);
+  st->print(", EBP=" INTPTR_FORMAT, uc->uc_mcontext.gregs[REG_EBP]);
+  st->print(", ESI=" INTPTR_FORMAT, uc->uc_mcontext.gregs[REG_ESI]);
+  st->print(", EDI=" INTPTR_FORMAT, uc->uc_mcontext.gregs[REG_EDI]);
+  st->cr();
+  st->print(  "EIP=" INTPTR_FORMAT, uc->uc_mcontext.gregs[REG_EIP]);
+  st->print(", EFLAGS=" INTPTR_FORMAT, uc->uc_mcontext.gregs[REG_EFL]);
+  st->print(", CR2=" PTR64_FORMAT, (uint64_t)uc->uc_mcontext.cr2);
+#endif // AMD64
+  st->cr();
+  st->cr();
+
+  intptr_t *sp = (intptr_t *)os::Linux::ucontext_get_sp(uc);
+  st->print_cr("Top of Stack: (sp=" PTR_FORMAT ")", p2i(sp));
+  print_hex_dump(st, (address)sp, (address)(sp + 8), sizeof(intptr_t));
+  st->cr();
+
+  // Note: it may be unsafe to inspect memory near pc. For example, pc may
+  // point to garbage if entry point in an nmethod is corrupted. Leave
+  // this at the end, and hope for the best.
+  address pc = os::Posix::ucontext_get_pc(uc);
+  print_instructions(st, pc, sizeof(char));
+  st->cr();
+}
+
+void os::print_register_info(outputStream *st, const void *context) {
+  if (context == NULL) return;
+
+  const ucontext_t *uc = (const ucontext_t*)context;
+
+  st->print_cr("Register to memory mapping:");
+  st->cr();
+
+  // this is horrendously verbose but the layout of the registers in the
+  // context does not match how we defined our abstract Register set, so
+  // we can't just iterate through the gregs area
+
+  // this is only for the "general purpose" registers
+
+#ifdef AMD64
+  st->print("RAX="); print_location(st, uc->uc_mcontext.gregs[REG_RAX]);
+  st->print("RBX="); print_location(st, uc->uc_mcontext.gregs[REG_RBX]);
+  st->print("RCX="); print_location(st, uc->uc_mcontext.gregs[REG_RCX]);
+  st->print("RDX="); print_location(st, uc->uc_mcontext.gregs[REG_RDX]);
+  st->print("RSP="); print_location(st, uc->uc_mcontext.gregs[REG_RSP]);
+  st->print("RBP="); print_location(st, uc->uc_mcontext.gregs[REG_RBP]);
+  st->print("RSI="); print_location(st, uc->uc_mcontext.gregs[REG_RSI]);
+  st->print("RDI="); print_location(st, uc->uc_mcontext.gregs[REG_RDI]);
+  st->print("R8 ="); print_location(st, uc->uc_mcontext.gregs[REG_R8]);
+  st->print("R9 ="); print_location(st, uc->uc_mcontext.gregs[REG_R9]);
+  st->print("R10="); print_location(st, uc->uc_mcontext.gregs[REG_R10]);
+  st->print("R11="); print_location(st, uc->uc_mcontext.gregs[REG_R11]);
+  st->print("R12="); print_location(st, uc->uc_mcontext.gregs[REG_R12]);
+  st->print("R13="); print_location(st, uc->uc_mcontext.gregs[REG_R13]);
+  st->print("R14="); print_location(st, uc->uc_mcontext.gregs[REG_R14]);
+  st->print("R15="); print_location(st, uc->uc_mcontext.gregs[REG_R15]);
+#else
+  st->print("EAX="); print_location(st, uc->uc_mcontext.gregs[REG_EAX]);
+  st->print("EBX="); print_location(st, uc->uc_mcontext.gregs[REG_EBX]);
+  st->print("ECX="); print_location(st, uc->uc_mcontext.gregs[REG_ECX]);
+  st->print("EDX="); print_location(st, uc->uc_mcontext.gregs[REG_EDX]);
+  st->print("ESP="); print_location(st, uc->uc_mcontext.gregs[REG_ESP]);
+  st->print("EBP="); print_location(st, uc->uc_mcontext.gregs[REG_EBP]);
+  st->print("ESI="); print_location(st, uc->uc_mcontext.gregs[REG_ESI]);
+  st->print("EDI="); print_location(st, uc->uc_mcontext.gregs[REG_EDI]);
+#endif // AMD64
+
+  st->cr();
+}
+
+void os::setup_fpu() {
+#ifndef AMD64
+  address fpu_cntrl = StubRoutines::addr_fpu_cntrl_wrd_std();
+  __asm__ volatile (  "fldcw (%0)" :
+                      : "r" (fpu_cntrl) : "memory");
+#endif // !AMD64
+}
+
+#ifndef PRODUCT
+void os::verify_stack_alignment() {
+#ifdef AMD64
+  assert(((intptr_t)os::current_stack_pointer() & (StackAlignmentInBytes-1)) == 0, "incorrect stack alignment");
+#endif
+}
+#endif
+
+
+/*
+ * IA32 only: execute code at a high address in case buggy NX emulation is present. I.e. avoid CS limit
+ * updates (JDK-8023956).
+ */
+void os::workaround_expand_exec_shield_cs_limit() {
+#if defined(IA32)
+  assert(Linux::initial_thread_stack_bottom() != NULL, "sanity");
+  size_t page_size = os::vm_page_size();
+
+  /*
+   * JDK-8197429
+   *
+   * Expand the stack mapping to the end of the initial stack before
+   * attempting to install the codebuf.  This is needed because newer
+   * Linux kernels impose a distance of a megabyte between stack
+   * memory and other memory regions.  If we try to install the
+   * codebuf before expanding the stack the installation will appear
+   * to succeed but we'll get a segfault later if we expand the stack
+   * in Java code.
+   *
+   */
+  if (os::is_primordial_thread()) {
+    address limit = Linux::initial_thread_stack_bottom();
+    if (! DisablePrimordialThreadGuardPages) {
+      limit += StackOverflow::stack_red_zone_size() +
+               StackOverflow::stack_yellow_zone_size();
+    }
+    os::Linux::expand_stack_to(limit);
+  }
+
+  /*
+   * Take the highest VA the OS will give us and exec
+   *
+   * Although using -(pagesz) as mmap hint works on newer kernel as you would
+   * think, older variants affected by this work-around don't (search forward only).
+   *
+   * On the affected distributions, we understand the memory layout to be:
+   *
+   *   TASK_LIMIT= 3G, main stack base close to TASK_LIMT.
+   *
+   * A few pages south main stack will do it.
+   *
+   * If we are embedded in an app other than launcher (initial != main stack),
+   * we don't have much control or understanding of the address space, just let it slide.
+   */
+  char* hint = (char*)(Linux::initial_thread_stack_bottom() -
+                       (StackOverflow::stack_guard_zone_size() + page_size));
+  char* codebuf = os::attempt_reserve_memory_at(hint, page_size);
+
+  if (codebuf == NULL) {
+    // JDK-8197429: There may be a stack gap of one megabyte between
+    // the limit of the stack and the nearest memory region: this is a
+    // Linux kernel workaround for CVE-2017-1000364.  If we failed to
+    // map our codebuf, try again at an address one megabyte lower.
+    hint -= 1 * M;
+    codebuf = os::attempt_reserve_memory_at(hint, page_size);
+  }
+
+  if ((codebuf == NULL) || (!os::commit_memory(codebuf, page_size, true))) {
+    return; // No matter, we tried, best effort.
+  }
+
+  MemTracker::record_virtual_memory_type((address)codebuf, mtInternal);
+
+  log_info(os)("[CS limit NX emulation work-around, exec code at: %p]", codebuf);
+
+  // Some code to exec: the 'ret' instruction
+  codebuf[0] = 0xC3;
+
+  // Call the code in the codebuf
+  __asm__ volatile("call *%0" : : "r"(codebuf));
+
+  // keep the page mapped so CS limit isn't reduced.
+#endif
+}
+
+int os::extra_bang_size_in_bytes() {
+  // JDK-8050147 requires the full cache line bang for x86.
+  return VM_Version::L1_line_size();
+}
diff --git a/src/hotspot/os_cpu/serenity_x86/os_serenity_x86.hpp b/src/hotspot/os_cpu/serenity_x86/os_serenity_x86.hpp
new file mode 100644
index 00000000000..07d9bfe22db
--- /dev/null
+++ b/src/hotspot/os_cpu/serenity_x86/os_serenity_x86.hpp
@@ -0,0 +1,51 @@
+/*
+ * Copyright (c) 1999, 2019, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#ifndef OS_CPU_SERENITY_X86_OS_SERENITY_X86_HPP
+#define OS_CPU_SERENITY_X86_OS_SERENITY_X86_HPP
+
+  static void setup_fpu();
+  static bool supports_sse();
+  static juint cpu_microcode_revision();
+
+  static jlong rdtsc();
+
+  // Used to register dynamic code cache area with the OS
+  // Note: Currently only used in 64 bit Windows implementations
+  static bool register_code_area(char *low, char *high) { return true; }
+
+  /*
+   * Work-around for broken NX emulation using CS limit, Red Hat patch "Exec-Shield"
+   * (IA32 only).
+   *
+   * Map and execute at a high VA to prevent CS lazy updates race with SMP MM
+   * invalidation.Further code generation by the JVM will no longer cause CS limit
+   * updates.
+   *
+   * Affects IA32: RHEL 5 & 6, Ubuntu 10.04 (LTS), 10.10, 11.04, 11.10, 12.04.
+   * @see JDK-8023956
+   */
+  static void workaround_expand_exec_shield_cs_limit();
+
+#endif // OS_CPU_SERENITY_X86_OS_SERENITY_X86_HPP
diff --git a/src/hotspot/os_cpu/serenity_x86/os_serenity_x86.inline.hpp b/src/hotspot/os_cpu/serenity_x86/os_serenity_x86.inline.hpp
new file mode 100644
index 00000000000..b4a7aa6210f
--- /dev/null
+++ b/src/hotspot/os_cpu/serenity_x86/os_serenity_x86.inline.hpp
@@ -0,0 +1,46 @@
+/*
+ * Copyright (c) 2011, 2019, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#ifndef OS_CPU_SERENITY_X86_OS_SERENITY_X86_INLINE_HPP
+#define OS_CPU_SERENITY_X86_OS_SERENITY_X86_INLINE_HPP
+
+#include "runtime/os.hpp"
+
+// See http://www.technovelty.org/code/c/reading-rdtsc.htl for details
+inline jlong os::rdtsc() {
+#ifndef AMD64
+  // 64 bit result in edx:eax
+  uint64_t res;
+  __asm__ __volatile__ ("rdtsc" : "=A" (res));
+  return (jlong)res;
+#else
+  uint64_t res;
+  uint32_t ts1, ts2;
+  __asm__ __volatile__ ("rdtsc" : "=a" (ts1), "=d" (ts2));
+  res = ((uint64_t)ts1 | (uint64_t)ts2 << 32);
+  return (jlong)res;
+#endif // AMD64
+}
+
+#endif // OS_CPU_SERENITY_X86_OS_SERENITY_X86_INLINE_HPP
diff --git a/src/hotspot/os_cpu/serenity_x86/prefetch_serenity_x86.inline.hpp b/src/hotspot/os_cpu/serenity_x86/prefetch_serenity_x86.inline.hpp
new file mode 100644
index 00000000000..01f7c46f700
--- /dev/null
+++ b/src/hotspot/os_cpu/serenity_x86/prefetch_serenity_x86.inline.hpp
@@ -0,0 +1,47 @@
+/*
+ * Copyright (c) 2003, 2019, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#ifndef OS_CPU_SERENITY_X86_PREFETCH_SERENITY_X86_INLINE_HPP
+#define OS_CPU_SERENITY_X86_PREFETCH_SERENITY_X86_INLINE_HPP
+
+#include "runtime/prefetch.hpp"
+
+
+inline void Prefetch::read (void *loc, intx interval) {
+#ifdef AMD64
+  __asm__ ("prefetcht0 (%0,%1,1)" : : "r" (loc), "r" (interval));
+#endif // AMD64
+}
+
+inline void Prefetch::write(void *loc, intx interval) {
+#ifdef AMD64
+
+  // Do not use the 3dnow prefetchw instruction.  It isn't supported on em64t.
+  //  __asm__ ("prefetchw (%0,%1,1)" : : "r" (loc), "r" (interval));
+  __asm__ ("prefetcht0 (%0,%1,1)" : : "r" (loc), "r" (interval));
+
+#endif // AMD64
+}
+
+#endif // OS_CPU_SERENITY_X86_PREFETCH_SERENITY_X86_INLINE_HPP
diff --git a/src/hotspot/os_cpu/serenity_x86/thread_serenity_x86.cpp b/src/hotspot/os_cpu/serenity_x86/thread_serenity_x86.cpp
new file mode 100644
index 00000000000..b030abe5b2b
--- /dev/null
+++ b/src/hotspot/os_cpu/serenity_x86/thread_serenity_x86.cpp
@@ -0,0 +1,94 @@
+/*
+ * Copyright (c) 2003, 2021, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#include "precompiled.hpp"
+#include "memory/metaspaceShared.hpp"
+#include "runtime/frame.inline.hpp"
+#include "runtime/thread.inline.hpp"
+
+frame JavaThread::pd_last_frame() {
+  assert(has_last_Java_frame(), "must have last_Java_sp() when suspended");
+  vmassert(_anchor.last_Java_pc() != NULL, "not walkable");
+  return frame(_anchor.last_Java_sp(), _anchor.last_Java_fp(), _anchor.last_Java_pc());
+}
+
+// For Forte Analyzer AsyncGetCallTrace profiling support - thread is
+// currently interrupted by SIGPROF
+bool JavaThread::pd_get_top_frame_for_signal_handler(frame* fr_addr,
+  void* ucontext, bool isInJava) {
+
+  assert(Thread::current() == this, "caller must be current thread");
+  return pd_get_top_frame(fr_addr, ucontext, isInJava);
+}
+
+bool JavaThread::pd_get_top_frame_for_profiling(frame* fr_addr, void* ucontext, bool isInJava) {
+  return pd_get_top_frame(fr_addr, ucontext, isInJava);
+}
+
+bool JavaThread::pd_get_top_frame(frame* fr_addr, void* ucontext, bool isInJava) {
+  // If we have a last_Java_frame, then we should use it even if
+  // isInJava == true.  It should be more reliable than ucontext info.
+  if (has_last_Java_frame() && frame_anchor()->walkable()) {
+    *fr_addr = pd_last_frame();
+    return true;
+  }
+
+  // At this point, we don't have a last_Java_frame, so
+  // we try to glean some information out of the ucontext
+  // if we were running Java code when SIGPROF came in.
+  if (isInJava) {
+    ucontext_t* uc = (ucontext_t*) ucontext;
+
+    intptr_t* ret_fp;
+    intptr_t* ret_sp;
+    address addr = os::fetch_frame_from_context(uc, &ret_sp, &ret_fp);
+    if (addr == NULL || ret_sp == NULL ) {
+      // ucontext wasn't useful
+      return false;
+    }
+
+    frame ret_frame(ret_sp, ret_fp, addr);
+    if (!ret_frame.safe_for_sender(this)) {
+#if COMPILER2_OR_JVMCI
+      // C2 and JVMCI use ebp as a general register see if NULL fp helps
+      frame ret_frame2(ret_sp, NULL, addr);
+      if (!ret_frame2.safe_for_sender(this)) {
+        // nothing else to try if the frame isn't good
+        return false;
+      }
+      ret_frame = ret_frame2;
+#else
+      // nothing else to try if the frame isn't good
+      return false;
+#endif // COMPILER2_OR_JVMCI
+    }
+    *fr_addr = ret_frame;
+    return true;
+  }
+
+  // nothing else to try
+  return false;
+}
+
+void JavaThread::cache_global_variables() { }
diff --git a/src/hotspot/os_cpu/serenity_x86/thread_serenity_x86.hpp b/src/hotspot/os_cpu/serenity_x86/thread_serenity_x86.hpp
new file mode 100644
index 00000000000..fae6aff6ad6
--- /dev/null
+++ b/src/hotspot/os_cpu/serenity_x86/thread_serenity_x86.hpp
@@ -0,0 +1,48 @@
+/*
+ * Copyright (c) 2000, 2020, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#ifndef OS_CPU_SERENITY_X86_THREAD_SERENITY_X86_HPP
+#define OS_CPU_SERENITY_X86_THREAD_SERENITY_X86_HPP
+
+ private:
+  void pd_initialize() {
+    _anchor.clear();
+  }
+
+  frame pd_last_frame();
+
+ public:
+  static ByteSize last_Java_fp_offset()          {
+    return byte_offset_of(JavaThread, _anchor) + JavaFrameAnchor::last_Java_fp_offset();
+  }
+
+  bool pd_get_top_frame_for_signal_handler(frame* fr_addr, void* ucontext,
+    bool isInJava);
+
+  bool pd_get_top_frame_for_profiling(frame* fr_addr, void* ucontext, bool isInJava);
+private:
+  bool pd_get_top_frame(frame* fr_addr, void* ucontext, bool isInJava);
+public:
+
+#endif // OS_CPU_SERENITY_X86_THREAD_SERENITY_X86_HPP
diff --git a/src/hotspot/os_cpu/serenity_x86/vmStructs_serenity_x86.hpp b/src/hotspot/os_cpu/serenity_x86/vmStructs_serenity_x86.hpp
new file mode 100644
index 00000000000..bb61ee8890e
--- /dev/null
+++ b/src/hotspot/os_cpu/serenity_x86/vmStructs_serenity_x86.hpp
@@ -0,0 +1,54 @@
+/*
+ * Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#ifndef OS_CPU_SERENITY_X86_VMSTRUCTS_SERENITY_X86_HPP
+#define OS_CPU_SERENITY_X86_VMSTRUCTS_SERENITY_X86_HPP
+
+// These are the OS and CPU-specific fields, types and integer
+// constants required by the Serviceability Agent. This file is
+// referenced by vmStructs.cpp.
+
+#define VM_STRUCTS_OS_CPU(nonstatic_field, static_field, unchecked_nonstatic_field, volatile_nonstatic_field, nonproduct_nonstatic_field, c2_nonstatic_field, unchecked_c1_static_field, unchecked_c2_static_field) \
+                                                                                                                                     \
+  /******************************/                                                                                                   \
+  /* Threads (NOTE: incomplete) */                                                                                                   \
+  /******************************/                                                                                                   \
+  nonstatic_field(OSThread,                      _thread_id,                                      OSThread::thread_id_t)             \
+  nonstatic_field(OSThread,                      _pthread_id,                                     pthread_t)
+
+
+#define VM_TYPES_OS_CPU(declare_type, declare_toplevel_type, declare_oop_type, declare_integer_type, declare_unsigned_integer_type, declare_c1_toplevel_type, declare_c2_type, declare_c2_toplevel_type) \
+                                                                          \
+  /**********************/                                                \
+  /* Posix Thread IDs   */                                                \
+  /**********************/                                                \
+                                                                          \
+  declare_integer_type(OSThread::thread_id_t)                             \
+  declare_unsigned_integer_type(pthread_t)
+
+#define VM_INT_CONSTANTS_OS_CPU(declare_constant, declare_preprocessor_constant, declare_c1_constant, declare_c2_constant, declare_c2_preprocessor_constant)
+
+#define VM_LONG_CONSTANTS_OS_CPU(declare_constant, declare_preprocessor_constant, declare_c1_constant, declare_c2_constant, declare_c2_preprocessor_constant)
+
+#endif // OS_CPU_SERENITY_X86_VMSTRUCTS_SERENITY_X86_HPP
diff --git a/src/hotspot/os_cpu/serenity_x86/vm_version_serenity_x86.cpp b/src/hotspot/os_cpu/serenity_x86/vm_version_serenity_x86.cpp
new file mode 100644
index 00000000000..ef85f6fb920
--- /dev/null
+++ b/src/hotspot/os_cpu/serenity_x86/vm_version_serenity_x86.cpp
@@ -0,0 +1,28 @@
+/*
+ * Copyright (c) 2006, 2019, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#include "precompiled.hpp"
+#include "runtime/os.hpp"
+#include "runtime/vm_version.hpp"
+
diff --git a/src/hotspot/share/utilities/globalDefinitions_gcc.hpp b/src/hotspot/share/utilities/globalDefinitions_gcc.hpp
index 30cca9ee7ae..fda7d9caf79 100644
--- a/src/hotspot/share/utilities/globalDefinitions_gcc.hpp
+++ b/src/hotspot/share/utilities/globalDefinitions_gcc.hpp
@@ -48,10 +48,14 @@
 #include <limits.h>
 #include <errno.h>
 
-#if defined(LINUX) || defined(_ALLBSD_SOURCE)
+#if defined(LINUX) || defined(_ALLBSD_SOURCE) || defined(__serenity__)
 #include <inttypes.h>
 #include <signal.h>
-#ifndef __OpenBSD__
+#ifdef __serenity__
+#include <strings.h>
+#include <stdint.h>
+#endif
+#if !defined(__OpenBSD__) && !defined(__serenity__)
 #include <ucontext.h>
 #endif
 #ifdef __APPLE__
@@ -79,7 +83,7 @@
   #define NULL_WORD  NULL
 #endif
 
-#if !defined(LINUX) && !defined(_ALLBSD_SOURCE)
+#if !defined(LINUX) && !defined(_ALLBSD_SOURCE) && !defined(__serenity__)
 // Compiler-specific primitive types
 typedef unsigned short     uint16_t;
 #ifndef _UINT32_T
@@ -111,7 +115,7 @@ typedef uint64_t julong;
 // checking for nanness
 #if defined(__APPLE__)
 inline int g_isnan(double f) { return isnan(f); }
-#elif defined(LINUX) || defined(_ALLBSD_SOURCE)
+#elif defined(LINUX) || defined(_ALLBSD_SOURCE) || defined(__serenity__)
 inline int g_isnan(float  f) { return isnan(f); }
 inline int g_isnan(double f) { return isnan(f); }
 #else
-- 
2.31.1

